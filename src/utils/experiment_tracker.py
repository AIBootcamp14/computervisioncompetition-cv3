"""
Experiment Tracker with wandb

wandbë¥¼ í™œìš©í•œ ì‹¤í—˜ ì¶”ì  ë° ê´€ë¦¬ í´ë˜ìŠ¤
"""

import os
import json
from typing import Dict, List, Optional, Any, Union
from pathlib import Path
import wandb
import torch
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime


class ExperimentTracker:
    """
    wandbë¥¼ í™œìš©í•œ ì‹¤í—˜ ì¶”ì  í´ë˜ìŠ¤
    
    ê° ë©¤ë²„ê°€ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í—˜ì„ ê´€ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„
    """
    
    def __init__(
        self,
        project_name: str = "doc-classification",
        member_name: str = "member1",
        experiment_name: Optional[str] = None,
        config: Optional[Dict] = None,
        offline_mode: bool = False
    ):
        """
        Args:
            project_name: wandb í”„ë¡œì íŠ¸ ì´ë¦„
            member_name: ë©¤ë²„ ì´ë¦„ (íƒœê·¸ë¡œ ì‚¬ìš©)
            experiment_name: ì‹¤í—˜ ì´ë¦„
            config: ì‹¤í—˜ ì„¤ì •
            offline_mode: ì˜¤í”„ë¼ì¸ ëª¨ë“œ ì—¬ë¶€
        """
        self.project_name = project_name
        self.member_name = member_name
        self.experiment_name = experiment_name or f"{member_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.config = config or {}
        self.offline_mode = offline_mode
        
        # wandb ì´ˆê¸°í™”
        self._init_wandb()
        
        # ì‹¤í—˜ ë©”íƒ€ë°ì´í„° ì €ì¥
        self.experiment_metadata = {
            'member': member_name,
            'start_time': datetime.now().isoformat(),
            'experiment_id': self.run.id if self.run else None,
            'metrics_history': [],
            'model_checkpoints': []
        }
    
    def _init_wandb(self) -> None:
        """wandb ì´ˆê¸°í™”"""
        try:
            if self.offline_mode:
                os.environ["WANDB_MODE"] = "offline"
            
            self.run = wandb.init(
                project=self.project_name,
                name=self.experiment_name,
                config=self.config,
                tags=[self.member_name, "document-classification"],
                reinit=True
            )
            
            print(f"âœ… wandb ì´ˆê¸°í™” ì™„ë£Œ: {self.experiment_name}")
            
        except Exception as e:
            print(f"âš ï¸ wandb ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("ì˜¤í”„ë¼ì¸ ëª¨ë“œë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            self.run = None
    
    def log_config(self, config: Dict[str, Any]) -> None:
        """
        ì‹¤í—˜ ì„¤ì • ë¡œê¹…
        
        Args:
            config: ì‹¤í—˜ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.config.update(config)
        
        if self.run:
            wandb.config.update(config)
        
        # ë¡œì»¬ì—ë„ ì €ì¥
        self._save_config_locally(config)
    
    def log_metrics(
        self,
        metrics: Dict[str, Union[float, int]],
        step: Optional[int] = None,
        prefix: str = ""
    ) -> None:
        """
        ë©”íŠ¸ë¦­ ë¡œê¹…
        
        Args:
            metrics: ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
            step: ìŠ¤í… ë²ˆí˜¸
            prefix: ë©”íŠ¸ë¦­ ì´ë¦„ ì ‘ë‘ì‚¬ (train_, val_ ë“±)
        """
        # ì ‘ë‘ì‚¬ ì ìš©
        if prefix:
            metrics = {f"{prefix}{k}": v for k, v in metrics.items()}
        
        # wandb ë¡œê¹…
        if self.run:
            if step is not None:
                wandb.log(metrics, step=step)
            else:
                wandb.log(metrics)
        
        # ë¡œì»¬ íˆìŠ¤í† ë¦¬ ì €ì¥
        timestamp = datetime.now().isoformat()
        log_entry = {
            'timestamp': timestamp,
            'step': step,
            'metrics': metrics
        }
        self.experiment_metadata['metrics_history'].append(log_entry)
        
        # ì½˜ì†” ì¶œë ¥
        metrics_str = " | ".join([f"{k}: {v:.4f}" if isinstance(v, float) else f"{k}: {v}" 
                                 for k, v in metrics.items()])
        print(f"ğŸ“Š Step {step}: {metrics_str}")
    
    def log_model_checkpoint(
        self,
        model: torch.nn.Module,
        checkpoint_path: Path,
        metrics: Dict[str, float],
        is_best: bool = False
    ) -> None:
        """
        ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œê¹…
        
        Args:
            model: ì €ì¥í•  ëª¨ë¸
            checkpoint_path: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
            metrics: í•´ë‹¹ ì²´í¬í¬ì¸íŠ¸ì˜ ì„±ëŠ¥ ë©”íŠ¸ë¦­
            is_best: ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì—¬ë¶€
        """
        # ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ê¸°ë¡
        checkpoint_info = {
            'path': str(checkpoint_path),
            'metrics': metrics,
            'is_best': is_best,
            'timestamp': datetime.now().isoformat()
        }
        self.experiment_metadata['model_checkpoints'].append(checkpoint_info)
        
        # wandbì— ì•„í‹°íŒ©íŠ¸ë¡œ ì—…ë¡œë“œ
        if self.run:
            artifact_name = f"model_checkpoint_{self.member_name}"
            if is_best:
                artifact_name += "_best"
            
            artifact = wandb.Artifact(
                name=artifact_name,
                type="model",
                metadata=checkpoint_info
            )
            artifact.add_file(str(checkpoint_path))
            self.run.log_artifact(artifact)
        
        print(f"ğŸ’¾ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")
        if is_best:
            print("ğŸ† ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥ ëª¨ë¸!")
    
    def log_images(
        self,
        images: Dict[str, Union[np.ndarray, plt.Figure]],
        step: Optional[int] = None
    ) -> None:
        """
        ì´ë¯¸ì§€ ë¡œê¹…
        
        Args:
            images: ì´ë¯¸ì§€ ë”•ì…”ë„ˆë¦¬ (ì´ë¦„: ì´ë¯¸ì§€)
            step: ìŠ¤í… ë²ˆí˜¸
        """
        if not self.run:
            return
        
        wandb_images = {}
        for name, image in images.items():
            if isinstance(image, plt.Figure):
                wandb_images[name] = wandb.Image(image)
            elif isinstance(image, np.ndarray):
                wandb_images[name] = wandb.Image(image)
            else:
                print(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹: {type(image)}")
        
        if step is not None:
            wandb.log(wandb_images, step=step)
        else:
            wandb.log(wandb_images)
    
    def log_confusion_matrix(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        class_names: List[str],
        step: Optional[int] = None
    ) -> None:
        """
        Confusion Matrix ë¡œê¹…
        
        Args:
            y_true: ì‹¤ì œ ë¼ë²¨
            y_pred: ì˜ˆì¸¡ ë¼ë²¨
            class_names: í´ë˜ìŠ¤ ì´ë¦„ ëª©ë¡
            step: ìŠ¤í… ë²ˆí˜¸
        """
        if not self.run:
            return
        
        # wandb confusion matrix ìƒì„±
        cm = wandb.plot.confusion_matrix(
            y_true=y_true,
            preds=y_pred,
            class_names=class_names
        )
        
        log_data = {"confusion_matrix": cm}
        if step is not None:
            wandb.log(log_data, step=step)
        else:
            wandb.log(log_data)
    
    def log_learning_curve(
        self,
        train_scores: List[float],
        val_scores: List[float],
        metric_name: str = "accuracy"
    ) -> None:
        """
        í•™ìŠµ ê³¡ì„  ë¡œê¹…
        
        Args:
            train_scores: í›ˆë ¨ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
            val_scores: ê²€ì¦ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
            metric_name: ë©”íŠ¸ë¦­ ì´ë¦„
        """
        if not self.run:
            return
        
        # í•™ìŠµ ê³¡ì„  í”Œë¡¯ ìƒì„±
        fig, ax = plt.subplots(figsize=(10, 6))
        epochs = range(1, len(train_scores) + 1)
        
        ax.plot(epochs, train_scores, 'o-', label=f'Train {metric_name}', color='blue')
        ax.plot(epochs, val_scores, 'o-', label=f'Val {metric_name}', color='red')
        ax.set_xlabel('Epoch')
        ax.set_ylabel(metric_name.capitalize())
        ax.set_title(f'Learning Curve - {metric_name.capitalize()}')
        ax.legend()
        ax.grid(True)
        
        # wandbì— ë¡œê¹…
        wandb.log({f"learning_curve_{metric_name}": wandb.Image(fig)})
        plt.close(fig)
    
    def save_experiment_summary(self, save_path: Optional[Path] = None) -> Path:
        """
        ì‹¤í—˜ ìš”ì•½ ì €ì¥
        
        Args:
            save_path: ì €ì¥ ê²½ë¡œ
            
        Returns:
            Path: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        if save_path is None:
            save_path = Path(f"experiment_summary_{self.member_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        
        # ì‹¤í—˜ ì¢…ë£Œ ì‹œê°„ ì¶”ê°€
        self.experiment_metadata['end_time'] = datetime.now().isoformat()
        
        # ìµœê³  ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì°¾ê¸°
        if self.experiment_metadata['metrics_history']:
            best_metrics = self._find_best_metrics()
            self.experiment_metadata['best_metrics'] = best_metrics
        
        # JSONìœ¼ë¡œ ì €ì¥
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(self.experiment_metadata, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“‹ ì‹¤í—˜ ìš”ì•½ ì €ì¥: {save_path}")
        return save_path
    
    def _save_config_locally(self, config: Dict[str, Any]) -> None:
        """ì„¤ì •ì„ ë¡œì»¬ì— ì €ì¥"""
        config_path = Path(f"config_{self.member_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
    
    def _find_best_metrics(self) -> Dict[str, Any]:
        """ìµœê³  ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì°¾ê¸°"""
        best_metrics = {}
        
        # ëª¨ë“  ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        all_metrics = {}
        for entry in self.experiment_metadata['metrics_history']:
            for metric_name, value in entry['metrics'].items():
                if isinstance(value, (int, float)):
                    if metric_name not in all_metrics:
                        all_metrics[metric_name] = []
                    all_metrics[metric_name].append(value)
        
        # ê° ë©”íŠ¸ë¦­ì˜ ìµœê³ ê°’ ì°¾ê¸°
        for metric_name, values in all_metrics.items():
            if 'loss' in metric_name.lower():
                # lossëŠ” ìµœì†Œê°’ì´ ìµœê³ 
                best_metrics[f"best_{metric_name}"] = min(values)
            else:
                # ë‹¤ë¥¸ ë©”íŠ¸ë¦­ë“¤ì€ ìµœëŒ€ê°’ì´ ìµœê³ 
                best_metrics[f"best_{metric_name}"] = max(values)
        
        return best_metrics
    
    def finish_experiment(self) -> None:
        """ì‹¤í—˜ ì¢…ë£Œ"""
        if self.run:
            # ìµœì¢… ìš”ì•½ ì €ì¥
            summary_path = self.save_experiment_summary()
            
            # wandb ì•„í‹°íŒ©íŠ¸ë¡œ ìš”ì•½ ì—…ë¡œë“œ
            artifact = wandb.Artifact(
                name=f"experiment_summary_{self.member_name}",
                type="summary"
            )
            artifact.add_file(str(summary_path))
            self.run.log_artifact(artifact)
            
            # wandb ì‹¤í–‰ ì¢…ë£Œ
            wandb.finish()
            print(f"ğŸ ì‹¤í—˜ ì¢…ë£Œ: {self.experiment_name}")
        else:
            # ì˜¤í”„ë¼ì¸ ëª¨ë“œì—ì„œë„ ìš”ì•½ ì €ì¥
            self.save_experiment_summary()
            print(f"ğŸ ì‹¤í—˜ ì¢…ë£Œ (ì˜¤í”„ë¼ì¸): {self.experiment_name}")
    
    def __enter__(self):
        """Context manager ì§„ì…"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager ì¢…ë£Œ"""
        self.finish_experiment()
