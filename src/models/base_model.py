"""
Base Model Class

ëª¨ë“  ë¬¸ì„œ ë¶„ë¥˜ ëª¨ë¸ì˜ ê¸°ë³¸ í´ë˜ìŠ¤
"""

from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Tuple, Any
import torch
import torch.nn as nn
import torch.nn.functional as F
from pathlib import Path


class BaseDocumentClassifier(nn.Module, ABC):
    """
    ë¬¸ì„œ ë¶„ë¥˜ë¥¼ ìœ„í•œ ê¸°ë³¸ ëª¨ë¸ í´ë˜ìŠ¤
    
    ëª¨ë“  ë©¤ë²„ê°€ ìƒì†ë°›ì•„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê³µí†µ ì¸í„°í˜ì´ìŠ¤ ì œê³µ
    """
    
    def __init__(
        self,
        num_classes: int,
        model_name: str = "base_model",
        dropout_rate: float = 0.1
    ):
        """
        Args:
            num_classes: ë¶„ë¥˜í•  í´ë˜ìŠ¤ ìˆ˜
            model_name: ëª¨ë¸ ì´ë¦„
            dropout_rate: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
        """
        super().__init__()
        
        self.num_classes = num_classes
        self.model_name = model_name
        self.dropout_rate = dropout_rate
        
        # ëª¨ë¸ êµ¬ì„± ìš”ì†Œë“¤ (ì„œë¸Œí´ë˜ìŠ¤ì—ì„œ ì •ì˜)
        self.backbone = None
        self.classifier = None
        
        # ëª¨ë¸ ë©”íƒ€ë°ì´í„°
        self.model_info = {
            'name': model_name,
            'num_classes': num_classes,
            'dropout_rate': dropout_rate,
            'created_at': None,
            'total_params': None,
            'trainable_params': None
        }
    
    @abstractmethod
    def _build_backbone(self) -> nn.Module:
        """
        ë°±ë³¸ ë„¤íŠ¸ì›Œí¬ êµ¬ì„± (ì¶”ìƒ ë©”ì„œë“œ)
        ê° ë©¤ë²„ê°€ ìì‹ ë§Œì˜ ì•„í‚¤í…ì²˜ë¡œ êµ¬í˜„
        
        Returns:
            nn.Module: ë°±ë³¸ ë„¤íŠ¸ì›Œí¬
        """
        pass
    
    @abstractmethod
    def _build_classifier(self, feature_dim: int) -> nn.Module:
        """
        ë¶„ë¥˜ê¸° í—¤ë“œ êµ¬ì„± (ì¶”ìƒ ë©”ì„œë“œ)
        
        Args:
            feature_dim: ë°±ë³¸ì—ì„œ ë‚˜ì˜¤ëŠ” íŠ¹ì„± ì°¨ì›
            
        Returns:
            nn.Module: ë¶„ë¥˜ê¸°
        """
        pass
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        ìˆœì „íŒŒ
        
        Args:
            x: ì…ë ¥ í…ì„œ [batch_size, channels, height, width]
            
        Returns:
            torch.Tensor: í´ë˜ìŠ¤ ë¡œì§“ [batch_size, num_classes]
        """
        # ë°±ë³¸ì„ í†µí•œ íŠ¹ì„± ì¶”ì¶œ
        features = self.extract_features(x)
        
        # ë¶„ë¥˜
        logits = self.classify(features)
        
        return logits
    
    def extract_features(self, x: torch.Tensor) -> torch.Tensor:
        """
        íŠ¹ì„± ì¶”ì¶œ (ë°±ë³¸ í†µê³¼)
        
        Args:
            x: ì…ë ¥ í…ì„œ
            
        Returns:
            torch.Tensor: ì¶”ì¶œëœ íŠ¹ì„±
        """
        if self.backbone is None:
            raise RuntimeError("Backbone not initialized. Call _build_backbone() first.")
        
        return self.backbone(x)
    
    def classify(self, features: torch.Tensor) -> torch.Tensor:
        """
        ë¶„ë¥˜ (ë¶„ë¥˜ê¸° í—¤ë“œ í†µê³¼)
        
        Args:
            features: ë°±ë³¸ì—ì„œ ì¶”ì¶œëœ íŠ¹ì„±
            
        Returns:
            torch.Tensor: í´ë˜ìŠ¤ ë¡œì§“
        """
        if self.classifier is None:
            raise RuntimeError("Classifier not initialized. Call _build_classifier() first.")
        
        return self.classifier(features)
    
    def predict(self, x: torch.Tensor, return_probs: bool = False) -> torch.Tensor:
        """
        ì˜ˆì¸¡ ìˆ˜í–‰
        
        Args:
            x: ì…ë ¥ í…ì„œ
            return_probs: í™•ë¥ ê°’ ë°˜í™˜ ì—¬ë¶€
            
        Returns:
            torch.Tensor: ì˜ˆì¸¡ ê²°ê³¼ (í´ë˜ìŠ¤ ì¸ë±ìŠ¤ ë˜ëŠ” í™•ë¥ )
        """
        self.eval()
        with torch.no_grad():
            logits = self.forward(x)
            
            if return_probs:
                return F.softmax(logits, dim=1)
            else:
                return torch.argmax(logits, dim=1)
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        ëª¨ë¸ ì •ë³´ ë°˜í™˜
        
        Returns:
            Dict[str, Any]: ëª¨ë¸ ë©”íƒ€ë°ì´í„°
        """
        # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        self.model_info.update({
            'total_params': total_params,
            'trainable_params': trainable_params
        })
        
        return self.model_info.copy()
    
    def freeze_backbone(self) -> None:
        """ë°±ë³¸ ë„¤íŠ¸ì›Œí¬ ë™ê²° (ì „ì´í•™ìŠµìš©)"""
        if self.backbone is None:
            raise RuntimeError("Backbone not initialized.")
        
        for param in self.backbone.parameters():
            param.requires_grad = False
        
        print("ğŸ§Š ë°±ë³¸ ë„¤íŠ¸ì›Œí¬ê°€ ë™ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def unfreeze_backbone(self) -> None:
        """ë°±ë³¸ ë„¤íŠ¸ì›Œí¬ ë™ê²° í•´ì œ"""
        if self.backbone is None:
            raise RuntimeError("Backbone not initialized.")
        
        for param in self.backbone.parameters():
            param.requires_grad = True
        
        print("ğŸ”¥ ë°±ë³¸ ë„¤íŠ¸ì›Œí¬ ë™ê²°ì´ í•´ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def get_layer_names(self) -> List[str]:
        """ëª¨ë¸ì˜ ëª¨ë“  ë ˆì´ì–´ ì´ë¦„ ë°˜í™˜"""
        return [name for name, _ in self.named_modules()]
    
    def get_feature_map_size(self, input_size: Tuple[int, int, int]) -> Tuple[int, ...]:
        """
        ì£¼ì–´ì§„ ì…ë ¥ í¬ê¸°ì— ëŒ€í•œ íŠ¹ì„± ë§µ í¬ê¸° ê³„ì‚°
        
        Args:
            input_size: ì…ë ¥ í¬ê¸° (C, H, W)
            
        Returns:
            Tuple[int, ...]: íŠ¹ì„± ë§µ í¬ê¸°
        """
        self.eval()
        with torch.no_grad():
            x = torch.randn(1, *input_size)
            features = self.extract_features(x)
            return features.shape[1:]  # ë°°ì¹˜ ì°¨ì› ì œì™¸
    
    def save_model(self, save_path: Path, save_config: bool = True) -> None:
        """
        ëª¨ë¸ ì €ì¥
        
        Args:
            save_path: ì €ì¥ ê²½ë¡œ
            save_config: ëª¨ë¸ ì„¤ì •ë„ í•¨ê»˜ ì €ì¥í• ì§€ ì—¬ë¶€
        """
        save_path = Path(save_path)
        save_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ëª¨ë¸ ìƒíƒœ ì €ì¥
        checkpoint = {
            'model_state_dict': self.state_dict(),
            'model_info': self.get_model_info(),
            'model_class': self.__class__.__name__
        }
        
        torch.save(checkpoint, save_path)
        
        # ì„¤ì • íŒŒì¼ ë³„ë„ ì €ì¥
        if save_config:
            config_path = save_path.with_suffix('.json')
            import json
            with open(config_path, 'w') as f:
                json.dump(self.model_info, f, indent=2)
        
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")
    
    @classmethod
    def load_model(cls, load_path: Path, **kwargs) -> 'BaseDocumentClassifier':
        """
        ëª¨ë¸ ë¡œë“œ
        
        Args:
            load_path: ë¡œë“œ ê²½ë¡œ
            **kwargs: ëª¨ë¸ ì´ˆê¸°í™” íŒŒë¼ë¯¸í„°
            
        Returns:
            BaseDocumentClassifier: ë¡œë“œëœ ëª¨ë¸
        """
        checkpoint = torch.load(load_path, map_location='cpu')
        
        # ëª¨ë¸ ì •ë³´ì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        model_info = checkpoint['model_info']
        
        # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        model = cls(
            num_classes=model_info['num_classes'],
            model_name=model_info['name'],
            dropout_rate=model_info.get('dropout_rate', 0.1),
            **kwargs
        )
        
        # ìƒíƒœ ë¡œë“œ
        model.load_state_dict(checkpoint['model_state_dict'])
        model.model_info = model_info
        
        print(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {load_path}")
        return model
    
    def summary(self, input_size: Tuple[int, int, int] = (3, 224, 224)) -> str:
        """
        ëª¨ë¸ ìš”ì•½ ì •ë³´ ìƒì„±
        
        Args:
            input_size: ì…ë ¥ í¬ê¸°
            
        Returns:
            str: ëª¨ë¸ ìš”ì•½
        """
        lines = []
        lines.append(f"ëª¨ë¸: {self.model_name}")
        lines.append(f"í´ë˜ìŠ¤ ìˆ˜: {self.num_classes}")
        lines.append(f"ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨: {self.dropout_rate}")
        lines.append("-" * 50)
        
        # íŒŒë¼ë¯¸í„° ì •ë³´
        info = self.get_model_info()
        lines.append(f"ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {info['total_params']:,}")
        lines.append(f"í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜: {info['trainable_params']:,}")
        
        # íŠ¹ì„± ë§µ í¬ê¸°
        try:
            feature_size = self.get_feature_map_size(input_size)
            lines.append(f"íŠ¹ì„± ë§µ í¬ê¸°: {feature_size}")
        except:
            lines.append("íŠ¹ì„± ë§µ í¬ê¸°: ê³„ì‚° ë¶ˆê°€")
        
        return "\n".join(lines)
