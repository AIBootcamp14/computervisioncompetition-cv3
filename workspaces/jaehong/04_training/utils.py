"""
ğŸ› ï¸ Training Utilities
ì‹œë‹ˆì–´ ìºê¸€ëŸ¬ ìˆ˜ì¤€ì˜ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤

Features:
- Reproducibility
- Model checkpointing
- Metrics tracking
- GPU optimization
"""

import os
import random
import numpy as np
import torch
from pathlib import Path
from typing import Dict, Any, Optional
import json
import time


def set_seed(seed: int = 42):
    """ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •"""
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    print(f"ğŸŒ± ì‹œë“œ ì„¤ì • ì™„ë£Œ: {seed}")


def get_gpu_info():
    """GPU ì •ë³´ ì¶œë ¥"""
    if torch.cuda.is_available():
        device_count = torch.cuda.device_count()
        print(f"ğŸ–¥ï¸ GPU ì •ë³´:")
        for i in range(device_count):
            props = torch.cuda.get_device_properties(i)
            memory_gb = props.total_memory / 1024**3
            print(f"   GPU {i}: {props.name} ({memory_gb:.1f} GB)")
            
        current_device = torch.cuda.current_device()
        memory_allocated = torch.cuda.memory_allocated(current_device) / 1024**2
        memory_cached = torch.cuda.memory_reserved(current_device) / 1024**2
        print(f"   í˜„ì¬ ë©”ëª¨ë¦¬: {memory_allocated:.1f} MB allocated, {memory_cached:.1f} MB cached")
        
        return {
            "available": True,
            "device_count": device_count,
            "current_device": current_device,
            "device_name": torch.cuda.get_device_name(current_device),
            "total_memory_gb": torch.cuda.get_device_properties(current_device).total_memory / 1024**3
        }
    else:
        print("âš ï¸ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ í›ˆë ¨í•©ë‹ˆë‹¤.")
        return {"available": False}


class AverageMeter:
    """í‰ê·  ë©”íŠ¸ë¦­ ì¶”ì ê¸°"""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0
    
    def update(self, val: float, n: int = 1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count


class Timer:
    """ì‹œê°„ ì¸¡ì •ê¸°"""
    
    def __init__(self):
        self.start_time = None
        self.end_time = None
    
    def start(self):
        self.start_time = time.time()
    
    def stop(self):
        self.end_time = time.time()
        return self.elapsed_time()
    
    def elapsed_time(self) -> float:
        if self.start_time is None:
            return 0.0
        end = self.end_time if self.end_time else time.time()
        return end - self.start_time
    
    def elapsed_time_str(self) -> str:
        elapsed = self.elapsed_time()
        hours = int(elapsed // 3600)
        minutes = int((elapsed % 3600) // 60)
        seconds = int(elapsed % 60)
        return f"{hours:02d}:{minutes:02d}:{seconds:02d}"


def save_checkpoint(
    state: Dict[str, Any],
    filepath: Path,
    is_best: bool = False
):
    """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
    filepath.parent.mkdir(parents=True, exist_ok=True)
    torch.save(state, filepath)
    
    if is_best:
        best_path = filepath.parent / "model_best.pth"
        torch.save(state, best_path)
    
    print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {filepath}")


def load_checkpoint(
    filepath: Path,
    model: torch.nn.Module,
    optimizer: Optional[torch.optim.Optimizer] = None,
    scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
    device: str = "cuda"
) -> Dict[str, Any]:
    """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
    if not filepath.exists():
        raise FileNotFoundError(f"ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}")
    
    checkpoint = torch.load(filepath, map_location=device)
    
    # ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë¡œë“œ
    if optimizer and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ë¡œë“œ
    if scheduler and 'scheduler_state_dict' in checkpoint:
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
    
    print(f"ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {filepath}")
    print(f"   ì—í¬í¬: {checkpoint.get('epoch', 'N/A')}")
    print(f"   ìµœê³  ì ìˆ˜: {checkpoint.get('best_score', 'N/A')}")
    
    return checkpoint


def calculate_model_size(model: torch.nn.Module) -> Dict[str, Any]:
    """ëª¨ë¸ í¬ê¸° ê³„ì‚°"""
    param_size = 0
    param_sum = 0
    
    for param in model.parameters():
        param_size += param.nelement() * param.element_size()
        param_sum += param.nelement()
    
    buffer_size = 0
    buffer_sum = 0
    
    for buffer in model.buffers():
        buffer_size += buffer.nelement() * buffer.element_size()
        buffer_sum += buffer.nelement()
    
    all_size = (param_size + buffer_size) / 1024 / 1024
    
    return {
        "total_params": param_sum,
        "total_buffers": buffer_sum,
        "model_size_mb": all_size,
        "param_size_mb": param_size / 1024 / 1024,
        "buffer_size_mb": buffer_size / 1024 / 1024
    }


def print_model_summary(model: torch.nn.Module, input_size: tuple = (3, 512, 512)):
    """ëª¨ë¸ ìš”ì•½ ì¶œë ¥"""
    model_stats = calculate_model_size(model)
    
    print(f"\nğŸ§  ëª¨ë¸ ìš”ì•½:")
    print(f"   ì´ íŒŒë¼ë¯¸í„°: {model_stats['total_params']:,}")
    print(f"   ëª¨ë¸ í¬ê¸°: {model_stats['model_size_mb']:.1f} MB")
    
    # ì…ë ¥ í¬ê¸°ë¡œ í…ŒìŠ¤íŠ¸
    try:
        dummy_input = torch.randn(1, *input_size)
        with torch.no_grad():
            output = model(dummy_input)
        print(f"   ì…ë ¥ í¬ê¸°: {input_size}")
        print(f"   ì¶œë ¥ í¬ê¸°: {output.shape[1:]}")
    except Exception as e:
        print(f"   âš ï¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")


def optimize_gpu_settings():
    """GPU ìµœì í™” ì„¤ì •"""
    if torch.cuda.is_available():
        # ë©”ëª¨ë¦¬ í• ë‹¹ ìµœì í™”
        torch.backends.cudnn.benchmark = True
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        torch.cuda.empty_cache()
        
        print("âš¡ GPU ìµœì í™” ì„¤ì • ì™„ë£Œ")
        return True
    return False


def save_experiment_results(
    results: Dict[str, Any],
    output_dir: Path,
    experiment_name: str
):
    """ì‹¤í—˜ ê²°ê³¼ ì €ì¥"""
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # JSON í˜•íƒœë¡œ ì €ì¥
    results_file = output_dir / f"{experiment_name}_results.json"
    
    # Numpy arraysë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    serializable_results = {}
    for key, value in results.items():
        if isinstance(value, np.ndarray):
            serializable_results[key] = value.tolist()
        elif isinstance(value, torch.Tensor):
            serializable_results[key] = value.cpu().numpy().tolist()
        else:
            serializable_results[key] = value
    
    with open(results_file, 'w') as f:
        json.dump(serializable_results, f, indent=2, default=str)
    
    print(f"ğŸ’¾ ì‹¤í—˜ ê²°ê³¼ ì €ì¥: {results_file}")


def create_submission(
    predictions: np.ndarray,
    sample_submission_path: Path,
    output_path: Path,
    experiment_name: str
):
    """ì œì¶œ íŒŒì¼ ìƒì„±"""
    import pandas as pd
    
    # ìƒ˜í”Œ ì œì¶œ íŒŒì¼ ë¡œë“œ
    sample_df = pd.read_csv(sample_submission_path)
    
    # ì˜ˆì¸¡ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸
    sample_df['target'] = predictions
    
    # ì €ì¥
    submission_file = output_path / f"{experiment_name}_submission.csv"
    sample_df.to_csv(submission_file, index=False)
    
    print(f"ğŸ“¤ ì œì¶œ íŒŒì¼ ìƒì„±: {submission_file}")
    
    # ì˜ˆì¸¡ ë¶„í¬ ì¶œë ¥
    unique, counts = np.unique(predictions, return_counts=True)
    print(f"ğŸ“Š ì˜ˆì¸¡ ë¶„í¬:")
    for class_id, count in zip(unique, counts):
        percentage = count / len(predictions) * 100
        print(f"   í´ë˜ìŠ¤ {class_id:2d}: {count:4d}ê°œ ({percentage:5.1f}%)")
    
    return submission_file


def log_system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ ë¡œê¹…"""
    import platform
    import psutil
    
    print(f"\nğŸ’» ì‹œìŠ¤í…œ ì •ë³´:")
    print(f"   OS: {platform.system()} {platform.release()}")
    print(f"   Python: {platform.python_version()}")
    print(f"   PyTorch: {torch.__version__}")
    
    # CPU ì •ë³´
    print(f"   CPU: {psutil.cpu_count()} cores")
    print(f"   RAM: {psutil.virtual_memory().total / 1024**3:.1f} GB")
    
    # GPU ì •ë³´
    gpu_info = get_gpu_info()
    
    return {
        "os": platform.system(),
        "python_version": platform.python_version(),
        "pytorch_version": torch.__version__,
        "cpu_cores": psutil.cpu_count(),
        "ram_gb": psutil.virtual_memory().total / 1024**3,
        "gpu_info": gpu_info
    }


class EarlyStopping:
    """Early Stopping í—¬í¼"""
    
    def __init__(
        self,
        patience: int = 7,
        min_delta: float = 0.0,
        mode: str = 'max',
        verbose: bool = True
    ):
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.verbose = verbose
        
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        
        if mode == 'min':
            self.val_score = np.Inf
        else:
            self.val_score = -np.Inf
    
    def __call__(self, val_score: float) -> bool:
        if self.mode == 'min':
            score = -val_score
        else:
            score = val_score
        
        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint = True
        elif score < self.best_score + self.min_delta:
            self.counter += 1
            self.save_checkpoint = False
            
            if self.verbose:
                print(f"EarlyStopping counter: {self.counter} out of {self.patience}")
            
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint = True
            self.counter = 0
        
        return self.early_stop


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    print("ğŸ› ï¸ ìœ í‹¸ë¦¬í‹° í…ŒìŠ¤íŠ¸:")
    
    # ì‹œë“œ ì„¤ì •
    set_seed(42)
    
    # ì‹œìŠ¤í…œ ì •ë³´
    system_info = log_system_info()
    
    # GPU ìµœì í™”
    optimize_gpu_settings()
    
    # íƒ€ì´ë¨¸ í…ŒìŠ¤íŠ¸
    timer = Timer()
    timer.start()
    time.sleep(0.1)
    elapsed = timer.stop()
    print(f"â±ï¸ íƒ€ì´ë¨¸ í…ŒìŠ¤íŠ¸: {elapsed:.3f}ì´ˆ")
    
    print("âœ… ìœ í‹¸ë¦¬í‹° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

