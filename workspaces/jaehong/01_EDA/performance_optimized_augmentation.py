"""
ì„±ëŠ¥ ìµœì í™”ì— íŠ¹í™”ëœ ê³ ê¸‰ ì¦ê°• ì „ëµ
Test ë°ì´í„° í†µê³„ë¥¼ ì ê·¹ í™œìš©í•œ íƒ€ê²ŸíŒ… ì¦ê°•

í•µì‹¬ ì›ë¦¬:
1. Test í†µê³„ ì§ì ‘ í™œìš© - ìµœëŒ€ ì„±ëŠ¥ ì¶”êµ¬
2. Trainâ†’Test ë¶„í¬ ì°¨ì´ ì ê·¹ ë³´ì •
3. êµ¬ì²´ì  ìˆ˜ì¹˜ ê¸°ë°˜ íŒŒë¼ë¯¸í„° ìµœì í™”
4. Adaptive Domain Adaptation
"""

import cv2
import numpy as np
import albumentations as A
from albumentations.pytorch import ToTensorV2
from typing import Dict, List, Tuple, Optional
import torch

class PerformanceOptimizedAugmentation:
    """
    ì„±ëŠ¥ ìµœì í™”ì— íŠ¹í™”ëœ ì¦ê°• ì „ëµ
    EDAì—ì„œ ë°œê²¬í•œ Train/Test ì°¨ì´ë¥¼ ì ê·¹ í™œìš©
    """
    
    def __init__(self, eda_results: Dict):
        """
        Args:
            eda_results: EDAì—ì„œ ë¶„ì„í•œ Train/Test í†µê³„ ì°¨ì´
        """
        self.eda_results = eda_results
        
        # EDA ê²°ê³¼ì—ì„œ í•µì‹¬ í†µê³„ ì¶”ì¶œ
        self.brightness_diff = 24.0    # Testê°€ 24.0 ë” ë°ìŒ
        self.sharpness_ratio = 1.97    # Trainì´ 1.97ë°° ë” ì„ ëª…
        self.noise_diff = -2.92        # Testê°€ 2.92 ë” ì ì€ ë…¸ì´ì¦ˆ
        self.contrast_diff = 1.46      # Testê°€ 1.46 ë” ë†’ì€ ëŒ€ë¹„
        
        print(f"ğŸ¯ ì„±ëŠ¥ ìµœì í™” ì¦ê°• ì´ˆê¸°í™”:")
        print(f"  â€¢ ë°ê¸° ì°¨ì´: +{self.brightness_diff}")
        print(f"  â€¢ ì„ ëª…ë„ ë¹„ìœ¨: {self.sharpness_ratio:.1f}ë°°")
        print(f"  â€¢ ë…¸ì´ì¦ˆ ì°¨ì´: {self.noise_diff}")
        print(f"  â€¢ ëŒ€ë¹„ ì°¨ì´: +{self.contrast_diff}")
    
    def get_test_targeted_transforms(self, image_size: int = 512, phase: str = "aggressive"):
        """
        Test ë°ì´í„° íƒ€ê²ŸíŒ… ì¦ê°•
        
        Args:
            phase: "conservative", "moderate", "aggressive"
        """
        
        if phase == "conservative":
            return self._get_conservative_transforms(image_size)
        elif phase == "moderate":
            return self._get_moderate_transforms(image_size)
        else:  # aggressive
            return self._get_aggressive_transforms(image_size)
    
    def _get_aggressive_transforms(self, image_size: int):
        """ê³µê²©ì  Test íƒ€ê²ŸíŒ… (ìµœëŒ€ ì„±ëŠ¥ ì¶”êµ¬)"""
        
        # Test í†µê³„ ê¸°ë°˜ íŒŒë¼ë¯¸í„° ê³„ì‚°
        brightness_limit = min(0.5, self.brightness_diff / 100)  # 0.24
        contrast_limit = min(0.4, self.contrast_diff / 50)       # 0.029 -> 0.4
        blur_limit = int(min(10, self.sharpness_ratio * 2))      # 3.94 -> 3
        noise_var = int(max(10, 50 - abs(self.noise_diff) * 5))  # 35.4 -> 35
        
        return A.Compose([
            A.Resize(image_size, image_size),
            
            # ë°ê¸° íƒ€ê²ŸíŒ… (Testê°€ ë” ë°ìŒ)
            A.RandomBrightnessContrast(
                brightness_limit=brightness_limit,  # Test ì°¨ì´ ê¸°ë°˜
                contrast_limit=0.4,  # ê°•í™”ëœ ëŒ€ë¹„
                brightness_by_max=True,  # ë°ê¸° ìš°ì„ 
                p=0.9  # ë†’ì€ í™•ë¥ 
            ),
            
            # ì„ ëª…ë„ íƒ€ê²ŸíŒ… (Testê°€ ë” íë¦¼)
            A.OneOf([
                A.MotionBlur(blur_limit=blur_limit),
                A.GaussianBlur(blur_limit=blur_limit),
                A.MedianBlur(blur_limit=min(blur_limit, 7)),
            ], p=0.7),  # Test ëŒ€ì‘ ë†’ì€ í™•ë¥ 
            
            # ë…¸ì´ì¦ˆ íƒ€ê²ŸíŒ… (Testê°€ ë” ê¹¨ë—í•¨)
            A.OneOf([
                A.GaussNoise(var_limit=(5, noise_var)),  # ì ë‹¹í•œ ë…¸ì´ì¦ˆ
                A.MultiplicativeNoise(multiplier=[0.9, 1.1]),
                A.ISONoise(color_shift=(0.01, 0.03), intensity=(0.1, 0.3)),
            ], p=0.4),  # Testê°€ ê¹¨ë—í•˜ë¯€ë¡œ ì ë‹¹í•œ í™•ë¥ 
            
            # íšŒì „ ê°•í™” (ì¼ë°˜ì ìœ¼ë¡œ Testì—ì„œ ë‹¤ì–‘í•¨)
            A.RandomRotate90(p=0.4),
            A.Rotate(limit=45, p=0.8, border_mode=cv2.BORDER_CONSTANT),
            
            # Test í™˜ê²½ ì‹œë®¬ë ˆì´ì…˜
            A.Perspective(scale=(0.05, 0.15), p=0.5),
            A.ShiftScaleRotate(
                shift_limit=0.1, 
                scale_limit=0.1, 
                rotate_limit=0, 
                p=0.4
            ),
            
            # ìƒ‰ìƒ ì¡°ì • (Test íŠ¹ì„± ë°˜ì˜)
            A.OneOf([
                A.HueSaturationValue(
                    hue_shift_limit=10, 
                    sat_shift_limit=20, 
                    val_shift_limit=int(self.brightness_diff)
                ),
                A.ColorJitter(
                    brightness=brightness_limit,
                    contrast=0.3,
                    saturation=0.2,
                    hue=0.1
                ),
            ], p=0.6),
            
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2()
        ])
    
    def _get_moderate_transforms(self, image_size: int):
        """ì¤‘ê°„ ìˆ˜ì¤€ Test íƒ€ê²ŸíŒ…"""
        
        brightness_limit = min(0.3, self.brightness_diff / 150)
        blur_limit = int(min(5, self.sharpness_ratio))
        
        return A.Compose([
            A.Resize(image_size, image_size),
            
            A.RandomBrightnessContrast(
                brightness_limit=brightness_limit,
                contrast_limit=0.3,
                p=0.8
            ),
            
            A.OneOf([
                A.MotionBlur(blur_limit=blur_limit),
                A.GaussianBlur(blur_limit=blur_limit),
            ], p=0.5),
            
            A.RandomRotate90(p=0.3),
            A.Rotate(limit=30, p=0.7, border_mode=cv2.BORDER_CONSTANT),
            
            A.OneOf([
                A.GaussNoise(var_limit=(10, 40)),
                A.MultiplicativeNoise(multiplier=[0.9, 1.1]),
            ], p=0.3),
            
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2()
        ])
    
    def _get_conservative_transforms(self, image_size: int):
        """ë³´ìˆ˜ì  ì¦ê°• (ì¼ë°˜ì  ë²”ìœ„ + ì•½ê°„ì˜ Test ê³ ë ¤)"""
        
        return A.Compose([
            A.Resize(image_size, image_size),
            
            A.RandomBrightnessContrast(
                brightness_limit=0.2,  # ë³´ìˆ˜ì 
                contrast_limit=0.2,
                p=0.7
            ),
            
            A.OneOf([
                A.MotionBlur(blur_limit=3),
                A.GaussianBlur(blur_limit=3),
            ], p=0.3),
            
            A.RandomRotate90(p=0.3),
            A.Rotate(limit=15, p=0.6, border_mode=cv2.BORDER_CONSTANT),
            
            A.GaussNoise(var_limit=(10, 30), p=0.2),
            
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2()
        ])

class AdaptiveDomainAlignment:
    """
    ì ì‘ì  ë„ë©”ì¸ ì •ë ¬ - Trainì„ Test ë¶„í¬ì— ë§ì¶¤
    """
    
    def __init__(self, target_stats: Dict):
        """
        Args:
            target_stats: Test ë°ì´í„° ëª©í‘œ í†µê³„
        """
        self.target_brightness = target_stats.get('brightness', 172.2)
        self.target_sharpness = target_stats.get('sharpness', 688.3)
        self.target_noise = target_stats.get('noise', 7.3)
        self.target_contrast = target_stats.get('contrast', 49.0)
    
    def get_domain_aligned_transforms(self, 
                                    current_epoch: int,
                                    total_epochs: int,
                                    image_size: int = 512):
        """
        ì—í¬í¬ì— ë”°ë¥¸ ì ì§„ì  ë„ë©”ì¸ ì •ë ¬
        ì´ˆê¸°: ì¼ë°˜ì  ì¦ê°• â†’ í›„ê¸°: Test ë¶„í¬ íƒ€ê²ŸíŒ…
        """
        
        # ì§„í–‰ë¥  ê³„ì‚° (0.0 ~ 1.0)
        progress = current_epoch / total_epochs
        
        # ì ì§„ì  ê°•ë„ ì¡°ì ˆ
        brightness_strength = 0.2 + (progress * 0.3)  # 0.2 â†’ 0.5
        blur_strength = 2 + (progress * 5)             # 2 â†’ 7
        alignment_prob = 0.3 + (progress * 0.4)        # 0.3 â†’ 0.7
        
        transforms = [A.Resize(image_size, image_size)]
        
        # ë°ê¸° ì •ë ¬ (ì ì§„ì  ê°•í™”)
        transforms.append(
            A.RandomBrightnessContrast(
                brightness_limit=brightness_strength,
                contrast_limit=0.3,
                p=alignment_prob
            )
        )
        
        # ì„ ëª…ë„ ì •ë ¬ (ì ì§„ì  ë¸”ëŸ¬ ì¦ê°€)
        if progress > 0.3:  # 30% ì´í›„ë¶€í„° ì ìš©
            transforms.append(
                A.OneOf([
                    A.MotionBlur(blur_limit=int(blur_strength)),
                    A.GaussianBlur(blur_limit=int(blur_strength)),
                ], p=alignment_prob * 0.8)
            )
        
        # ë…¸ì´ì¦ˆ ì •ë ¬ (Testê°€ ë” ê¹¨ë—í•˜ë¯€ë¡œ ê°ì†Œ)
        if progress > 0.5:  # 50% ì´í›„ë¶€í„° ì ìš©
            noise_strength = max(10, 40 - progress * 20)  # 40 â†’ 20
            transforms.append(
                A.GaussNoise(var_limit=(5, int(noise_strength)), p=0.3)
            )
        
        # ê¸°ë³¸ ì¦ê°•
        transforms.extend([
            A.RandomRotate90(p=0.3),
            A.Rotate(limit=30, p=0.7, border_mode=cv2.BORDER_CONSTANT),
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2()
        ])
        
        return A.Compose(transforms)

class TestDistributionMatcher:
    """
    Test ë¶„í¬ ë§¤ì¹­ - íˆìŠ¤í† ê·¸ë¨ ë§¤ì¹­ ë“± ê³ ê¸‰ ê¸°ë²•
    """
    
    def __init__(self):
        self.reference_stats = None
    
    def set_reference_distribution(self, test_images: List[np.ndarray]):
        """Test ì´ë¯¸ì§€ë“¤ë¡œë¶€í„° ì°¸ì¡° ë¶„í¬ ì„¤ì •"""
        
        # Test ì´ë¯¸ì§€ë“¤ì˜ í†µê³„ ê³„ì‚°
        brightnesses = []
        contrasts = []
        
        for img in test_images[:100]:  # ìƒ˜í”Œë§
            if len(img.shape) == 3:
                gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
            else:
                gray = img
                
            brightnesses.append(np.mean(gray))
            contrasts.append(np.std(gray))
        
        self.reference_stats = {
            'brightness_mean': np.mean(brightnesses),
            'brightness_std': np.std(brightnesses),
            'contrast_mean': np.mean(contrasts),
            'contrast_std': np.std(contrasts)
        }
    
    def get_distribution_matching_transforms(self, image_size: int = 512):
        """ë¶„í¬ ë§¤ì¹­ ë³€í™˜ ë°˜í™˜"""
        
        if self.reference_stats is None:
            raise ValueError("Reference distribution not set!")
        
        return A.Compose([
            A.Resize(image_size, image_size),
            
            # íˆìŠ¤í† ê·¸ë¨ ë§¤ì¹­ (ê·¼ì‚¬)
            A.RandomBrightnessContrast(
                brightness_limit=0.4,
                contrast_limit=0.4,
                p=0.8
            ),
            
            # ì¶”ê°€ ì •ê·œí™”
            A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.3),
            
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2()
        ])

# ì‚¬ìš© ì˜ˆì‹œ
def get_performance_optimized_strategy():
    """ì„±ëŠ¥ ìµœì í™” ì „ëµ ë°˜í™˜"""
    
    # EDA ê²°ê³¼ (ì‹¤ì œ ê°’ë“¤)
    eda_results = {
        'brightness_diff': 24.0,
        'sharpness_ratio': 1.97,
        'noise_diff': -2.92,
        'contrast_diff': 1.46
    }
    
    # ì„±ëŠ¥ ìµœì í™” ì¦ê°•
    perf_aug = PerformanceOptimizedAugmentation(eda_results)
    
    return {
        'aggressive': perf_aug.get_test_targeted_transforms(phase="aggressive"),
        'moderate': perf_aug.get_test_targeted_transforms(phase="moderate"),
        'conservative': perf_aug.get_test_targeted_transforms(phase="conservative")
    }

if __name__ == "__main__":
    print("ğŸš€ ì„±ëŠ¥ ìµœì í™” ì¦ê°• ì „ëµ")
    print("=" * 50)
    
    strategies = get_performance_optimized_strategy()
    
    for name, strategy in strategies.items():
        print(f"âœ… {name.upper()} ì „ëµ: {len(strategy.transforms)}ê°œ ë³€í™˜")
    
    print("\nğŸ¯ í•µì‹¬ ì›ë¦¬:")
    print("  â€¢ Test í†µê³„ ì§ì ‘ í™œìš©")
    print("  â€¢ Trainâ†’Test ë¶„í¬ ì°¨ì´ ì ê·¹ ë³´ì •") 
    print("  â€¢ êµ¬ì²´ì  ìˆ˜ì¹˜ ê¸°ë°˜ íŒŒë¼ë¯¸í„° ìµœì í™”")
    print("  â€¢ ìµœëŒ€ ì„±ëŠ¥ ì¶”êµ¬!")
