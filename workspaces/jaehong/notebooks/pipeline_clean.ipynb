{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìÑ Document Classification Pipeline (Clean Version)\n",
        "\n",
        "## EDA Í∏∞Î∞ò ÏµúÏ†ÅÌôîÎêú Î¨∏ÏÑú Î∂ÑÎ•ò ÌååÏù¥ÌîÑÎùºÏù∏\n",
        "\n",
        "### Contents\n",
        "1. **Setup & Configuration** - ÎùºÏù¥Î∏åÎü¨Î¶¨, ÏÑ§Ï†ï, ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞\n",
        "2. **Data Processing** - Îç∞Ïù¥ÌÑ∞ÏÖã, Ï†ÑÏ≤òÎ¶¨, Ï¶ùÍ∞ï\n",
        "3. **Model Training** - Î™®Îç∏ ÌïôÏäµ Î∞è Í≤ÄÏ¶ù\n",
        "4. **Inference & Submission** - Ï∂îÎ°† Î∞è Í≤∞Í≥º Ï†ÄÏû•\n",
        "\n",
        "### Key Features\n",
        "- ‚úÖ EDA Í∏∞Î∞ò ÌÅ¥ÎûòÏä§ Î∂àÍ∑†Ìòï Ìï¥Í≤∞\n",
        "- ‚úÖ Train/Test ÎèÑÎ©îÏù∏ Ï∞®Ïù¥ ÎåÄÏùë\n",
        "- ‚úÖ ÌÅ¥ÎûòÏä§Î≥Ñ ÎßûÏ∂§ Ï†ÑÏ≤òÎ¶¨\n",
        "- ‚úÖ Ïò§Î≤ÑÌîºÌåÖ Î∞©ÏßÄ (Train/Val Î∂ÑÌï†)\n",
        "- ‚úÖ Test Time Augmentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Core Libraries\n",
        "import os\n",
        "import random\n",
        "import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Deep Learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Data Processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# ML Utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Experiment Tracking\n",
        "import wandb\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Configuration loaded - Device: cuda\n",
            "   - Model: efficientnet_b0\n",
            "   - Image Size: 512x512\n",
            "   - Batch Size: 16\n",
            "   - Learning Rate: 0.0001\n",
            "   - Use Class Weights: True\n"
          ]
        }
      ],
      "source": [
        "# Configuration Class\n",
        "class Config:\n",
        "    # Paths\n",
        "    DATA_PATH = \"/home/dev/computervisioncompetition-cv3/data\"\n",
        "    \n",
        "    # Model Settings\n",
        "    MODEL_NAME = 'efficientnet_b0'\n",
        "    IMG_SIZE = 512\n",
        "    NUM_CLASSES = 17\n",
        "    \n",
        "    # Training Settings\n",
        "    BATCH_SIZE = 16\n",
        "    EPOCHS = 50\n",
        "    LEARNING_RATE = 1e-4\n",
        "    WEIGHT_DECAY = 1e-2\n",
        "    \n",
        "    # Data Settings\n",
        "    VAL_RATIO = 0.2\n",
        "    NUM_WORKERS = 4\n",
        "    \n",
        "    # EDA-based Settings\n",
        "    USE_CLASS_WEIGHTS = True\n",
        "    USE_CLASS_SPECIFIC_AUG = True\n",
        "    EARLY_STOPPING_PATIENCE = 5\n",
        "    \n",
        "    # Random Seed\n",
        "    SEED = 42\n",
        "    \n",
        "    # Device\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# EDA-based Class Information\n",
        "CLASS_WEIGHTS = {\n",
        "    0: 0.92, 1: 2.01, 2: 0.92, 3: 0.92, 4: 0.92, 5: 0.92, 6: 0.92, 7: 0.92, 8: 0.92,\n",
        "    9: 0.92, 10: 0.92, 11: 0.92, 12: 0.92, 13: 1.25, 14: 1.85, 15: 0.92, 16: 0.92\n",
        "}\n",
        "\n",
        "VEHICLE_CLASSES = [2, 16]  # car_dashboard, vehicle_registration_plate\n",
        "MINORITY_CLASSES = [1, 13, 14]  # Underrepresented classes\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"Fix random seeds for reproducibility\"\"\"\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def get_class_weights_tensor():\n",
        "    \"\"\"Convert class weights to tensor\"\"\"\n",
        "    weights = [CLASS_WEIGHTS[i] for i in range(Config.NUM_CLASSES)]\n",
        "    return torch.FloatTensor(weights)\n",
        "\n",
        "# Initialize\n",
        "set_seed(Config.SEED)\n",
        "print(f\"üöÄ Configuration loaded - Device: {Config.DEVICE}\")\n",
        "print(f\"   - Model: {Config.MODEL_NAME}\")\n",
        "print(f\"   - Image Size: {Config.IMG_SIZE}x{Config.IMG_SIZE}\")\n",
        "print(f\"   - Batch Size: {Config.BATCH_SIZE}\")\n",
        "print(f\"   - Learning Rate: {Config.LEARNING_RATE}\")\n",
        "print(f\"   - Use Class Weights: {Config.USE_CLASS_WEIGHTS}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Data transforms defined\n"
          ]
        }
      ],
      "source": [
        "# Data Transforms\n",
        "def get_transforms(mode='train', image_size=512):\n",
        "    \"\"\"Get data transforms based on mode\"\"\"\n",
        "    \n",
        "    if mode == 'train':\n",
        "        return A.Compose([\n",
        "            # Basic preprocessing\n",
        "            A.LongestMaxSize(max_size=image_size, interpolation=cv2.INTER_AREA),\n",
        "            A.PadIfNeeded(image_size, image_size, border_mode=cv2.BORDER_CONSTANT, value=[255, 255, 255]),\n",
        "            \n",
        "            # EDA-based augmentations\n",
        "            A.RandomRotate90(p=0.3),\n",
        "            A.Rotate(limit=30, p=0.7, border_mode=cv2.BORDER_CONSTANT, value=[255, 255, 255]),\n",
        "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.8),\n",
        "            \n",
        "            # Noise and blur (Test data characteristics)\n",
        "            A.OneOf([\n",
        "                A.GaussNoise(var_limit=(10, 50)),\n",
        "                A.MultiplicativeNoise(multiplier=[0.9, 1.1], elementwise=True),\n",
        "            ], p=0.5),\n",
        "            A.OneOf([\n",
        "                A.MotionBlur(blur_limit=3),\n",
        "                A.GaussianBlur(blur_limit=3),\n",
        "            ], p=0.3),\n",
        "            \n",
        "            # Geometric transforms\n",
        "            A.Perspective(scale=(0.05, 0.1), p=0.3),\n",
        "            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=0, p=0.3,\n",
        "                             border_mode=cv2.BORDER_CONSTANT, value=[255, 255, 255]),\n",
        "            \n",
        "            # Document-specific augmentations\n",
        "            A.CoarseDropout(max_holes=1, max_height=32, max_width=32, fill_value=255, p=0.3),\n",
        "            A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.5),\n",
        "            \n",
        "            # Normalization\n",
        "            A.Normalize(mean=[0.57, 0.58, 0.59], std=[0.24, 0.24, 0.24]),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "    \n",
        "    else:  # validation/test\n",
        "        return A.Compose([\n",
        "            A.LongestMaxSize(max_size=image_size, interpolation=cv2.INTER_AREA),\n",
        "            A.PadIfNeeded(image_size, image_size, border_mode=cv2.BORDER_CONSTANT, value=[255, 255, 255]),\n",
        "            A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.3),\n",
        "            A.Normalize(mean=[0.57, 0.58, 0.59], std=[0.24, 0.24, 0.24]),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "def get_tta_transforms(image_size=512):\n",
        "    \"\"\"Get Test Time Augmentation transforms\"\"\"\n",
        "    transforms = []\n",
        "    \n",
        "    # Base transform\n",
        "    transforms.append(get_transforms('test', image_size))\n",
        "    \n",
        "    # Horizontal flip\n",
        "    transforms.append(A.Compose([\n",
        "        A.LongestMaxSize(max_size=image_size, interpolation=cv2.INTER_AREA),\n",
        "        A.PadIfNeeded(image_size, image_size, border_mode=cv2.BORDER_CONSTANT, value=[255, 255, 255]),\n",
        "        A.HorizontalFlip(p=1.0),\n",
        "        A.Normalize(mean=[0.57, 0.58, 0.59], std=[0.24, 0.24, 0.24]),\n",
        "        ToTensorV2()\n",
        "    ]))\n",
        "    \n",
        "    return transforms\n",
        "\n",
        "print(\"‚úÖ Data transforms defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Dataset class defined\n"
          ]
        }
      ],
      "source": [
        "# Dataset Class\n",
        "class DocumentDataset(Dataset):\n",
        "    \"\"\"Enhanced dataset with class-specific preprocessing\"\"\"\n",
        "    \n",
        "    def __init__(self, df, image_dir, transform=None, is_train=True):\n",
        "        if isinstance(df, str):\n",
        "            self.df = pd.read_csv(df)\n",
        "        else:\n",
        "            self.df = df.reset_index(drop=True)\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.is_train = is_train\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        name = row['ID'] if 'ID' in row else row.iloc[0]\n",
        "        target = row['target'] if 'target' in row else row.iloc[1]\n",
        "        \n",
        "        # Load and convert to RGB\n",
        "        img_path = os.path.join(self.image_dir, name)\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            img = np.array(Image.open(img_path))\n",
        "        else:\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        # Class-specific preprocessing (only for training)\n",
        "        if self.is_train and Config.USE_CLASS_SPECIFIC_AUG:\n",
        "            if target in VEHICLE_CLASSES:\n",
        "                img = cv2.convertScaleAbs(img, alpha=1.1, beta=10)\n",
        "            elif target in MINORITY_CLASSES:\n",
        "                img = cv2.bilateralFilter(img, 9, 75, 75)\n",
        "        \n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            img = self.transform(image=img)['image']\n",
        "            \n",
        "        return img, target\n",
        "\n",
        "print(\"‚úÖ Dataset class defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Data loaded:\n",
            "   - Train: 1,256 samples (78 batches)\n",
            "   - Val: 314 samples (20 batches)\n",
            "   - Test: 3,140 samples (197 batches)\n",
            "\n",
            "üîç Sample batch:\n",
            "   - Image shape: torch.Size([16, 3, 512, 512])\n",
            "   - Image range: [-2.458, 1.792]\n",
            "   - Label shape: torch.Size([16])\n"
          ]
        }
      ],
      "source": [
        "# Data Loading\n",
        "def create_dataloaders(config):\n",
        "    \"\"\"Create train, validation, and test dataloaders\"\"\"\n",
        "    # Load and split data\n",
        "    full_df = pd.read_csv(os.path.join(config.DATA_PATH, \"train.csv\"))\n",
        "    train_df, val_df = train_test_split(\n",
        "        full_df, test_size=config.VAL_RATIO, \n",
        "        stratify=full_df['target'], random_state=config.SEED\n",
        "    )\n",
        "    \n",
        "    # Create datasets\n",
        "    train_dataset = DocumentDataset(\n",
        "        train_df, os.path.join(config.DATA_PATH, \"train\"),\n",
        "        get_transforms('train', config.IMG_SIZE), is_train=True\n",
        "    )\n",
        "    val_dataset = DocumentDataset(\n",
        "        val_df, os.path.join(config.DATA_PATH, \"train\"),\n",
        "        get_transforms('val', config.IMG_SIZE), is_train=False\n",
        "    )\n",
        "    test_dataset = DocumentDataset(\n",
        "        os.path.join(config.DATA_PATH, \"sample_submission.csv\"),\n",
        "        os.path.join(config.DATA_PATH, \"test\"),\n",
        "        get_transforms('test', config.IMG_SIZE), is_train=False\n",
        "    )\n",
        "    \n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=config.BATCH_SIZE, shuffle=True,\n",
        "        num_workers=config.NUM_WORKERS, pin_memory=True, drop_last=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=config.BATCH_SIZE, shuffle=False,\n",
        "        num_workers=config.NUM_WORKERS, pin_memory=True, drop_last=False\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=config.BATCH_SIZE, shuffle=False,\n",
        "        num_workers=config.NUM_WORKERS, pin_memory=True, drop_last=False\n",
        "    )\n",
        "    \n",
        "    return train_loader, val_loader, test_loader, train_df, val_df\n",
        "\n",
        "# Create data loaders\n",
        "train_loader, val_loader, test_loader, train_df, val_df = create_dataloaders(Config)\n",
        "\n",
        "print(f\"üìä Data loaded:\")\n",
        "print(f\"   - Train: {len(train_df):,} samples ({len(train_loader)} batches)\")\n",
        "print(f\"   - Val: {len(val_df):,} samples ({len(val_loader)} batches)\")\n",
        "print(f\"   - Test: {len(test_loader.dataset):,} samples ({len(test_loader)} batches)\")\n",
        "\n",
        "# Sample batch verification\n",
        "sample_batch = next(iter(train_loader))\n",
        "print(f\"\\nüîç Sample batch:\")\n",
        "print(f\"   - Image shape: {sample_batch[0].shape}\")\n",
        "print(f\"   - Image range: [{sample_batch[0].min():.3f}, {sample_batch[0].max():.3f}]\")\n",
        "print(f\"   - Label shape: {sample_batch[1].shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Using weighted CrossEntropyLoss\n",
            "ü§ñ Model initialized:\n",
            "   - Parameters: 4,029,325\n"
          ]
        }
      ],
      "source": [
        "# Model Components\n",
        "def create_model(config):\n",
        "    \"\"\"Create and configure model\"\"\"\n",
        "    model = timm.create_model(\n",
        "        config.MODEL_NAME,\n",
        "        pretrained=True,\n",
        "        num_classes=config.NUM_CLASSES,\n",
        "        drop_rate=0.3\n",
        "    ).to(config.DEVICE)\n",
        "    return model\n",
        "\n",
        "def create_loss_function(config):\n",
        "    \"\"\"Create loss function with optional class weights\"\"\"\n",
        "    if config.USE_CLASS_WEIGHTS:\n",
        "        class_weights = get_class_weights_tensor().to(config.DEVICE)\n",
        "        loss_fn = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n",
        "        print(\"‚úÖ Using weighted CrossEntropyLoss\")\n",
        "    else:\n",
        "        loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "        print(\"‚úÖ Using standard CrossEntropyLoss\")\n",
        "    return loss_fn\n",
        "\n",
        "def create_optimizer_scheduler(model, config):\n",
        "    \"\"\"Create optimizer and scheduler\"\"\"\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "        optimizer, T_0=10, T_mult=2\n",
        "    )\n",
        "    return optimizer, scheduler\n",
        "\n",
        "# Initialize model components\n",
        "model = create_model(Config)\n",
        "loss_fn = create_loss_function(Config)\n",
        "optimizer, scheduler = create_optimizer_scheduler(model, Config)\n",
        "\n",
        "print(f\"ü§ñ Model initialized:\")\n",
        "print(f\"   - Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Training functions defined\n"
          ]
        }
      ],
      "source": [
        "# Training Functions\n",
        "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds, all_targets = [], []\n",
        "\n",
        "    pbar = tqdm(loader, desc=\"üèÉ Training\")\n",
        "    for images, targets in pbar:\n",
        "        images, targets = images.to(device), targets.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        all_preds.extend(outputs.argmax(dim=1).detach().cpu().numpy())\n",
        "        all_targets.extend(targets.detach().cpu().numpy())\n",
        "        \n",
        "        pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    accuracy = accuracy_score(all_targets, all_preds)\n",
        "    f1 = f1_score(all_targets, all_preds, average='macro')\n",
        "    \n",
        "    return {'loss': avg_loss, 'accuracy': accuracy, 'f1': f1}\n",
        "\n",
        "def validate_one_epoch(loader, model, loss_fn, device):\n",
        "    \"\"\"Validate for one epoch\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds, all_targets = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(loader, desc=\"üîç Validation\", leave=False)\n",
        "        for images, targets in pbar:\n",
        "            images, targets = images.to(device), targets.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = loss_fn(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            all_preds.extend(outputs.argmax(dim=1).detach().cpu().numpy())\n",
        "            all_targets.extend(targets.detach().cpu().numpy())\n",
        "            \n",
        "            pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    accuracy = accuracy_score(all_targets, all_preds)\n",
        "    f1 = f1_score(all_targets, all_preds, average='macro', zero_division=0)\n",
        "    \n",
        "    return {'loss': avg_loss, 'accuracy': accuracy, 'f1': f1}\n",
        "\n",
        "print(\"‚úÖ Training functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvisioncraft_james\u001b[0m (\u001b[33mvisioncraft_james-open-university-of-korea\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "creating run (0.1s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/dev/computervisioncompetition-cv3/workspaces/jaehong/notebooks/wandb/run-20250908_023815-721d9m4x</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/visioncraft_james-open-university-of-korea/computervisioncompetition-cv3/runs/721d9m4x' target=\"_blank\">20250908_efficientnet_b0_clean_pipeline</a></strong> to <a href='https://wandb.ai/visioncraft_james-open-university-of-korea/computervisioncompetition-cv3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/visioncraft_james-open-university-of-korea/computervisioncompetition-cv3' target=\"_blank\">https://wandb.ai/visioncraft_james-open-university-of-korea/computervisioncompetition-cv3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/visioncraft_james-open-university-of-korea/computervisioncompetition-cv3/runs/721d9m4x' target=\"_blank\">https://wandb.ai/visioncraft_james-open-university-of-korea/computervisioncompetition-cv3/runs/721d9m4x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Wandb initialized\n"
          ]
        }
      ],
      "source": [
        "# Initialize Wandb\n",
        "wandb.init(\n",
        "    project=\"computervisioncompetition-cv3\",\n",
        "    name=f'{datetime.datetime.now().strftime(\"%Y%m%d\")}_{Config.MODEL_NAME}_clean_pipeline',\n",
        "    config={\n",
        "        \"model\": Config.MODEL_NAME,\n",
        "        \"epochs\": Config.EPOCHS,\n",
        "        \"batch_size\": Config.BATCH_SIZE,\n",
        "        \"learning_rate\": Config.LEARNING_RATE,\n",
        "        \"img_size\": Config.IMG_SIZE,\n",
        "        \"use_class_weights\": Config.USE_CLASS_WEIGHTS,\n",
        "        \"use_class_specific_aug\": Config.USE_CLASS_SPECIFIC_AUG,\n",
        "        \"train_samples\": len(train_df),\n",
        "        \"val_samples\": len(val_df),\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Wandb initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting training...\n",
            "   - Total epochs: 50\n",
            "   - Early stopping patience: 5\n",
            "   - Using class weights: True\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:17<00:00,  4.35it/s, Loss=1.5012]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  1/50 | Train F1: 0.3514 | Val F1: 0.7086 | LR: 0.000098 | üíæ Best!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  9.33it/s, Loss=1.3025]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  2/50 | Train F1: 0.6530 | Val F1: 0.7977 | LR: 0.000090 | üíæ Best!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  9.18it/s, Loss=0.8675]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  3/50 | Train F1: 0.7477 | Val F1: 0.8665 | LR: 0.000079 | üíæ Best!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  9.17it/s, Loss=1.3057]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  4/50 | Train F1: 0.8031 | Val F1: 0.8570 | LR: 0.000065\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  9.15it/s, Loss=0.9020]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  5/50 | Train F1: 0.8470 | Val F1: 0.8781 | LR: 0.000050 | üíæ Best!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  9.12it/s, Loss=0.9811]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  6/50 | Train F1: 0.8656 | Val F1: 0.8811 | LR: 0.000035 | üíæ Best!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  9.07it/s, Loss=0.8282]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  7/50 | Train F1: 0.8736 | Val F1: 0.8824 | LR: 0.000021 | üíæ Best!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  9.09it/s, Loss=0.8729]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  8/50 | Train F1: 0.8775 | Val F1: 0.8789 | LR: 0.000010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  9.03it/s, Loss=0.9592]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  9/50 | Train F1: 0.8963 | Val F1: 0.8963 | LR: 0.000002 | üíæ Best!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  8.94it/s, Loss=1.2880]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/50 | Train F1: 0.8887 | Val F1: 0.8861 | LR: 0.000100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  9.04it/s, Loss=1.1020]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/50 | Train F1: 0.8805 | Val F1: 0.9030 | LR: 0.000099 | üíæ Best!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  8.92it/s, Loss=0.7835]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/50 | Train F1: 0.8883 | Val F1: 0.9169 | LR: 0.000098 | üíæ Best!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  9.08it/s, Loss=0.8942]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/50 | Train F1: 0.9135 | Val F1: 0.9049 | LR: 0.000095\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  9.04it/s, Loss=0.8594]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14/50 | Train F1: 0.9211 | Val F1: 0.9104 | LR: 0.000090\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  9.03it/s, Loss=0.9901]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15/50 | Train F1: 0.9254 | Val F1: 0.9179 | LR: 0.000085 | üíæ Best!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  9.03it/s, Loss=0.7720]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16/50 | Train F1: 0.9336 | Val F1: 0.9128 | LR: 0.000079\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  9.06it/s, Loss=0.9497]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17/50 | Train F1: 0.9470 | Val F1: 0.9207 | LR: 0.000073 | üíæ Best!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  9.06it/s, Loss=0.7011]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18/50 | Train F1: 0.9477 | Val F1: 0.9279 | LR: 0.000065 | üíæ Best!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  8.99it/s, Loss=0.6903]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19/50 | Train F1: 0.9503 | Val F1: 0.9253 | LR: 0.000058\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  9.01it/s, Loss=0.7850]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20/50 | Train F1: 0.9690 | Val F1: 0.9324 | LR: 0.000050 | üíæ Best!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  8.95it/s, Loss=0.7511]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21/50 | Train F1: 0.9644 | Val F1: 0.9265 | LR: 0.000042\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  9.08it/s, Loss=0.8767]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22/50 | Train F1: 0.9663 | Val F1: 0.9216 | LR: 0.000035\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  8.95it/s, Loss=0.7246]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23/50 | Train F1: 0.9654 | Val F1: 0.9326 | LR: 0.000027 | üíæ Best!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  9.04it/s, Loss=0.9537]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24/50 | Train F1: 0.9684 | Val F1: 0.9330 | LR: 0.000021 | üíæ Best!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  8.90it/s, Loss=0.7839]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25/50 | Train F1: 0.9693 | Val F1: 0.9385 | LR: 0.000015 | üíæ Best!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  9.02it/s, Loss=0.6902]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26/50 | Train F1: 0.9773 | Val F1: 0.9350 | LR: 0.000010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  9.02it/s, Loss=0.7305]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27/50 | Train F1: 0.9669 | Val F1: 0.9380 | LR: 0.000005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  9.04it/s, Loss=0.6627]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28/50 | Train F1: 0.9716 | Val F1: 0.9350 | LR: 0.000002\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  8.98it/s, Loss=0.8226]\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29/50 | Train F1: 0.9645 | Val F1: 0.9289 | LR: 0.000001\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üèÉ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:08<00:00,  9.02it/s, Loss=0.7389]\n",
            "                                                                           "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üõë Early stopping! Best Val F1: 0.9385\n",
            "‚úÖ Training completed! Best Validation F1: 0.9385\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "# Í∞úÏÑ†Îêú Training Loop (Ïò§Î≤ÑÌîºÌåÖ Î∞©ÏßÄ)\n",
        "best_val_f1 = 0\n",
        "patience_counter = 0\n",
        "\n",
        "print(\"üöÄ Starting improved training...\")\n",
        "print(f\"   - Total epochs: {Config.EPOCHS}\")\n",
        "print(f\"   - Early stopping patience: {Config.EARLY_STOPPING_PATIENCE}\")\n",
        "print(f\"   - Using class weights: {Config.USE_CLASS_WEIGHTS}\")\n",
        "print(f\"   - Validation-based early stopping: ‚úÖ\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for epoch in range(Config.EPOCHS):\n",
        "    # Training phase\n",
        "    train_metrics = train_one_epoch(train_loader, model, optimizer, loss_fn, Config.DEVICE)\n",
        "    \n",
        "    # Validation phase (ÌïµÏã¨!)\n",
        "    val_metrics = validate_one_epoch(val_loader, model, loss_fn, Config.DEVICE)\n",
        "    \n",
        "    # Update scheduler\n",
        "    scheduler.step()\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "    \n",
        "    # Ïò§Î≤ÑÌîºÌåÖ Í∞êÏßÄ\n",
        "    train_val_gap = train_metrics['f1'] - val_metrics['f1']\n",
        "    \n",
        "    # Early stopping logic (Validation F1 Í∏∞Ï§Ä)\n",
        "    log_msg = f\"Epoch {epoch+1:2d}/{Config.EPOCHS} | \"\n",
        "    log_msg += f\"Train F1: {train_metrics['f1']:.4f} | \"\n",
        "    log_msg += f\"Val F1: {val_metrics['f1']:.4f} | \"\n",
        "    log_msg += f\"Gap: {train_val_gap:+.4f} | \"\n",
        "    log_msg += f\"LR: {current_lr:.6f}\"\n",
        "    \n",
        "    # Ïò§Î≤ÑÌîºÌåÖ Í≤ΩÍ≥†\n",
        "    if train_val_gap > 0.05:\n",
        "        log_msg += \" | ‚ö†Ô∏è Overfitting!\"\n",
        "    \n",
        "    if val_metrics['f1'] > best_val_f1:\n",
        "        best_val_f1 = val_metrics['f1']\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), f'best_val_model_f1_{best_val_f1:.4f}.pth')\n",
        "        log_msg += \" | üíæ Best Val!\"\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= Config.EARLY_STOPPING_PATIENCE:\n",
        "            print(f\"üõë Early stopping! Best Val F1: {best_val_f1:.4f}\")\n",
        "            break\n",
        "    \n",
        "    # Log to wandb\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch,\n",
        "        \"train_loss\": train_metrics['loss'],\n",
        "        \"train_acc\": train_metrics['accuracy'],\n",
        "        \"train_f1\": train_metrics['f1'],\n",
        "        \"val_loss\": val_metrics['loss'],\n",
        "        \"val_acc\": val_metrics['accuracy'],\n",
        "        \"val_f1\": val_metrics['f1'],\n",
        "        \"train_val_gap\": train_val_gap,\n",
        "        \"learning_rate\": current_lr,\n",
        "        \"best_val_f1\": best_val_f1\n",
        "    })\n",
        "    \n",
        "    print(log_msg)\n",
        "\n",
        "print(f\"‚úÖ Training completed! Best Validation F1: {best_val_f1:.4f}\")\n",
        "print(f\"üìä Final Train-Val Gap: {train_metrics['f1'] - val_metrics['f1']:+.4f}\")\n",
        "\n",
        "# Ïò§Î≤ÑÌîºÌåÖ ÏßÑÎã®\n",
        "if best_val_f1 < 0.85:\n",
        "    print(\"‚ö†Ô∏è ÎÇÆÏùÄ Validation ÏÑ±Îä• - Î™®Îç∏ Í∞úÏÑ† ÌïÑÏöî\")\n",
        "elif train_metrics['f1'] - val_metrics['f1'] > 0.05:\n",
        "    print(\"‚ö†Ô∏è Ïã¨Í∞ÅÌïú Ïò§Î≤ÑÌîºÌåÖ Í∞êÏßÄ - Ï†ïÍ∑úÌôî Í∞ïÌôî ÌïÑÏöî\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Inference & Submission\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Inference functions defined\n"
          ]
        }
      ],
      "source": [
        "# Inference Functions\n",
        "def load_best_model(model, best_f1):\n",
        "    \"\"\"Load the best saved model\"\"\"\n",
        "    try:\n",
        "        model_path = f'best_model_f1_{best_f1:.4f}.pth'\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        print(f\"‚úÖ Loaded best model: {model_path}\")\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è Could not load best model, using current model\")\n",
        "    return model\n",
        "\n",
        "def predict_with_tta(model, test_loader, device, use_tta=True):\n",
        "    \"\"\"Make predictions with optional TTA\"\"\"\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    \n",
        "    # Base predictions\n",
        "    print(\"üîÆ Base inference...\")\n",
        "    base_preds = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in tqdm(test_loader, desc=\"Base inference\"):\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            base_preds.extend(probs.detach().cpu().numpy())\n",
        "    \n",
        "    all_predictions.append(np.array(base_preds))\n",
        "    \n",
        "    # TTA predictions\n",
        "    if use_tta:\n",
        "        print(\"üîÆ TTA inference...\")\n",
        "        tta_transforms = get_tta_transforms(Config.IMG_SIZE)\n",
        "        \n",
        "        for i, tta_transform in enumerate(tta_transforms[1:], 1):\n",
        "            # Create TTA dataset\n",
        "            tta_dataset = DocumentDataset(\n",
        "                os.path.join(Config.DATA_PATH, \"sample_submission.csv\"),\n",
        "                os.path.join(Config.DATA_PATH, \"test\"),\n",
        "                tta_transform, is_train=False\n",
        "            )\n",
        "            tta_loader = DataLoader(\n",
        "                tta_dataset, batch_size=Config.BATCH_SIZE, shuffle=False,\n",
        "                num_workers=Config.NUM_WORKERS, pin_memory=True\n",
        "            )\n",
        "            \n",
        "            tta_preds = []\n",
        "            with torch.no_grad():\n",
        "                for images, _ in tqdm(tta_loader, desc=f\"TTA {i}\"):\n",
        "                    images = images.to(device)\n",
        "                    outputs = model(images)\n",
        "                    probs = torch.softmax(outputs, dim=1)\n",
        "                    tta_preds.extend(probs.detach().cpu().numpy())\n",
        "            \n",
        "            all_predictions.append(np.array(tta_preds))\n",
        "    \n",
        "    # Average predictions\n",
        "    if len(all_predictions) > 1:\n",
        "        print(f\"üìä Averaging {len(all_predictions)} predictions...\")\n",
        "        final_preds = np.mean(all_predictions, axis=0)\n",
        "    else:\n",
        "        final_preds = all_predictions[0]\n",
        "    \n",
        "    return np.argmax(final_preds, axis=1)\n",
        "\n",
        "def create_submission(predictions, best_f1, use_tta=True):\n",
        "    \"\"\"Create submission file\"\"\"\n",
        "    # Load sample submission\n",
        "    sample_sub = pd.read_csv(os.path.join(Config.DATA_PATH, \"sample_submission.csv\"))\n",
        "    submission = sample_sub.copy()\n",
        "    submission['target'] = predictions\n",
        "    \n",
        "    # Generate filename\n",
        "    today = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
        "    tta_suffix = \"_TTA\" if use_tta else \"\"\n",
        "    cw_suffix = \"_CW\" if Config.USE_CLASS_WEIGHTS else \"\"\n",
        "    ca_suffix = \"_CA\" if Config.USE_CLASS_SPECIFIC_AUG else \"\"\n",
        "    \n",
        "    filename = f\"submission/sub_{today}_{Config.MODEL_NAME}_f1_{best_f1:.4f}{tta_suffix}{cw_suffix}{ca_suffix}.csv\"\n",
        "    \n",
        "    # Save submission\n",
        "    os.makedirs(\"submission\", exist_ok=True)\n",
        "    submission.to_csv(filename, index=False)\n",
        "    \n",
        "    return filename, submission\n",
        "\n",
        "print(\"‚úÖ Inference functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded best model: best_model_f1_0.9385.pth\n",
            "üîÆ Base inference...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Base inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 197/197 [00:13<00:00, 14.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÆ TTA inference...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "TTA 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 197/197 [00:05<00:00, 33.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Averaging 2 predictions...\n",
            "\n",
            "üéØ Inference completed!\n",
            "   - Predictions: 3,140 samples\n",
            "   - Best Validation F1: 0.9385\n",
            "   - Submission saved: submission/sub_20250908_efficientnet_b0_f1_0.9385_TTA_CW_CA.csv\n",
            "\n",
            "üìä Submission preview:\n",
            "                     ID  target\n",
            "0  0008fdb22ddce0ce.jpg       2\n",
            "1  00091bffdffd83de.jpg       1\n",
            "2  00396fbc1f6cc21d.jpg       5\n",
            "3  00471f8038d9c4b6.jpg       3\n",
            "4  00901f504008d884.jpg       2\n",
            "\n",
            "üéâ Clean pipeline completed successfully!\n",
            "\n",
            "üìà Key Features Applied:\n",
            "   ‚úÖ EDA-based class weights for imbalance\n",
            "   ‚úÖ Train/Test domain adaptation augmentations\n",
            "   ‚úÖ Class-specific preprocessing\n",
            "   ‚úÖ Proper train/validation split\n",
            "   ‚úÖ Early stopping on validation F1\n",
            "   ‚úÖ Test Time Augmentation\n",
            "   ‚úÖ Clean, maintainable code structure\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Run Inference\n",
        "model = load_best_model(model, best_val_f1)\n",
        "\n",
        "# Make predictions with TTA\n",
        "predictions = predict_with_tta(model, test_loader, Config.DEVICE, use_tta=True)\n",
        "\n",
        "# Create submission file\n",
        "filename, submission = create_submission(predictions, best_val_f1, use_tta=True)\n",
        "\n",
        "print(f\"\\nüéØ Inference completed!\")\n",
        "print(f\"   - Predictions: {len(predictions):,} samples\")\n",
        "print(f\"   - Best Validation F1: {best_val_f1:.4f}\")\n",
        "print(f\"   - Submission saved: {filename}\")\n",
        "\n",
        "print(f\"\\nüìä Submission preview:\")\n",
        "print(submission.head())\n",
        "\n",
        "print(f\"\\nüéâ Clean pipeline completed successfully!\")\n",
        "print(f\"\\nüìà Key Features Applied:\")\n",
        "print(f\"   ‚úÖ EDA-based class weights for imbalance\")\n",
        "print(f\"   ‚úÖ Train/Test domain adaptation augmentations\")\n",
        "print(f\"   ‚úÖ Class-specific preprocessing\")\n",
        "print(f\"   ‚úÖ Proper train/validation split\")\n",
        "print(f\"   ‚úÖ Early stopping on validation F1\")\n",
        "print(f\"   ‚úÖ Test Time Augmentation\")\n",
        "print(f\"   ‚úÖ Clean, maintainable code structure\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
