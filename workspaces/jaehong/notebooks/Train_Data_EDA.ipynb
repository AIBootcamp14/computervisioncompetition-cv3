{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f10421a5",
   "metadata": {},
   "source": [
    "\n",
    "## Train Data EDA\n",
    "* ë°ì´í„° ê¸°ì¡´ ì •ë³´ (ìƒ˜í”Œ ìˆ˜, í´ë˜ìŠ¤ ë¶„í¬, ì´ë¯¸ì§€ í¬ê¸° ë“±)\n",
    "* ì‹œê°í™” (í´ë˜ìŠ¤ë³„ ì´ë¯¸ì§€ ì˜ˆì‹œ, ì´ìƒì¹˜ íƒì§€ ë“±)\n",
    "* ì¸ì‚¬ì´íŠ¸ ì •ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01a3df84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDAë¥¼ ìœ„í•œ ì¶”ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
    "import matplotlib.pyplot as plt         # ì‹œê°í™” (ë¼ì¸ ê·¸ë˜í”„, ë§‰ëŒ€ ê·¸ë˜í”„ ë“±)\n",
    "import seaborn as sns                   # ê³ ê¸‰ ì‹œê°í™” (íˆíŠ¸ë§µ, ë°•ìŠ¤í”Œë¡¯ ë“±)\n",
    "from collections import Counter         # í´ë˜ìŠ¤ ë¶„í¬ ê³„ì‚°\n",
    "import cv2                              # openCV, ì´ë¯¸ì§€ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from pathlib import Path                # ìš´ì˜ì²´ì œ ë…ë¦½ì ì¸ ê²½ë¡œ ê´€ë¦¬\n",
    "import matplotlib.font_manager as fm    # í°íŠ¸ ê´€ë¦¬\n",
    "import pandas as pd                     # ë°ì´í„° ì²˜ë¦¬\n",
    "import os                               # íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬\n",
    "import numpy as np                      # ìˆ˜ì¹˜ ì—°ì‚°\n",
    "\n",
    "# ë°ì´í„°ì…‹ ê²½ë¡œ\n",
    "data_path = \"/home/dev/computervisioncompetition-cv3/data\"\n",
    "font_path = \"/home/dev/computervisioncompetition-cv3/workspaces/jaehong/notebooks/Fonts/NanumGothic-Regular.ttf\"\n",
    "\n",
    "\n",
    "# ì‹œê°í™” ì„¤ì •\n",
    "plt.style.use('default')                # Matplotlib ê¸°ë³¸ ìŠ¤íƒ€ì¼ë¡œ ì´ˆê¸°í™”\n",
    "sns.set_palette(\"husl\")                 # seaborn íŒ”ë ˆíŠ¸ ì„¤ì • (husl: ë‹¤ì–‘í•œ ìƒ‰ìƒ íŒ”ë ˆíŠ¸)\n",
    "plt.rcParams['figure.figsize'] = (12, 8)# ê·¸ë˜í”„ ì „ì²´ í¬ê¸° ê¸°ë³¸ê°’ ì„¤ì • (ê°€ë¡œ 12, ì„¸ë¡œ 8)\n",
    "plt.rcParams['font.size'] = 10          # ê¸€ê¼´ í¬ê¸° ê¸°ë³¸ê°’ ì„¤ì • (ì¶•, ì œëª©, ë ˆì´ë¸” ë“± ì ìš©)\n",
    "\n",
    "print(\"EDA ë¼ì´ë¸ŒëŸ´ ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67d882d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ë¶„ì„ì„ ìœ„í•œ í´ë˜ìŠ¤\n",
    "class DatasetAnalyzer:\n",
    "    \"\"\"\n",
    "    ë°ì´í„°ì…‹ ë¶„ì„ì„ ìœ„í•œ í´ë˜ìŠ¤\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, train_csv_path: str, train_img_path: str, test_csv_path: str, test_img_path: str):\n",
    "        \"\"\"\n",
    "        ë°ì´í„°ì…‹ ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "        \n",
    "        Args:\n",
    "            train_csv_path: í›ˆë ¨ ë°ì´í„° CSV íŒŒì¼ ê²½ë¡œ\n",
    "            train_img_path: í›ˆë ¨ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œ\n",
    "            test_csv_path: í…ŒìŠ¤íŠ¸ ë°ì´í„° CSV íŒŒì¼ ê²½ë¡œ\n",
    "            test_img_path: í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œ \n",
    "        \"\"\"\n",
    "        self.train_csv_path = train_csv_path\n",
    "        self.train_img_path = train_img_path\n",
    "        self.test_csv_path = test_csv_path\n",
    "        self.test_img_path = test_img_path\n",
    "        \n",
    "        # ë°ì´í„° ë¡œë“œ\n",
    "        self.train_df = pd.read_csv(train_csv_path)\n",
    "        self.test_df = pd.read_csv(test_csv_path)\n",
    "        \n",
    "    def get_basic_info(self) -> dict:\n",
    "        \"\"\"\n",
    "        ë°ì´í„°ì…‹ì˜ ê¸°ë³¸ ì •ë³´ë¥¼ ë°˜í™˜\n",
    "        \n",
    "        Returns:\n",
    "            dict: ë°ì´í„°ì…‹ ê¸°ë³¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬\n",
    "        \"\"\"\n",
    "        info = {\n",
    "            \"train_samples\": len(self.train_df),\n",
    "            \"test_samples\": len(self.test_df),\n",
    "            \"num_classes\": self.train_df['target'].nunique(),\n",
    "            \"class_names\": sorted(self.train_df['target'].unique()),\n",
    "            \"train_img_path\": self.train_img_path,\n",
    "            \"test_img_path\": self.test_img_path\n",
    "        }\n",
    "        return info\n",
    "    \n",
    "    def analyze_class_distribution(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        í´ë˜ìŠ¤ë³„ ë¶„í¬ ë¶„ì„\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: í´ë˜ìŠ¤ë³„ ë¶„í¬ ë°ì´í„°í”„ë ˆì„\n",
    "        \"\"\"\n",
    "        class_counts = self.train_df['target'].value_counts()\n",
    "        class_distribution = pd.DataFrame({\n",
    "            'class': class_counts.index,\n",
    "            'count': class_counts.values,\n",
    "            'percentage': (class_counts.values / len(self.train_df) * 100).round(2)\n",
    "        })\n",
    "        return class_distribution\n",
    "    \n",
    "    def analyze_image_properties(self, sample_size: int = 100) -> dict:\n",
    "        \"\"\"\n",
    "        ì´ë¯¸ì§€ ì†ì„± ë¶„ì„ (í¬ê¸°, ì±„ë„ ë“±)\n",
    "        \n",
    "        Args:\n",
    "            sample_size: ë¶„ì„í•  ìƒ˜í”Œ ì´ë¯¸ì§€ ìˆ˜\n",
    "            \n",
    "        returns:\n",
    "            dics: ì´ë¯¸ì§€ ì†ì„± ì •ë³´\n",
    "        \"\"\"\n",
    "        \n",
    "        # í›ˆë ¨ ì´ë¯¸ì§€ ìƒ˜í”Œë§\n",
    "        sample_files = self.train_df.sample(min(sample_size, len(self.train_df)))['ID'].tolist()\n",
    "        \n",
    "        heights, widths, channels = [], [], []\n",
    "        \n",
    "        for img_file in sample_files:\n",
    "            img_path = os.path.join(self.train_img_path, img_file)\n",
    "            if os.path.exists(img_path):\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is not None:\n",
    "                    h, w, c = img.shape\n",
    "                    heights.append(h)\n",
    "                    widths.append(w)\n",
    "                    channels.append(c)\n",
    "        \n",
    "        properties = {\n",
    "            'height_stats': {\n",
    "                'min': min(heights),\n",
    "                'max': max(heights),\n",
    "                'mean': np.mean(heights),\n",
    "                'std': np.std(heights)\n",
    "            },\n",
    "            'width_stats': {\n",
    "                'min': min(widths),\n",
    "                'max': max(widths),\n",
    "                'mean': np.mean(widths),\n",
    "                'std': np.std(widths)\n",
    "            },\n",
    "            'channels': list(set(channels)),\n",
    "            'sample_size': len(heights)\n",
    "        }\n",
    "                    \n",
    "        return properties\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "analyzer = DatasetAnalyzer(\n",
    "    train_csv_path=data_path + \"/train.csv\",\n",
    "    train_img_path=data_path + \"/train/\",\n",
    "    test_csv_path=data_path + \"/sample_submission.csv\",\n",
    "    test_img_path=data_path + \"/test/\"\n",
    ")            \n",
    "\n",
    "print(\"ë°ì´í„°ì…‹ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0056f9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ë¯¸ì§€ ì‹œê°í™”ë¥¼ ìœ„í•œ í´ë˜ìŠ¤\n",
    "class ImageVisualizer:\n",
    "    \"\"\"\n",
    "    ì´ë¯¸ì§€ ì‹œê°í™”ë¥¼ ìœ„í•œ í´ë˜ìŠ¤\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, analyzer: DatasetAnalyzer):\n",
    "        \"\"\"\n",
    "        ì´ë¯¸ì§€ ì‹œê°í™”ê¸° ì´ˆê¸°í™”\n",
    "        \n",
    "        Args:\n",
    "            analyzer: ë°ì´í„°ì…‹ ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤\n",
    "        \"\"\"\n",
    "        self.analyzer = analyzer\n",
    "    \n",
    "    def visualize_class_samples(self, samples_per_class: int = 4, figsize: tuple = (20, 15)):\n",
    "        \"\"\"\n",
    "        ê° í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ì´ë¯¸ì§€ë¥¼ ì‹œê°í™”\n",
    "        \n",
    "        Args:\n",
    "            samples_per_class: í´ë˜ìŠ¤ë‹¹ í‘œì‹œí•  ìƒ˜í”Œ ìˆ˜\n",
    "            figsize: ê·¸ë¦¼ í¬ê¸°\n",
    "        \"\"\"\n",
    "        num_classes = len(self.analyzer.train_df['target'].unique())\n",
    "        cols = samples_per_class\n",
    "        rows = num_classes\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "        fig.suptitle('Example Images per Class', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # ê° í´ë˜ìŠ¤ë³„ë¡œ ìƒ˜í”Œ ì´ë¯¸ì§€ ì„ íƒ\n",
    "        for class_idx, class_label in enumerate(sorted(self.analyzer.train_df['target'].unique())):\n",
    "            class_samples = self.analyzer.train_df[\n",
    "                self.analyzer.train_df['target'] == class_label\n",
    "            ].sample(min(samples_per_class, len(self.analyzer.train_df[\n",
    "                self.analyzer.train_df['target'] == class_label\n",
    "            ])))['ID'].tolist()\n",
    "            \n",
    "            for sample_idx, img_file in enumerate(class_samples):\n",
    "                img_path = os.path.join(self.analyzer.train_img_path, img_file)\n",
    "                \n",
    "                if os.path.exists(img_path):\n",
    "                    img = cv2.imread(img_path)\n",
    "                    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    \n",
    "                    axes[class_idx, sample_idx].imshow(img_rgb)\n",
    "                    axes[class_idx, sample_idx].set_title(f'Class {class_label}', fontweight='bold')\n",
    "                    axes[class_idx, sample_idx].axis('off')\n",
    "                else:\n",
    "                    axes[class_idx, sample_idx].text(0.5, 0.5, 'Image Not Found', \n",
    "                                                   ha='center', va='center', transform=axes[class_idx, sample_idx].transAxes)\n",
    "                    axes[class_idx, sample_idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def analyze_image_statistics(self, sample_size: int = 100) -> dict:\n",
    "        \"\"\"\n",
    "        ì´ë¯¸ì§€ í†µê³„ ì •ë³´ ë¶„ì„ (RGB ì±„ë„ë³„ í‰ê· , í‘œì¤€í¸ì°¨ ë“±)\n",
    "        \n",
    "        Args:\n",
    "            sample_size: ë¶„ì„í•  ìƒ˜í”Œ ì´ë¯¸ì§€ ìˆ˜\n",
    "            \n",
    "        Returns:\n",
    "            dict: ì´ë¯¸ì§€ í†µê³„ ì •ë³´\n",
    "        \"\"\"\n",
    "        sample_files = self.analyzer.train_df.sample(min(sample_size, len(self.analyzer.train_df)))['ID'].tolist()\n",
    "        \n",
    "        r_values, g_values, b_values = [], [], []\n",
    "        \n",
    "        for img_file in sample_files:\n",
    "            img_path = os.path.join(self.analyzer.train_img_path, img_file)\n",
    "            if os.path.exists(img_path):\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is not None:\n",
    "                    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "                    img_resized = cv2.resize(img_rgb, (64, 64))\n",
    "                    \n",
    "                    r_values.extend(img_resized[:, :, 0].flatten())\n",
    "                    g_values.extend(img_resized[:, :, 1].flatten())\n",
    "                    b_values.extend(img_resized[:, :, 2].flatten())\n",
    "        \n",
    "        statistics = {\n",
    "            'r_channel': {\n",
    "                'mean': np.mean(r_values),\n",
    "                'std': np.std(r_values),\n",
    "                'min': np.min(r_values),\n",
    "                'max': np.max(r_values)\n",
    "            },\n",
    "            'g_channel': {\n",
    "                'mean': np.mean(g_values),\n",
    "                'std': np.std(g_values),\n",
    "                'min': np.min(g_values),\n",
    "                'max': np.max(g_values)\n",
    "            },\n",
    "            'b_channel': {\n",
    "                'mean': np.mean(b_values),\n",
    "                'std': np.std(b_values),\n",
    "                'min': np.min(b_values),\n",
    "                'max': np.max(b_values)\n",
    "            },\n",
    "            'sample_size': len(sample_files)\n",
    "        }\n",
    "        \n",
    "        return statistics\n",
    "\n",
    "# ì´ë¯¸ì§€ ì‹œê°í™”ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "visualizer = ImageVisualizer(analyzer)\n",
    "\n",
    "print(\" ì´ë¯¸ì§€ ì‹œê°í™”ê¸° ì´ˆê¸°í™” ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c8f3dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ë°ì´í„°ì…‹ ê¸°ë³¸ ì •ë³´ ì¶œë ¥\n",
    "print(\"=\" * 60)\n",
    "print(\" ë°ì´í„°ì…‹ ê¸°ë³¸ ì •ë³´\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "basic_info = analyzer.get_basic_info()\n",
    "for key, value in basic_info.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" í´ë˜ìŠ¤ë³„ ë¶„í¬ ë¶„ì„\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class_distribution = analyzer.analyze_class_distribution()\n",
    "print(class_distribution.to_string(index=False))\n",
    "\n",
    "# í´ë˜ìŠ¤ ë¶„í¬ ì‹œê°í™”\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "fontprop = fm.FontProperties(fname=font_path)\n",
    "\n",
    "# ì„œë¸Œí”Œë¡¯ 1: í´ë˜ìŠ¤ë³„ ê°œìˆ˜\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(class_distribution['class'], class_distribution['count'], \n",
    "        color=plt.cm.Set3(np.linspace(0, 1, len(class_distribution))))\n",
    "plt.title('í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜', fontsize=14, fontweight='bold', fontproperties=fontprop)\n",
    "plt.xlabel('í´ë˜ìŠ¤', fontproperties=fontprop)\n",
    "plt.ylabel('ìƒ˜í”Œ ìˆ˜', fontproperties=fontprop)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# ì„œë¸Œí”Œë¡¯ 2: í´ë˜ìŠ¤ë³„ ë¹„ìœ¨\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(class_distribution['count'], \n",
    "        labels=class_distribution['class'], \n",
    "        autopct='%1.1f%%',\n",
    "        colors=plt.cm.Set3(np.linspace(0, 1, len(class_distribution))))\n",
    "plt.title('í´ë˜ìŠ¤ë³„ ë¶„í¬ ë¹„ìœ¨', fontsize=14, fontweight='bold', fontproperties=fontprop)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# í´ë˜ìŠ¤ ë¶„í¬ í†µê³„\n",
    "print(f\"\\n í´ë˜ìŠ¤ ë¶„í¬ í†µê³„:\")\n",
    "print(f\"ìµœëŒ€ í´ë˜ìŠ¤ ìƒ˜í”Œ ìˆ˜: {class_distribution['count'].max()}\")\n",
    "print(f\"ìµœì†Œ í´ë˜ìŠ¤ ìƒ˜í”Œ ìˆ˜: {class_distribution['count'].min()}\")\n",
    "print(f\"í‰ê·  í´ë˜ìŠ¤ ìƒ˜í”Œ ìˆ˜: {class_distribution['count'].mean():.1f}\")\n",
    "print(f\"í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¹„ìœ¨: {class_distribution['count'].max() / class_distribution['count'].min():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "762a4413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ì´ë¯¸ì§€ ì‹œê°í™”\n",
    "print(\"=\" * 60)\n",
    "print(\" í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ì´ë¯¸ì§€ ì‹œê°í™”\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ì´ë¯¸ì§€ í‘œì‹œ\n",
    "visualizer.visualize_class_samples(samples_per_class=3, figsize=(60,80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53afde11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ë¯¸ì§€ ì†ì„± ë¶„ì„\n",
    "print(\"=\" * 60)\n",
    "print(\" ì´ë¯¸ì§€ ì†ì„± ë¶„ì„\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "image_properties = analyzer.analyze_image_properties(sample_size=len(analyzer.train_df))\n",
    "\n",
    "print(f\"ë¶„ì„ ìƒ˜í”Œ ê°œìˆ˜: {image_properties['sample_size']}\")\n",
    "print(f\"ì±„ë„ ìˆ˜: {image_properties['channels']}\")\n",
    "\n",
    "print(\"\\n ì´ë¯¸ì§€ í¬ê¸° í†µê³„:\")\n",
    "print(f\"ì„¸ë¡œ(Height) - ìµœì†Œ: {image_properties['height_stats']['min']}, \"\n",
    "      f\"ìµœëŒ€: {image_properties['height_stats']['max']}, \"\n",
    "      f\"í‰ê· : {image_properties['height_stats']['mean']:.1f}\")\n",
    "\n",
    "print(f\"ê°€ë¡œ(Width) - ìµœì†Œ: {image_properties['width_stats']['min']}, \"\n",
    "      f\"ìµœëŒ€: {image_properties['width_stats']['max']}, \"\n",
    "      f\"í‰ê· : {image_properties['width_stats']['mean']:.1f}\")\n",
    "\n",
    "# ì‹¤ì œ ì´ë¯¸ì§€ í¬ê¸° ë¶„ì„ì„ ìœ„í•œ ìƒ˜í”Œ ì´ë¯¸ì§€ë“¤\n",
    "sample_files = analyzer.train_df.sample(min(50, len(analyzer.train_df)))['ID'].tolist()\n",
    "heights, widths = [], []\n",
    "\n",
    "for img_file in sample_files:\n",
    "    img_path = os.path.join(analyzer.train_img_path, img_file)\n",
    "    if os.path.exists(img_path):\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is not None:\n",
    "            h, w, _ = img.shape\n",
    "            heights.append(h)\n",
    "            widths.append(w)       \n",
    "                 \n",
    "def create_image_size_plots(heights, widths):\n",
    "    \"\"\"\n",
    "    ì´ë¯¸ì§€ í¬ê¸° ë¶„í¬ ì‹œê°í™” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # ë†’ì´ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨\n",
    "    axes[0, 0].hist(heights, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_title('Image Height Distribution', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Height (pixels)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ë„ˆë¹„ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨\n",
    "    axes[0, 1].hist(widths, bins=20, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    axes[0, 1].set_title('Image Width Distribution', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Width (pixels)')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ë†’ì´ vs ë„ˆë¹„ ìŠ¤ìºí„° í”Œë¡¯\n",
    "    axes[1, 0].scatter(widths, heights, alpha=0.6, color='green')\n",
    "    axes[1, 0].set_title('Image Size Distribution (Width vs Height)', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Width (pixels)')\n",
    "    axes[1, 0].set_ylabel('Height (pixels)')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ì¢…íš¡ë¹„ ë¶„í¬\n",
    "    aspect_ratios = [w/h for w, h in zip(widths, heights)]\n",
    "    axes[1, 1].hist(aspect_ratios, bins=20, alpha=0.7, color='gold', edgecolor='black')\n",
    "    axes[1, 1].set_title('Image Aspect Ratio Distribution', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Aspect Ratio (Width/Height)')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return aspect_ratios\n",
    "# ì´ë¯¸ì§€ í¬ê¸° ë¶„í¬ ì‹œê°í™”\n",
    "aspect_ratios = create_image_size_plots(heights, widths)\n",
    "\n",
    "print(f\"\\n ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆ í†µê³„:\")\n",
    "print(f\"í‰ê·  ë¹„ìœ¨: {np.mean(aspect_ratios):.2f}\")\n",
    "print(f\"ë¹„ìœ¨ í‘œì¤€í¸ì°¨: {np.std(aspect_ratios):.2f}\")\n",
    "print(f\"ì •ì‚¬ê°í˜• ì´ë¯¸ì§€ ë¹„ìœ¨: {sum(1 for ratio in aspect_ratios if 0.9 <= ratio <= 1.1) / len(aspect_ratios) * 100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d65a5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ë¯¸ì§€ í†µê³„ ì •ë³´ ë¶„ì„\n",
    "print(\"=\" * 60)\n",
    "print(\" ì´ë¯¸ì§€ í†µê³„ ì •ë³´ ë¶„ì„\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# RGB ì±„ë„ë³„ í†µê³„ ë¶„ì„\n",
    "image_stats = visualizer.analyze_image_statistics(sample_size=len(analyzer.train_df))\n",
    "\n",
    "print(f\"ë¶„ì„ ìƒ˜í”Œ ê°œìˆ˜: {image_stats['sample_size']}\")\n",
    "print(f\"\\nğŸ”´ Red ì±„ë„ í†µê³„:\")\n",
    "print(f\"  í‰ê· : {image_stats['r_channel']['mean']:.2f}\")\n",
    "print(f\"  í‘œì¤€í¸ì°¨: {image_stats['r_channel']['std']:.2f}\")\n",
    "print(f\"  ìµœì†Œ: {image_stats['r_channel']['min']}\")\n",
    "print(f\"  ìµœëŒ€: {image_stats['r_channel']['max']}\")\n",
    "\n",
    "print(f\"\\nğŸŸ¢ Green ì±„ë„ í†µê³„:\")\n",
    "print(f\"  í‰ê· : {image_stats['g_channel']['mean']:.2f}\")\n",
    "print(f\"  í‘œì¤€í¸ì°¨: {image_stats['g_channel']['std']:.2f}\")\n",
    "print(f\"  ìµœì†Œ: {image_stats['g_channel']['min']}\")\n",
    "print(f\"  Max: {image_stats['g_channel']['max']}\")\n",
    "\n",
    "print(f\"\\nğŸ”µ Blue ì±„ë„ í†µê³„:\")\n",
    "print(f\"  í‰ê· : {image_stats['b_channel']['mean']:.2f}\")\n",
    "print(f\"  í‘œì¤€í¸ì°¨: {image_stats['b_channel']['std']:.2f}\")\n",
    "print(f\"  ìµœì†Œ: {image_stats['b_channel']['min']}\")\n",
    "print(f\"  ìµœëŒ€: {image_stats['b_channel']['max']}\")\n",
    "\n",
    "# ì‹¤ì œ í”½ì…€ ê°’ ë¶„ì„ì„ ìœ„í•œ ìƒ˜í”Œ ì´ë¯¸ì§€ë“¤\n",
    "sample_files = analyzer.train_df.sample(min(50, len(analyzer.train_df)))['ID'].tolist()\n",
    "r_values, g_values, b_values = [], [], []\n",
    "\n",
    "for img_file in sample_files:\n",
    "    img_path = os.path.join(analyzer.train_img_path, img_file)\n",
    "    if os.path.exists(img_path):\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is not None:\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img_resized = cv2.resize(img_rgb, (32, 32))  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì‘ê²Œ\n",
    "            \n",
    "            r_values.extend(img_resized[:, :, 0].flatten())\n",
    "            g_values.extend(img_resized[:, :, 1].flatten())\n",
    "            b_values.extend(img_resized[:, :, 2].flatten())\n",
    "            \n",
    "def create_rgb_channel_plots(r_values, g_values, b_values):\n",
    "    \"\"\"\n",
    "    RGB ì±„ë„ ë¶„í¬ ì‹œê°í™” í•¨ìˆ˜ (ì˜ì–´ ì œëª© ì‚¬ìš©)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Red ì±„ë„ íˆìŠ¤í† ê·¸ë¨\n",
    "    axes[0, 0].hist(r_values, bins=50, alpha=0.7, color='red', edgecolor='black')\n",
    "    axes[0, 0].set_title('Red Channel Distribution', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Pixel Value')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Green ì±„ë„ íˆìŠ¤í† ê·¸ë¨\n",
    "    axes[0, 1].hist(g_values, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "    axes[0, 1].set_title('Green Channel Distribution', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Pixel Value')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Blue ì±„ë„ íˆìŠ¤í† ê·¸ë¨\n",
    "    axes[1, 0].hist(b_values, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "    axes[1, 0].set_title('Blue Channel Distribution', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Pixel Value')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # RGB ì±„ë„ ë¹„êµ\n",
    "    axes[1, 1].hist(r_values, bins=30, alpha=0.5, color='red', label='Red', edgecolor='black')\n",
    "    axes[1, 1].hist(g_values, bins=30, alpha=0.5, color='green', label='Green', edgecolor='black')\n",
    "    axes[1, 1].hist(b_values, bins=30, alpha=0.5, color='blue', label='Blue', edgecolor='black')\n",
    "    axes[1, 1].set_title('RGB Channel Comparison', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Pixel Value')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\" ì‹œê°í™” í•¨ìˆ˜ë“¤ ì •ì˜ ì™„ë£Œ\")\n",
    "# RGB ì±„ë„ ë¶„í¬ ì‹œê°í™”\n",
    "create_rgb_channel_plots(r_values, g_values, b_values)\n",
    "\n",
    "# ì •ê·œí™” ê¶Œì¥ì‚¬í•­ ì¶œë ¥\n",
    "print(\"=\" * 60)\n",
    "print(f\" ì •ê·œí™” ê¶Œì¥ì‚¬í•­:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\" í˜„ì¬ ImageNet ì •ê·œí™” ê°’:\")\n",
    "print(f\"   í‰ê· =[0.485, 0.456, 0.406], í‘œì¤€í¸ì°¨=[0.229, 0.224, 0.225]\")\n",
    "print(f\"Dataset ì‹¤ì œ í‰ê·  ê°’:\")\n",
    "print(f\"   í‰ê· =[{image_stats['r_channel']['mean']/255:.3f}, {image_stats['g_channel']['mean']/255:.3f}, {image_stats['b_channel']['mean']/255:.3f}]\")\n",
    "print(f\"Dataset ì‹¤ì œ í‘œì¤€í¸ì°¨ ê°’:\")\n",
    "print(f\"   í‘œì¤€í¸ì°¨=[{image_stats['r_channel']['std']/255:.3f}, {image_stats['g_channel']['std']/255:.3f}, {image_stats['b_channel']['std']/255:.3f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "530cf89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA ìš”ì•½ ë° ì¸ì‚¬ì´íŠ¸\n",
    "print(\"=\" * 60)\n",
    "print(\" EDA ìš”ì•½ ë° ì¸ì‚¬ì´íŠ¸\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\" ì£¼ìš” ë°œê²¬ì‚¬í•­:\")\n",
    "print(\"1. ë°ì´í„°ì…‹ ê·œëª¨:\")\n",
    "print(f\"   - í›ˆë ¨ ìƒ˜í”Œ: {basic_info['train_samples']:,}ê°œ\")\n",
    "print(f\"   - í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {basic_info['test_samples']:,}ê°œ\")\n",
    "print(f\"   - í´ë˜ìŠ¤ ìˆ˜: {basic_info['num_classes']}ê°œ\")\n",
    "\n",
    "print(\"\\n2. í´ë˜ìŠ¤ ë¶„í¬:\")\n",
    "print(f\"   - ê°€ì¥ ë§ì€ í´ë˜ìŠ¤: {class_distribution.loc[class_distribution['count'].idxmax(), 'class']} ({class_distribution['count'].max()}ê°œ)\")\n",
    "print(f\"   - ê°€ì¥ ì ì€ í´ë˜ìŠ¤: {class_distribution.loc[class_distribution['count'].idxmin(), 'class']} ({class_distribution['count'].min()}ê°œ)\")\n",
    "print(f\"   - í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¹„ìœ¨: {class_distribution['count'].max() / class_distribution['count'].min():.2f}:1\")\n",
    "\n",
    "print(\"\\n3. ì´ë¯¸ì§€ íŠ¹ì„±:\")\n",
    "print(f\"   - ì´ë¯¸ì§€ ì±„ë„: {image_properties['channels']}\")\n",
    "print(f\"   - í‰ê·  ì´ë¯¸ì§€ í¬ê¸°: {image_properties['height_stats']['mean']:.0f} x {image_properties['width_stats']['mean']:.0f}\")\n",
    "print(f\"   - í¬ê¸° ë²”ìœ„: {image_properties['height_stats']['min']}-{image_properties['height_stats']['max']} x {image_properties['width_stats']['min']}-{image_properties['width_stats']['max']}\")\n",
    "\n",
    "print(\"\\n4. í”½ì…€ ê°’ ë¶„í¬:\")\n",
    "print(f\"   - Red ì±„ë„ í‰ê· : {image_stats['r_channel']['mean']:.1f}\")\n",
    "print(f\"   - Green ì±„ë„ í‰ê· : {image_stats['g_channel']['mean']:.1f}\")\n",
    "print(f\"   - Blue ì±„ë„ í‰ê· : {image_stats['b_channel']['mean']:.1f}\")\n",
    "\n",
    "print(\"\\n ëª¨ë¸ë§ ê¶Œì¥ì‚¬í•­:\")\n",
    "print(\"1. í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°:\")\n",
    "print(\"   - í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš© ë˜ëŠ” ì˜¤ë²„ìƒ˜í”Œë§ ê³ ë ¤\")\n",
    "print(\"   - F1-scoreë¥¼ ì£¼ìš” í‰ê°€ ì§€í‘œë¡œ ì‚¬ìš©\")\n",
    "\n",
    "print(\"\\n2. ì´ë¯¸ì§€ ì „ì²˜ë¦¬:\")\n",
    "print(\"   - ë‹¤ì–‘í•œ í¬ê¸°ì˜ ì´ë¯¸ì§€ë¥¼ ê³ ë ¤í•œ augmentation ì ìš©\")\n",
    "print(\"   - í˜„ì¬ ImageNet ì •ê·œí™” ì‚¬ìš©ì´ ì ì ˆí•¨\")\n",
    "\n",
    "print(\"\\n3. ëª¨ë¸ ì„ íƒ:\")\n",
    "print(\"   - ì‘ì€ ì´ë¯¸ì§€ í¬ê¸°(32x32)ì— ì í•©í•œ ëª¨ë¸ ì„ íƒ\")\n",
    "print(\"   - Transfer learning í™œìš© ê¶Œì¥\")\n",
    "\n",
    "print(\"\\n4. í•˜ì´í¼íŒŒë¼ë¯¸í„°:\")\n",
    "print(\"   - í˜„ì¬ ì„¤ì •ì´ ì ì ˆí•¨ (ë°°ì¹˜ í¬ê¸° 32, í•™ìŠµë¥  1e-3)\")\n",
    "print(\"   - ì—í¬í¬ ìˆ˜ëŠ” ì¡°ê¸° ì¢…ë£Œë¡œ ì¡°ì • ê°€ëŠ¥\")\n",
    "\n",
    "print(\"\\n EDA ë¶„ì„ ì™„ë£Œ!\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a97908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae0b001",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
