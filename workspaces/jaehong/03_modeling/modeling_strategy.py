"""
ğŸ† ìºê¸€ 1ë“±ê¸‰ ê·¸ëœë“œë§ˆìŠ¤í„° ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë§ ì „ëµ
Kaggle Grandmaster Level Competition-Winning Modeling Strategy

ğŸ¯ ëª©í‘œ: ìºê¸€ ëŒ€íšŒ 1ë“± ë‹¬ì„±ì„ ìœ„í•œ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë§ ì‹œìŠ¤í…œ

í•µì‹¬ ì „ëµ:
1. ğŸ§  Multi-Architecture Ensemble (ë‹¤ì–‘ì„± ê·¹ëŒ€í™”)
2. ğŸ¯ EDA ê¸°ë°˜ Domain-Specific Optimization
3. ğŸ”„ Progressive Training with Pseudo Labeling
4. ğŸ“Š Advanced Loss Functions & Regularization
5. ğŸš€ Test-Time Augmentation & Model Averaging
6. ğŸ’¡ Meta-Learning & Knowledge Distillation

Clean Architecture & Clean Code ì ìš©:
- Strategy Pattern: ë‹¤ì–‘í•œ ëª¨ë¸ë§ ì „ëµ ì„ íƒ
- Factory Pattern: ëª¨ë¸ ë° ì»´í¬ë„ŒíŠ¸ ìƒì„±
- Observer Pattern: í›ˆë ¨ ê³¼ì • ëª¨ë‹ˆí„°ë§
- Single Responsibility: ê° í´ë˜ìŠ¤ë³„ ëª…í™•í•œ ì—­í• 
"""

import os
import sys
import json
import pickle
import random
import warnings
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Union
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from enum import Enum
import numpy as np
import pandas as pd
from datetime import datetime
from collections import defaultdict
import math

# Deep Learning
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torch.cuda.amp import GradScaler, autocast
import timm
from tqdm import tqdm

# ê¸°ì¡´ ëª¨ë“ˆ import
sys.path.append('../02_preprocessing')
sys.path.append('../01_EDA')

try:
    from grandmaster_processor import (
        GrandmasterProcessor, GrandmasterConfig, ProcessingStrategy,
        create_grandmaster_processor, load_competition_data
    )
    from improved_model_factory import (
        ImprovedModelFactory, ImprovedDocumentClassifier,
        AdvancedFocalLoss, AttentionModule
    )
    DEPENDENCIES_AVAILABLE = True
except ImportError:
    print("âš ï¸ ì¼ë¶€ ì˜ì¡´ì„±ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ êµ¬í˜„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    DEPENDENCIES_AVAILABLE = False

warnings.filterwarnings('ignore')

class ModelingStrategy(Enum):
    """ëª¨ë¸ë§ ì „ëµ íƒ€ì…"""
    SINGLE_BEST = "single_best"                    # ë‹¨ì¼ ìµœê³  ëª¨ë¸
    DIVERSE_ENSEMBLE = "diverse_ensemble"          # ë‹¤ì–‘ì„± ì•™ìƒë¸”
    STACKING_ENSEMBLE = "stacking_ensemble"        # ìŠ¤íƒí‚¹ ì•™ìƒë¸”  
    KNOWLEDGE_DISTILLATION = "knowledge_distillation"  # ì§€ì‹ ì¦ë¥˜
    META_LEARNING = "meta_learning"                # ë©”íƒ€ ëŸ¬ë‹

@dataclass
class GrandmasterModelConfig:
    """ê·¸ëœë“œë§ˆìŠ¤í„° ëª¨ë¸ë§ ì„¤ì •"""
    
    # ê¸°ë³¸ ì„¤ì •
    strategy: ModelingStrategy = ModelingStrategy.DIVERSE_ENSEMBLE
    target_score: float = 0.95  # ëª©í‘œ ì ìˆ˜ (ìºê¸€ 1ë“± ìˆ˜ì¤€)
    
    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì•™ìƒë¸” ì„¤ì • (RTX 4090 Laptop ìµœì í™”)
    image_size: int = 224   # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ì ì ˆí•œ í¬ê¸°
    batch_size: int = 2     # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ì‘ì€ ë°°ì¹˜ í¬ê¸°
    
    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì•™ìƒë¸” ì„¤ì • (3ê°œ ëª¨ë¸)
    ensemble_size: int = 3  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ 3ê°œ ëª¨ë¸ë¡œ ì¡°ì •
    ensemble_architectures: List[str] = field(default_factory=lambda: [
        "efficientnet_b3",     # íš¨ìœ¨ì ì¸ CNN (12M params)
        "convnext_small",      # ìµœì‹  CNN ì†Œí˜• ë²„ì „ (50M params)
        "swin_small"           # Vision Transformer ì†Œí˜• ë²„ì „ (28M params)
    ])
    
    # ìµœê³  ì„±ëŠ¥ì„ ìœ„í•œ ê³ ê¸‰ ê¸°ëŠ¥ í™œì„±í™”
    use_pseudo_labeling: bool = True       # Pseudo Labelingìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
    pseudo_confidence_threshold: float = 0.95
    use_knowledge_distillation: bool = True  # Knowledge Distillation í™œì„±í™”
    use_mixup_cutmix: bool = True         # MixUp/CutMixë¡œ ì •ê·œí™”
    use_test_time_augmentation: bool = True  # TTAë¡œ ì˜ˆì¸¡ ì•ˆì •ì„± í™•ë³´
    tta_rounds: int = 8                   # 8ë¼ìš´ë“œ TTA
    
    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í›ˆë ¨ ì„¤ì •
    max_epochs: int = 20   # ì¶©ë¶„í•œ í›ˆë ¨ì„ ìœ„í•œ 20 ì—í¬í¬
    patience: int = 5      # Early stoppingìœ¼ë¡œ ê³¼ì í•© ë°©ì§€
    gradient_accumulation_steps: int = 4  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ Gradient Accumulation
    mixed_precision: bool = True  # ì†ë„ í–¥ìƒ
    
    # ì•ˆì •ì ì¸ í•™ìŠµì„ ìœ„í•œ ìŠ¤ì¼€ì¤„ë§ ì„¤ì •
    warmup_epochs: int = 2  # ì•ˆì •ì ì¸ í•™ìŠµì„ ìœ„í•œ Warmup
    cosine_restarts: bool = False  # ë³µì¡í•œ ìŠ¤ì¼€ì¤„ë§ ë¹„í™œì„±í™”
    
    # ìµœê³  ì„±ëŠ¥ì„ ìœ„í•œ ì •ê·œí™” í™œì„±í™”
    label_smoothing: float = 0.1   # Label Smoothingìœ¼ë¡œ ì •ê·œí™”
    mixup_alpha: float = 0.2       # MixUp í™œì„±í™”
    cutmix_alpha: float = 0.0     # CutMix ë¹„í™œì„±í™”
    dropout_rate: float = 0.0  # Dropout ë¹„í™œì„±í™”ë¡œ ì†ë„ í–¥ìƒ
    
    # ì‹¤í—˜ ê´€ë¦¬
    experiment_name: str = "grandmaster_v1"
    save_every_fold: bool = True
    track_metrics: List[str] = field(default_factory=lambda: [
        'accuracy', 'f1_macro', 'f1_weighted', 'precision', 'recall'
    ])


class AdvancedLossFunction(nn.Module):
    """
    ê³ ê¸‰ ì†ì‹¤ í•¨ìˆ˜ ì¡°í•©
    ìºê¸€ ëŒ€íšŒ ìµœì í™”ë¥¼ ìœ„í•œ ë‹¤ì¤‘ ì†ì‹¤ í•¨ìˆ˜
    """
    
    def __init__(self, 
                 class_weights: Optional[torch.Tensor] = None,
                 focal_gamma: float = 3.0,
                 label_smoothing: float = 0.1,
                 arcface_margin: float = 0.5,
                 use_arcface: bool = False):
        super().__init__()
        
        self.use_arcface = use_arcface
        
        # ì£¼ìš” ì†ì‹¤ í•¨ìˆ˜
        self.focal_loss = AdvancedFocalLoss(
            alpha=class_weights,
            gamma=focal_gamma,
            label_smoothing=label_smoothing
        )
        
        # Cross Entropy (ë°±ì—…)
        self.ce_loss = nn.CrossEntropyLoss(
            weight=class_weights,
            label_smoothing=label_smoothing
        )
        
        # ArcFace (ì„ íƒì )
        if use_arcface:
            self.arcface = ArcMarginProduct(512, 17, margin=arcface_margin)
    
    def forward(self, 
                outputs: torch.Tensor, 
                targets: torch.Tensor,
                features: Optional[torch.Tensor] = None,
                epoch: int = 0) -> torch.Tensor:
        
        # ì£¼ìš” ì†ì‹¤: Focal Loss
        focal_loss = self.focal_loss(outputs, targets)
        
        # ë³´ì¡° ì†ì‹¤: Cross Entropy
        ce_loss = self.ce_loss(outputs, targets)
        
        # ì†ì‹¤ ê°€ì¤‘ì¹˜ (ì—í¬í¬ì— ë”°ë¼ ì¡°ì •)
        focal_weight = 0.8 + 0.2 * min(epoch / 20, 1.0)  # ì ì§„ì  ì¦ê°€
        ce_weight = 1.0 - focal_weight
        
        total_loss = focal_weight * focal_loss + ce_weight * ce_loss
        
        # ArcFace ì¶”ê°€ (ì„ íƒì )
        if self.use_arcface and features is not None:
            arcface_outputs = self.arcface(features, targets)
            arcface_loss = F.cross_entropy(arcface_outputs, targets)
            total_loss = total_loss + 0.1 * arcface_loss
        
        return total_loss


class ArcMarginProduct(nn.Module):
    """ArcFace: Additive Angular Margin Loss"""
    
    def __init__(self, in_features: int, out_features: int, 
                 scale: float = 30.0, margin: float = 0.50):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.scale = scale
        self.margin = margin
        
        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))
        nn.init.xavier_uniform_(self.weight)
        
        self.cos_m = math.cos(margin)
        self.sin_m = math.sin(margin)
        self.th = math.cos(math.pi - margin)
        self.mm = math.sin(math.pi - margin) * margin
    
    def forward(self, features: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        cosine = F.linear(F.normalize(features), F.normalize(self.weight))
        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))
        
        phi = cosine * self.cos_m - sine * self.sin_m
        phi = torch.where(cosine > self.th, phi, cosine - self.mm)
        
        one_hot = torch.zeros(cosine.size(), device=features.device)
        one_hot.scatter_(1, targets.view(-1, 1).long(), 1)
        
        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)
        output *= self.scale
        
        return output


class EnsembleModel(nn.Module):
    """
    ì•™ìƒë¸” ëª¨ë¸ ë˜í¼
    ë‹¤ì¤‘ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ê²°í•©
    """
    
    def __init__(self, models: List[nn.Module], ensemble_method: str = "soft_voting"):
        super().__init__()
        self.models = nn.ModuleList(models)
        self.ensemble_method = ensemble_method
        self.num_models = len(models)
        
        # ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜ (í•™ìŠµ ê°€ëŠ¥)
        if ensemble_method == "weighted_voting":
            self.model_weights = nn.Parameter(torch.ones(self.num_models))
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        outputs = []
        
        for model in self.models:
            with torch.no_grad() if not self.training else torch.enable_grad():
                output = model(x)
                outputs.append(F.softmax(output, dim=1))
        
        if self.ensemble_method == "soft_voting":
            # ë‹¨ìˆœ í‰ê· 
            ensemble_output = torch.stack(outputs).mean(0)
            
        elif self.ensemble_method == "weighted_voting":
            # ê°€ì¤‘ í‰ê· 
            weights = F.softmax(self.model_weights, dim=0)
            ensemble_output = sum(w * out for w, out in zip(weights, outputs))
            
        else:  # hard_voting
            # í•˜ë“œ ë³´íŒ…
            predictions = [torch.argmax(out, dim=1) for out in outputs]
            ensemble_prediction = torch.mode(torch.stack(predictions), dim=0)[0]
            # ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ ë³€í™˜
            ensemble_output = F.one_hot(ensemble_prediction, num_classes=outputs[0].size(1)).float()
        
        return ensemble_output


class PseudoLabelingTrainer:
    """
    Pseudo Labeling í›ˆë ¨ ê´€ë¦¬ì
    ì ì§„ì  Pseudo Label ìƒì„± ë° ì ìš©
    """
    
    def __init__(self, 
                 models: List[nn.Module],
                 confidence_threshold: float = 0.95,
                 agreement_threshold: int = 3):
        self.models = models
        self.confidence_threshold = confidence_threshold
        self.agreement_threshold = agreement_threshold
        self.pseudo_history = defaultdict(list)
    
    def generate_pseudo_labels(self, 
                             test_loader: DataLoader,
                             device: torch.device) -> Tuple[List, List, List]:
        """
        ì•™ìƒë¸” í•©ì˜ ê¸°ë°˜ ê³ í’ˆì§ˆ Pseudo Label ìƒì„±
        
        Returns:
            pseudo_data, pseudo_labels, confidence_scores
        """
        all_predictions = []
        all_data = []
        
        # ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ ìˆ˜ì§‘
        for model in self.models:
            model.eval()
            predictions = []
            data_batch = []
            
            with torch.no_grad():
                for batch_data in test_loader:
                    # Multi-Modal ë°ì´í„° ì²˜ë¦¬
                    if len(batch_data) == 3:  # image, metadata, target
                        data, metadata, _ = batch_data
                        data = data.to(device)
                    else:  # image, target
                        data, _ = batch_data
                        data = data.to(device)
                    
                    outputs = model(data)
                    predictions.append(F.softmax(outputs, dim=1))
                    data_batch.append(data)
            
            all_predictions.append(torch.cat(predictions, dim=0))
            if not all_data:
                all_data = torch.cat(data_batch, dim=0)
        
        # ì•™ìƒë¸” í•©ì˜ í™•ì¸
        ensemble_preds = torch.stack(all_predictions)
        pred_labels = ensemble_preds.argmax(dim=2)
        
        pseudo_data, pseudo_labels, confidence_scores = [], [], []
        
        for i in range(pred_labels.shape[1]):
            sample_preds = pred_labels[:, i]
            
            # í•©ì˜ í™•ì¸
            label_counts = torch.bincount(sample_preds, minlength=17)
            max_count = label_counts.max()
            
            if max_count >= self.agreement_threshold:
                agreed_label = label_counts.argmax()
                confidence = ensemble_preds[:, i, agreed_label].mean()
                
                if confidence >= self.confidence_threshold:
                    pseudo_data.append(all_data[i])
                    pseudo_labels.append(agreed_label)
                    confidence_scores.append(confidence)
        
        print(f"ğŸ·ï¸ Pseudo Labels ìƒì„±: {len(pseudo_labels)}ê°œ (ì‹ ë¢°ë„ > {self.confidence_threshold})")
        
        return pseudo_data, pseudo_labels, confidence_scores


class GrandmasterTrainer:
    """
    ğŸ† ê·¸ëœë“œë§ˆìŠ¤í„° ìˆ˜ì¤€ì˜ í›ˆë ¨ ê´€ë¦¬ì
    
    íŠ¹ì§•:
    - Multi-Architecture Ensemble Training
    - Progressive Pseudo Labeling
    - Advanced Regularization
    - Test-Time Augmentation
    - Knowledge Distillation
    """
    
    def __init__(self, 
                 config: GrandmasterModelConfig,
                 processor: Optional[Any] = None):
        self.config = config
        self.processor = processor
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ëª¨ë¸ë“¤ ì´ˆê¸°í™”
        self.models = []
        self.optimizers = []
        self.schedulers = []
        self.scaler = GradScaler() if config.mixed_precision else None
        
        # ë©”íŠ¸ë¦­ ì¶”ì 
        self.training_history = defaultdict(list)
        self.best_scores = {}
        
        print(f"ğŸ† Grandmaster Trainer ì´ˆê¸°í™”")
        print(f"   ì „ëµ: {config.strategy.value}")
        print(f"   ëª©í‘œ ì ìˆ˜: {config.target_score}")
        print(f"   ì•™ìƒë¸” í¬ê¸°: {config.ensemble_size}")
    
    def create_ensemble_models(self) -> List[nn.Module]:
        """ì•™ìƒë¸” ëª¨ë¸ë“¤ ìƒì„±"""
        models = []
        
        print(f"ğŸ§  ì•™ìƒë¸” ëª¨ë¸ ìƒì„± ì¤‘... ({len(self.config.ensemble_architectures)}ê°œ)")
        
        for i, arch in enumerate(self.config.ensemble_architectures):
            print(f"   ëª¨ë¸ {i+1}: {arch}")
            
            try:
                if DEPENDENCIES_AVAILABLE:
                    model = ImprovedModelFactory.create_model(
                        architecture=arch,
                        num_classes=17,
                        pretrained=True,
                        dropout_rate=self.config.dropout_rate,
                        use_attention=True,
                        use_gem_pooling=True
                    )
                else:
                    # ê¸°ë³¸ êµ¬í˜„ ì‚¬ìš©
                    model = self._create_basic_model(arch)
                
                models.append(model.to(self.device))
                
            except Exception as e:
                print(f"âš ï¸ {arch} ìƒì„± ì‹¤íŒ¨: {e}")
                print(f"   ê¸°ë³¸ ëª¨ë¸ë¡œ ëŒ€ì²´")
                model = self._create_basic_model("efficientnet_b3")
                models.append(model.to(self.device))
        
        self.models = models
        return models
    
    def _create_basic_model(self, arch: str) -> nn.Module:
        """ê¸°ë³¸ ëª¨ë¸ ìƒì„± (ì˜ì¡´ì„± ì—†ì´)"""
        class BasicModel(nn.Module):
            def __init__(self, arch, num_classes=17):
                super().__init__()
                self.backbone = timm.create_model(arch, pretrained=True, num_classes=0)
                self.classifier = nn.Linear(self.backbone.num_features, num_classes)
            
            def forward(self, x):
                features = self.backbone(x)
                return self.classifier(features)
        
        return BasicModel(arch)
    
    def setup_training_components(self):
        """í›ˆë ¨ ì»´í¬ë„ŒíŠ¸ ì„¤ì •"""
        
        # ì˜µí‹°ë§ˆì´ì €ë“¤ ìƒì„±
        for model in self.models:
            optimizer = torch.optim.AdamW(
                model.parameters(),
                lr=1e-4,
                weight_decay=1e-2,
                betas=(0.9, 0.999)
            )
            self.optimizers.append(optimizer)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬
            if self.config.cosine_restarts:
                scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
                    optimizer, T_0=10, T_mult=2, eta_min=1e-6
                )
            else:
                scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                    optimizer, T_max=self.config.max_epochs, eta_min=1e-6
                )
            self.schedulers.append(scheduler)
    
    def train_ensemble(self, 
                      train_loader: DataLoader,
                      valid_loader: DataLoader,
                      test_loader: Optional[DataLoader] = None) -> Dict[str, Any]:
        """
        ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨
        
        Returns:
            í›ˆë ¨ ê²°ê³¼ ë° ìµœì¢… ì„±ëŠ¥ ë©”íŠ¸ë¦­
        """
        
        print(f"\nğŸš€ ì•™ìƒë¸” í›ˆë ¨ ì‹œì‘!")
        print(f"   ëª¨ë¸ ìˆ˜: {len(self.models)}")
        print(f"   ìµœëŒ€ ì—í¬í¬: {self.config.max_epochs}")
        
        # ì†ì‹¤ í•¨ìˆ˜ ìƒì„±
        if DEPENDENCIES_AVAILABLE and hasattr(self.processor, 'config'):
            class_weights = torch.tensor([
                self.processor.config.class_weights.get(i, 1.0) 
                for i in range(17)
            ], dtype=torch.float32).to(self.device)
        else:
            class_weights = None
        
        criterion = AdvancedLossFunction(
            class_weights=class_weights,
            focal_gamma=3.0,
            label_smoothing=self.config.label_smoothing
        )
        
        # Pseudo Labeling ê´€ë¦¬ì
        pseudo_trainer = None
        if self.config.use_pseudo_labeling and test_loader:
            pseudo_trainer = PseudoLabelingTrainer(
                models=self.models,
                confidence_threshold=self.config.pseudo_confidence_threshold
            )
        
        best_ensemble_score = 0.0
        patience_counter = 0
        
        # ì „ì²´ í›ˆë ¨ ì§„í–‰ ìƒíƒœ
        epoch_pbar = tqdm(range(self.config.max_epochs), desc="ğŸ† ì „ì²´ í›ˆë ¨ ì§„í–‰", position=0)
        
        for epoch in epoch_pbar:
            epoch_pbar.set_description(f"ğŸ† Epoch {epoch+1}/{self.config.max_epochs}")
            
            # ê°œë³„ ëª¨ë¸ ìˆœì°¨ í›ˆë ¨ (ë©”ëª¨ë¦¬ ìµœì í™”)
            epoch_losses = []
            model_pbar = tqdm(
                enumerate(zip(self.models, self.optimizers, self.schedulers)), 
                total=len(self.models),
                desc="ğŸ§  ëª¨ë¸ í›ˆë ¨",
                position=1,
                leave=False
            )
            
            for i, (model, optimizer, scheduler) in model_pbar:
                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                torch.cuda.empty_cache()
                
                model.train()
                epoch_loss = 0.0
                num_batches = 0
                
                # ë°°ì¹˜ ì§„í–‰ ìƒíƒœ ë°”
                batch_pbar = tqdm(
                    enumerate(train_loader), 
                    total=len(train_loader),
                    desc=f"ğŸ“Š ëª¨ë¸ {i+1}/{len(self.models)} ë°°ì¹˜",
                    position=2,
                    leave=False
                )
                
                for batch_idx, batch_data in batch_pbar:
                    # Multi-Modal ë°ì´í„° ì²˜ë¦¬
                    if len(batch_data) == 3:  # image, metadata, target
                        data, metadata, targets = batch_data
                        data = data.to(self.device)
                        metadata = metadata.to(self.device) if metadata is not None else None
                        targets = targets.to(self.device)
                    else:  # image, target
                        data, targets = batch_data
                        data, targets = data.to(self.device), targets.to(self.device)
                        metadata = None
                    
                    # Mixed Precision Training
                    if self.config.mixed_precision and self.scaler:
                        with autocast():
                            outputs = model(data)
                            loss = criterion(outputs, targets, epoch=epoch)
                    else:
                        outputs = model(data)
                        loss = criterion(outputs, targets, epoch=epoch)
                    
                    # Gradient Accumulation
                    loss = loss / self.config.gradient_accumulation_steps
                    
                    if self.config.mixed_precision and self.scaler:
                        self.scaler.scale(loss).backward()
                        
                        if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:
                            self.scaler.step(optimizer)
                            self.scaler.update()
                            optimizer.zero_grad()
                    else:
                        loss.backward()
                        
                        if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:
                            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                            optimizer.step()
                            optimizer.zero_grad()
                    
                    epoch_loss += loss.item() * self.config.gradient_accumulation_steps
                    num_batches += 1
                    
                    # ì‹¤ì‹œê°„ ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
                    current_avg_loss = epoch_loss / num_batches
                    batch_pbar.set_postfix({
                        'Loss': f'{current_avg_loss:.4f}',
                        'GPU': f'{torch.cuda.memory_allocated()/1024**3:.1f}GB'
                    })
                
                epoch_losses.append(epoch_loss / num_batches)
                scheduler.step()
                
                # ëª¨ë¸ë³„ ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
                model_pbar.set_postfix({
                    'Loss': f'{epoch_losses[-1]:.4f}',
                    'GPU': f'{torch.cuda.memory_allocated()/1024**3:.1f}GB'
                })
            
            # ê²€ì¦ í‰ê°€
            ensemble_score = self._evaluate_ensemble(valid_loader)
            
            # ì „ì²´ ì—í¬í¬ ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
            epoch_pbar.set_postfix({
                'Val_Score': f'{ensemble_score:.4f}',
                'Best': f'{best_ensemble_score:.4f}',
                'Patience': f'{patience_counter}/{self.config.patience}'
            })
            
            # ìµœê³  ì„±ëŠ¥ ì—…ë°ì´íŠ¸
            if ensemble_score > best_ensemble_score:
                best_ensemble_score = ensemble_score
                patience_counter = 0
                self._save_best_models(epoch)
                print(f"   ğŸ‰ ìƒˆë¡œìš´ ìµœê³  ì ìˆ˜!")
            else:
                patience_counter += 1
            
            # Pseudo Labeling (ì¤‘ë°˜ë¶€í„° ì ìš©)
            if (pseudo_trainer and epoch >= self.config.max_epochs // 3 and 
                epoch % 5 == 0):  # 5 ì—í¬í¬ë§ˆë‹¤
                
                print(f"   ğŸ·ï¸ Pseudo Labeling ì ìš©...")
                pseudo_data, pseudo_labels, confidence_scores = pseudo_trainer.generate_pseudo_labels(
                    test_loader, self.device
                )
                
                if len(pseudo_data) > 0:
                    # Pseudo ë°ì´í„°ë¡œ ì¶”ê°€ í›ˆë ¨
                    self._train_with_pseudo_data(pseudo_data, pseudo_labels)
            
            # Early Stopping
            if patience_counter >= self.config.patience:
                print(f"   â° Early Stopping (patience: {self.config.patience})")
                break
            
            # ëª©í‘œ ì ìˆ˜ ë‹¬ì„± ì‹œ ì¢…ë£Œ
            if ensemble_score >= self.config.target_score:
                print(f"   ğŸ† ëª©í‘œ ì ìˆ˜ ë‹¬ì„±! ({self.config.target_score})")
                break
        
        print(f"\nâœ… ì•™ìƒë¸” í›ˆë ¨ ì™„ë£Œ!")
        print(f"   ìµœê³  ê²€ì¦ ì ìˆ˜: {best_ensemble_score:.4f}")
        
        return {
            'best_score': best_ensemble_score,
            'training_history': dict(self.training_history),
            'models': self.models,
            'final_epoch': epoch + 1
        }
    
    def _evaluate_ensemble(self, data_loader: DataLoader) -> float:
        """ì•™ìƒë¸” ëª¨ë¸ í‰ê°€"""
        
        all_predictions = []
        all_targets = []
        
        # ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ ìˆœì°¨ ìˆ˜ì§‘ (ë©”ëª¨ë¦¬ ìµœì í™”)
        eval_pbar = tqdm(self.models, desc="ğŸ” ì•™ìƒë¸” í‰ê°€", position=1, leave=False)
        for model in eval_pbar:
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            torch.cuda.empty_cache()
            
            model.eval()
            model_predictions = []
            
            with torch.no_grad():
                for batch_data in data_loader:
                    # Multi-Modal ë°ì´í„° ì²˜ë¦¬
                    if len(batch_data) == 3:  # image, metadata, target
                        data, metadata, targets = batch_data
                        data = data.to(self.device)
                    else:  # image, target
                        data, targets = batch_data
                        data = data.to(self.device)
                    
                    outputs = model(data)
                    predictions = F.softmax(outputs, dim=1)
                    model_predictions.append(predictions)
                    
                    if len(all_targets) == 0:  # ì²« ë²ˆì§¸ ëª¨ë¸ì—ì„œë§Œ íƒ€ê²Ÿ ìˆ˜ì§‘
                        all_targets.extend(targets.cpu().numpy())
            
            all_predictions.append(torch.cat(model_predictions, dim=0))
        
        # ì•™ìƒë¸” ì˜ˆì¸¡ (ì†Œí”„íŠ¸ ë³´íŒ…)
        ensemble_predictions = torch.stack(all_predictions).mean(0)
        ensemble_labels = ensemble_predictions.argmax(dim=1).cpu().numpy()
        
        # ì •í™•ë„ ê³„ì‚° (í¬ê¸° ë§ì¶¤)
        all_targets_array = np.array(all_targets)
        if len(ensemble_labels) != len(all_targets_array):
            # í¬ê¸°ê°€ ë‹¤ë¥´ë©´ ë” ì‘ì€ í¬ê¸°ì— ë§ì¶¤
            min_len = min(len(ensemble_labels), len(all_targets_array))
            ensemble_labels = ensemble_labels[:min_len]
            all_targets_array = all_targets_array[:min_len]
        
        accuracy = (ensemble_labels == all_targets_array).mean()
        
        return accuracy
    
    def _train_with_pseudo_data(self, pseudo_data: List, pseudo_labels: List):
        """Pseudo ë°ì´í„°ë¡œ ì¶”ê°€ í›ˆë ¨"""
        
        if len(pseudo_data) == 0:
            return
        
        # Pseudo ë°ì´í„°ë¥¼ ë°°ì¹˜ë¡œ êµ¬ì„±
        pseudo_dataset = list(zip(pseudo_data, pseudo_labels))
        
        # ê°„ë‹¨í•œ ì¶”ê°€ í›ˆë ¨ (1-2 ì—í¬í¬)
        for model, optimizer in zip(self.models, self.optimizers):
            model.train()
            
            # ë°°ì¹˜ë³„ í›ˆë ¨
            batch_size = 32
            for i in range(0, len(pseudo_dataset), batch_size):
                batch = pseudo_dataset[i:i+batch_size]
                if len(batch) == 0:
                    continue
                
                data_batch = torch.stack([item[0] for item in batch])
                label_batch = torch.tensor([item[1] for item in batch])
                
                data_batch = data_batch.to(self.device)
                label_batch = label_batch.to(self.device)
                
                optimizer.zero_grad()
                outputs = model(data_batch)
                loss = F.cross_entropy(outputs, label_batch)
                loss.backward()
                optimizer.step()
    
    def _save_best_models(self, epoch: int):
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë“¤ ì €ì¥"""
        
        if not self.config.save_every_fold:
            return
        
        save_dir = Path(f"./saved_models/{self.config.experiment_name}")
        save_dir.mkdir(parents=True, exist_ok=True)
        
        for i, model in enumerate(self.models):
            model_path = save_dir / f"model_{i}_epoch_{epoch}.pt"
            torch.save({
                'model_state_dict': model.state_dict(),
                'architecture': self.config.ensemble_architectures[i],
                'epoch': epoch,
                'config': self.config.__dict__
            }, model_path)
        
        print(f"   ğŸ’¾ ëª¨ë¸ë“¤ ì €ì¥: {save_dir}")
    
    def predict_with_tta(self, test_loader: DataLoader) -> np.ndarray:
        """
        Test-Time Augmentationì„ ì‚¬ìš©í•œ ì˜ˆì¸¡
        
        Returns:
            ìµœì¢… ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼
        """
        
        if not self.config.use_test_time_augmentation:
            return self._predict_simple(test_loader)
        
        print(f"ğŸ”® TTA ì˜ˆì¸¡ ì‹œì‘ (ë¼ìš´ë“œ: {self.config.tta_rounds})")
        
        all_tta_predictions = []
        
        for tta_round in range(self.config.tta_rounds):
            round_predictions = []
            
            for model in self.models:
                model.eval()
                model_predictions = []
                
                with torch.no_grad():
                    for batch_data in test_loader:
                        # Multi-Modal ë°ì´í„° ì²˜ë¦¬
                        if len(batch_data) == 3:  # image, metadata, target
                            data, metadata, _ = batch_data
                            data = data.to(self.device)
                        else:  # image, target
                            data, _ = batch_data
                            data = data.to(self.device)
                        
                        # TTA ë³€í˜• ì ìš© (ê°„ë‹¨í•œ ë²„ì „)
                        if tta_round % 2 == 1:  # ìˆ˜í‰ í”Œë¦½
                            data = torch.flip(data, dims=[3])
                        if tta_round >= 4:  # ì‘ì€ íšŒì „
                            # ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ TTA êµ¬í˜„
                            pass
                        
                        outputs = model(data)
                        predictions = F.softmax(outputs, dim=1)
                        model_predictions.append(predictions)
                
                round_predictions.append(torch.cat(model_predictions, dim=0))
            
            # ë¼ìš´ë“œë³„ ì•™ìƒë¸”
            round_ensemble = torch.stack(round_predictions).mean(0)
            all_tta_predictions.append(round_ensemble)
        
        # ìµœì¢… TTA ì•™ìƒë¸”
        final_predictions = torch.stack(all_tta_predictions).mean(0)
        final_labels = final_predictions.argmax(dim=1).cpu().numpy()
        
        print(f"âœ… TTA ì˜ˆì¸¡ ì™„ë£Œ")
        
        return final_labels
    
    def _predict_simple(self, test_loader: DataLoader) -> np.ndarray:
        """ë‹¨ìˆœ ì•™ìƒë¸” ì˜ˆì¸¡"""
        
        all_predictions = []
        
        for model in self.models:
            model.eval()
            model_predictions = []
            
            with torch.no_grad():
                for batch_data in test_loader:
                    # Multi-Modal ë°ì´í„° ì²˜ë¦¬
                    if len(batch_data) == 3:  # image, metadata, target
                        data, metadata, _ = batch_data
                        data = data.to(self.device)
                    else:  # image, target
                        data, _ = batch_data
                        data = data.to(self.device)
                    outputs = model(data)
                    predictions = F.softmax(outputs, dim=1)
                    model_predictions.append(predictions)
            
            all_predictions.append(torch.cat(model_predictions, dim=0))
        
        # ì•™ìƒë¸” ì˜ˆì¸¡
        ensemble_predictions = torch.stack(all_predictions).mean(0)
        ensemble_labels = ensemble_predictions.argmax(dim=1).cpu().numpy()
        
        return ensemble_labels


def create_grandmaster_modeling_system(
    strategy: str = "diverse_ensemble",
    target_score: float = 0.95,
    experiment_name: str = None
) -> Tuple[GrandmasterTrainer, Any]:
    """
    ê·¸ëœë“œë§ˆìŠ¤í„° ëª¨ë¸ë§ ì‹œìŠ¤í…œ ìƒì„±
    
    Args:
        strategy: ëª¨ë¸ë§ ì „ëµ
        target_score: ëª©í‘œ ì ìˆ˜
        experiment_name: ì‹¤í—˜ ì´ë¦„
        
    Returns:
        (trainer, processor) íŠœí”Œ
    """
    
    # ì„¤ì • ìƒì„±
    config = GrandmasterModelConfig(
        strategy=ModelingStrategy(strategy),
        target_score=target_score,
        experiment_name=experiment_name or f"grandmaster_{datetime.now().strftime('%m%d_%H%M')}"
    )
    
    # ì „ì²˜ë¦¬ í”„ë¡œì„¸ì„œ ìƒì„± (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
    processor = None
    if DEPENDENCIES_AVAILABLE:
        try:
            processor = create_grandmaster_processor(
                strategy="eda_optimized",
                image_size=640,
                experiment_name=config.experiment_name
            )
            print(f"âœ… ì „ì²˜ë¦¬ í”„ë¡œì„¸ì„œ ì—°ê²° ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ì „ì²˜ë¦¬ í”„ë¡œì„¸ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„±
    trainer = GrandmasterTrainer(config, processor)
    
    return trainer, processor


# ë©”ì¸ ì‹¤í–‰ ì˜ˆì‹œ
if __name__ == "__main__":
    print("ğŸ† ìºê¸€ ê·¸ëœë“œë§ˆìŠ¤í„° ëª¨ë¸ë§ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # ì‹œìŠ¤í…œ ìƒì„±
    trainer, processor = create_grandmaster_modeling_system(
        strategy="diverse_ensemble",
        target_score=0.95,
        experiment_name="test_grandmaster"
    )
    
    # ëª¨ë¸ ìƒì„±
    models = trainer.create_ensemble_models()
    trainer.setup_training_components()
    
    print(f"\nâœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
    print(f"   ìƒì„±ëœ ëª¨ë¸ ìˆ˜: {len(models)}")
    print(f"   ëª©í‘œ ì ìˆ˜: {trainer.config.target_score}")
    print(f"   ì „ëµ: {trainer.config.strategy.value}")
    
    # ì‹¤ì œ í›ˆë ¨ì€ ë°ì´í„° ë¡œë”ì™€ í•¨ê»˜ ì‹¤í–‰
    # training_results = trainer.train_ensemble(train_loader, valid_loader, test_loader)
    
    print(f"\nğŸ¯ ìºê¸€ 1ë“±ì„ ìœ„í•œ ì¤€ë¹„ ì™„ë£Œ!")
    print(f"   ë‹¤ìŒ ë‹¨ê³„: ì‹¤ì œ ë°ì´í„°ë¡œ í›ˆë ¨ ì‹¤í–‰")
