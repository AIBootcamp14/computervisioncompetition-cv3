"""
ğŸ§  Improved Model Factory
í´ë¦° ì½”ë“œì™€ í´ë¦° ì•„í‚¤í…ì²˜ë¥¼ ì ìš©í•œ ê°œì„ ëœ ëª¨ë¸ íŒ©í† ë¦¬

Features:
- ë” ê°•ë ¥í•œ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì§€ì›
- ê³ ê¸‰ ì†ì‹¤ í•¨ìˆ˜ (Focal Loss, Label Smoothing ë“±)
- ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ í†µí•©
- ì•™ìƒë¸”ì„ ìœ„í•œ ë‹¤ì¤‘ ëª¨ë¸ ì§€ì›
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ êµ¬í˜„
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import timm
from typing import Optional, Dict, Any, Union, List
import math
import numpy as np
from abc import ABC, abstractmethod


class BaseDocumentClassifier(nn.Module, ABC):
    """
    ë¬¸ì„œ ë¶„ë¥˜ ëª¨ë¸ì˜ ì¶”ìƒ ê¸°ë°˜ í´ë˜ìŠ¤
    í´ë¦° ì•„í‚¤í…ì²˜ ì›ì¹™: ì˜ì¡´ì„± ì—­ì „
    """
    
    def __init__(self, num_classes: int = 17):
        super().__init__()
        self.num_classes = num_classes
    
    @abstractmethod
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """ìˆœì „íŒŒ ì¶”ìƒ ë©”ì„œë“œ"""
        pass
    
    @abstractmethod
    def get_features(self, x: torch.Tensor) -> torch.Tensor:
        """íŠ¹ì§• ë²¡í„° ì¶”ì¶œ ì¶”ìƒ ë©”ì„œë“œ"""
        pass


class AttentionModule(nn.Module):
    """
    ì±„ë„ ì–´í…ì…˜ê³¼ ê³µê°„ ì–´í…ì…˜ì„ ê²°í•©í•œ ëª¨ë“ˆ
    ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ í•µì‹¬ ì»´í¬ë„ŒíŠ¸
    """
    
    def __init__(self, in_channels: int, reduction: int = 16):
        super().__init__()
        
        # ì±„ë„ ì–´í…ì…˜ (Squeeze-and-Excitation)
        self.channel_attention = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False),
            nn.Sigmoid()
        )
        
        # ê³µê°„ ì–´í…ì…˜ (Spatial Attention)
        self.spatial_attention = nn.Sequential(
            nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False),
            nn.Sigmoid()
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # ì±„ë„ ì–´í…ì…˜ ì ìš©
        ca_weight = self.channel_attention(x)
        x = x * ca_weight
        
        # ê³µê°„ ì–´í…ì…˜ì„ ìœ„í•œ í†µê³„ ê³„ì‚°
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        spatial_input = torch.cat([avg_out, max_out], dim=1)
        
        # ê³µê°„ ì–´í…ì…˜ ì ìš©
        sa_weight = self.spatial_attention(spatial_input)
        x = x * sa_weight
        
        return x


class GeMPooling(nn.Module):
    """
    Generalized Mean Pooling
    Global Average Poolingë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥
    """
    
    def __init__(self, p: float = 3.0, eps: float = 1e-6):
        super().__init__()
        self.p = nn.Parameter(torch.ones(1) * p)
        self.eps = eps
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return F.avg_pool2d(
            x.clamp(min=self.eps).pow(self.p), 
            (x.size(-2), x.size(-1))
        ).pow(1. / self.p)


class ImprovedDocumentClassifier(BaseDocumentClassifier):
    """
    ê°œì„ ëœ ë¬¸ì„œ ë¶„ë¥˜ ëª¨ë¸
    ë” ê°•ë ¥í•œ ì•„í‚¤í…ì²˜ì™€ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ í†µí•©
    """
    
    def __init__(
        self,
        architecture: str = "efficientnet_b3",
        num_classes: int = 17,
        pretrained: bool = True,
        dropout_rate: float = 0.4,
        use_gem_pooling: bool = True,
        use_attention: bool = True,
        use_mixup: bool = True,
        image_size: int = 512
    ):
        """
        Args:
            architecture: ë°±ë³¸ ì•„í‚¤í…ì²˜ (ë” ê°•ë ¥í•œ ëª¨ë¸ë“¤)
            num_classes: í´ë˜ìŠ¤ ìˆ˜
            pretrained: ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©
            dropout_rate: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
            use_gem_pooling: GeM í’€ë§ ì‚¬ìš© ì—¬ë¶€
            use_attention: ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ì‚¬ìš© ì—¬ë¶€
            use_mixup: MixUp ì§€ì› ì—¬ë¶€
            image_size: ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°
        """
        super().__init__(num_classes)
        
        self.architecture = architecture
        self.use_attention = use_attention
        self.use_mixup = use_mixup
        self.image_size = image_size
        
        # ë°±ë³¸ ëª¨ë¸ ìƒì„±
        self.backbone = self._create_backbone(architecture, pretrained)
        
        # íŠ¹ì§• ì°¨ì› ë™ì  ê³„ì‚°
        self.feature_dim = self._get_feature_dim()
        
        # ì–´í…ì…˜ ëª¨ë“ˆ (ì„ íƒì )
        if use_attention:
            self.attention = AttentionModule(self.feature_dim)
        
        # í’€ë§ ë ˆì´ì–´
        if use_gem_pooling:
            self.global_pool = GeMPooling()
        else:
            self.global_pool = nn.AdaptiveAvgPool2d(1)
        
        # í–¥ìƒëœ ë¶„ë¥˜ í—¤ë“œ
        self.classifier = self._create_classifier_head(dropout_rate)
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._init_weights()
    
    def _create_backbone(self, architecture: str, pretrained: bool) -> nn.Module:
        """
        ë°±ë³¸ ëª¨ë¸ ìƒì„±
        ë” ê°•ë ¥í•œ ì•„í‚¤í…ì²˜ë“¤ ì§€ì›
        """
        try:
            backbone = timm.create_model(
                architecture,
                pretrained=pretrained,
                num_classes=0,  # ë¶„ë¥˜ í—¤ë“œ ì œê±°
                global_pool='',  # í’€ë§ ì œê±°
                drop_rate=0.0   # ë°±ë³¸ì˜ ë“œë¡­ì•„ì›ƒ ì œê±° (ë¶„ë¥˜ í—¤ë“œì—ì„œ ê´€ë¦¬)
            )
            
            print(f"âœ… ë°±ë³¸ ëª¨ë¸ ìƒì„± ì„±ê³µ: {architecture}")
            return backbone
            
        except Exception as e:
            print(f"âš ï¸ {architecture} ìƒì„± ì‹¤íŒ¨: {e}")
            print("   ê¸°ë³¸ ëª¨ë¸(efficientnet_b0)ë¡œ ëŒ€ì²´")
            
            return timm.create_model(
                "efficientnet_b0",
                pretrained=pretrained,
                num_classes=0,
                global_pool='',
                drop_rate=0.0
            )
    
    def _get_feature_dim(self) -> int:
        """íŠ¹ì§• ì°¨ì› ë™ì  ê³„ì‚°"""
        with torch.no_grad():
            dummy_input = torch.randn(1, 3, self.image_size, self.image_size)
            features = self.backbone(dummy_input)
            return features.shape[1]
    
    def _create_classifier_head(self, dropout_rate: float) -> nn.Module:
        """
        í–¥ìƒëœ ë¶„ë¥˜ í—¤ë“œ ìƒì„±
        ë” ê¹Šì€ êµ¬ì¡°ë¡œ ì„±ëŠ¥ í–¥ìƒ
        """
        # ì¤‘ê°„ ì°¨ì› ê³„ì‚° (íŠ¹ì§• ì°¨ì›ì— ë”°ë¼ ì ì‘ì  ì¡°ì •)
        if self.feature_dim >= 2048:
            hidden_dim = 1024
        elif self.feature_dim >= 1280:
            hidden_dim = 640
        else:
            hidden_dim = 512
        
        return nn.Sequential(
            # ì²« ë²ˆì§¸ ë ˆì´ì–´
            nn.Dropout(dropout_rate),
            nn.Linear(self.feature_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(inplace=True),
            
            # ë‘ ë²ˆì§¸ ë ˆì´ì–´
            nn.Dropout(dropout_rate * 0.7),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(inplace=True),
            
            # ì¶œë ¥ ë ˆì´ì–´
            nn.Dropout(dropout_rate * 0.5),
            nn.Linear(hidden_dim // 2, self.num_classes)
        )
    
    def _init_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (Xavier/He ì´ˆê¸°í™”)"""
        for module in self.classifier.modules():
            if isinstance(module, nn.Linear):
                # He ì´ˆê¸°í™” (ReLUì™€ í•¨ê»˜ ì‚¬ìš©)
                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.BatchNorm1d):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """ìˆœì „íŒŒ"""
        # ë°±ë³¸ì„ í†µí•œ íŠ¹ì§• ì¶”ì¶œ
        features = self.backbone(x)
        
        # ì–´í…ì…˜ ì ìš© (ì„ íƒì )
        if self.use_attention:
            features = self.attention(features)
        
        # ê¸€ë¡œë²Œ í’€ë§
        features = self.global_pool(features)
        features = features.flatten(1)
        
        # ë¶„ë¥˜
        output = self.classifier(features)
        
        return output
    
    def get_features(self, x: torch.Tensor) -> torch.Tensor:
        """íŠ¹ì§• ë²¡í„° ì¶”ì¶œ (ì•™ìƒë¸”ìš©)"""
        with torch.no_grad():
            features = self.backbone(x)
            
            if self.use_attention:
                features = self.attention(features)
            
            features = self.global_pool(features)
            features = features.flatten(1)
            
        return features


class AdvancedFocalLoss(nn.Module):
    """
    ê°œì„ ëœ Focal Loss
    í´ë˜ìŠ¤ ë¶ˆê· í˜•ê³¼ ì–´ë ¤ìš´ ìƒ˜í”Œì— ë” íš¨ê³¼ì 
    """
    
    def __init__(
        self,
        alpha: Optional[torch.Tensor] = None,
        gamma: float = 3.0,  # ë” ê°•í•œ focusing
        reduction: str = 'mean',
        label_smoothing: float = 0.1
    ):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        self.label_smoothing = label_smoothing
    
    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        # Label smoothing ì ìš©
        if self.label_smoothing > 0:
            num_classes = inputs.size(-1)
            smoothed_targets = targets * (1 - self.label_smoothing) + \
                             self.label_smoothing / num_classes
            ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        else:
            ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        
        # Focal weight ê³„ì‚°
        pt = torch.exp(-ce_loss)
        focal_weight = (1 - pt) ** self.gamma
        
        # Alpha weight ì ìš©
        if self.alpha is not None:
            if self.alpha.device != inputs.device:
                self.alpha = self.alpha.to(inputs.device)
            alpha_weight = self.alpha.gather(0, targets)
            focal_loss = alpha_weight * focal_weight * ce_loss
        else:
            focal_loss = focal_weight * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


class MixUpCutMixLoss(nn.Module):
    """
    MixUpê³¼ CutMixë¥¼ ìœ„í•œ ì†ì‹¤ í•¨ìˆ˜
    ë°ì´í„° ì¦ê°•ê³¼ í•¨ê»˜ ì‚¬ìš©
    """
    
    def __init__(self, criterion: nn.Module):
        super().__init__()
        self.criterion = criterion
    
    def forward(
        self,
        outputs: torch.Tensor,
        targets_a: torch.Tensor,
        targets_b: torch.Tensor,
        lam: float
    ) -> torch.Tensor:
        """
        MixUp/CutMix ì†ì‹¤ ê³„ì‚°
        ë‘ íƒ€ê²Ÿì— ëŒ€í•œ ê°€ì¤‘ í‰ê· 
        """
        return lam * self.criterion(outputs, targets_a) + \
               (1 - lam) * self.criterion(outputs, targets_b)


class ImprovedModelFactory:
    """
    ê°œì„ ëœ ëª¨ë¸ íŒ©í† ë¦¬
    ë” ê°•ë ¥í•œ ëª¨ë¸ë“¤ê³¼ ê³ ê¸‰ ê¸°ë²• ì§€ì›
    """
    
    # ì§€ì›í•˜ëŠ” ê°•ë ¥í•œ ì•„í‚¤í…ì²˜ë“¤
    POWERFUL_ARCHITECTURES = [
        # EfficientNet ê³„ì—´ (ê· í˜•ì¡íŒ ì„±ëŠ¥)
        'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5',
        'efficientnetv2_m', 'efficientnetv2_l',
        
        # ConvNeXt ê³„ì—´ (ìµœì‹  CNN)
        'convnext_tiny', 'convnext_small', 'convnext_base',
        
        # Swin Transformer ê³„ì—´ (Vision Transformer)
        'swin_tiny_patch4_window7_224', 
        'swin_small_patch4_window7_224',
        'swin_base_patch4_window7_224',
        
        # RegNet ê³„ì—´ (íš¨ìœ¨ì ì¸ CNN)
        'regnetx_032', 'regnetx_040', 'regnetx_064',
        'regnety_032', 'regnety_040', 'regnety_064',
        
        # ResNet ê³„ì—´ (ì•ˆì •ì ì¸ ê¸°ë³¸)
        'resnet50', 'resnet101', 'resnet152',
        'resnext50_32x4d', 'resnext101_32x8d',
    ]
    
    @staticmethod
    def create_model(
        architecture: str,
        num_classes: int = 17,
        pretrained: bool = True,
        **kwargs
    ) -> ImprovedDocumentClassifier:
        """
        ê°œì„ ëœ ëª¨ë¸ ìƒì„±
        ë” ê°•ë ¥í•œ ì•„í‚¤í…ì²˜ ìš°ì„  ì‚¬ìš©
        """
        # ì•„í‚¤í…ì²˜ ê²€ì¦ ë° ì¶”ì²œ
        if architecture not in ImprovedModelFactory.POWERFUL_ARCHITECTURES:
            print(f"âš ï¸ {architecture}ëŠ” ìµœì í™”ë˜ì§€ ì•Šì€ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤.")
            
            # ì„±ëŠ¥ ê¸°ë°˜ ì¶”ì²œ
            if 'efficientnet' in architecture.lower():
                recommended = 'efficientnet_b3'
            elif 'convnext' in architecture.lower():
                recommended = 'convnext_base'
            elif 'swin' in architecture.lower():
                recommended = 'swin_base_patch4_window7_224'
            else:
                recommended = 'efficientnet_b3'  # ê¸°ë³¸ ì¶”ì²œ
            
            print(f"   ì¶”ì²œ ì•„í‚¤í…ì²˜: {recommended}")
            architecture = recommended
        
        # ëª¨ë¸ ìƒì„±
        model = ImprovedDocumentClassifier(
            architecture=architecture,
            num_classes=num_classes,
            pretrained=pretrained,
            **kwargs
        )
        
        print(f"ğŸ§  ê°œì„ ëœ ëª¨ë¸ ìƒì„± ì™„ë£Œ:")
        print(f"   ì•„í‚¤í…ì²˜: {architecture}")
        print(f"   íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
        print(f"   íŠ¹ì§• ì°¨ì›: {model.feature_dim}")
        
        return model
    
    @staticmethod
    def create_ensemble_models(
        architectures: List[str],
        num_classes: int = 17,
        pretrained: bool = True,
        **kwargs
    ) -> List[ImprovedDocumentClassifier]:
        """
        ì•™ìƒë¸”ìš© ë‹¤ì¤‘ ëª¨ë¸ ìƒì„±
        ë‹¤ì–‘í•œ ì•„í‚¤í…ì²˜ ì¡°í•©
        """
        models = []
        
        print(f"ğŸ­ ì•™ìƒë¸” ëª¨ë¸ ìƒì„± ì¤‘... ({len(architectures)}ê°œ)")
        
        for i, arch in enumerate(architectures):
            print(f"   ëª¨ë¸ {i+1}/{len(architectures)}: {arch}")
            
            model = ImprovedModelFactory.create_model(
                architecture=arch,
                num_classes=num_classes,
                pretrained=pretrained,
                **kwargs
            )
            models.append(model)
        
        print(f"âœ… ì•™ìƒë¸” ëª¨ë¸ ìƒì„± ì™„ë£Œ!")
        return models
    
    @staticmethod
    def create_loss_function(
        loss_type: str = "advanced_focal",
        class_weights: Optional[torch.Tensor] = None,
        **kwargs
    ) -> nn.Module:
        """
        ê°œì„ ëœ ì†ì‹¤ í•¨ìˆ˜ ìƒì„±
        í´ë˜ìŠ¤ ë¶ˆê· í˜•ì— ë” íš¨ê³¼ì 
        """
        if loss_type == "advanced_focal":
            return AdvancedFocalLoss(
                alpha=class_weights,
                gamma=kwargs.get('focal_gamma', 3.0),  # ë” ê°•í•œ focusing
                label_smoothing=kwargs.get('label_smoothing', 0.1)
            )
        
        elif loss_type == "focal":
            # ê¸°ì¡´ Focal Lossì™€ í˜¸í™˜ì„± - ê°„ë‹¨í•œ Focal Loss êµ¬í˜„
            class FocalLoss(nn.Module):
                def __init__(self, alpha=None, gamma=2.0):
                    super().__init__()
                    self.alpha = alpha
                    self.gamma = gamma
                
                def forward(self, inputs, targets):
                    ce_loss = F.cross_entropy(inputs, targets, reduction='none')
                    pt = torch.exp(-ce_loss)
                    focal_loss = (1 - pt) ** self.gamma * ce_loss
                    
                    if self.alpha is not None:
                        if self.alpha.device != inputs.device:
                            self.alpha = self.alpha.to(inputs.device)
                        at = self.alpha.gather(0, targets)
                        focal_loss = at * focal_loss
                    
                    return focal_loss.mean()
            
            return FocalLoss(
                alpha=class_weights,
                gamma=kwargs.get('focal_gamma', 2.0)
            )
        
        elif loss_type == "label_smoothing":
            return nn.CrossEntropyLoss(
                weight=class_weights,
                label_smoothing=kwargs.get('label_smoothing', 0.1)
            )
        
        elif loss_type == "cross_entropy":
            return nn.CrossEntropyLoss(weight=class_weights)
        
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì†ì‹¤ í•¨ìˆ˜: {loss_type}")
    
    @staticmethod
    def create_optimizer(
        model: nn.Module,
        optimizer_type: str = "adamw",
        learning_rate: float = 1e-4,
        weight_decay: float = 1e-2,
        **kwargs
    ) -> torch.optim.Optimizer:
        """
        ê°œì„ ëœ ì˜µí‹°ë§ˆì´ì € ìƒì„±
        ë” ë‚˜ì€ ê¸°ë³¸ê°’ ì‚¬ìš©
        """
        # íŒŒë¼ë¯¸í„° ê·¸ë£¹ ë¶„ë¦¬ (ë°±ë³¸ vs ë¶„ë¥˜ í—¤ë“œ)
        backbone_params = []
        classifier_params = []
        
        for name, param in model.named_parameters():
            if 'classifier' in name:
                classifier_params.append(param)
            else:
                backbone_params.append(param)
        
        if optimizer_type == "adamw":
            return torch.optim.AdamW([
                {'params': backbone_params, 'lr': learning_rate * 0.1},  # ë°±ë³¸ì€ ë‚®ì€ í•™ìŠµë¥ 
                {'params': classifier_params, 'lr': learning_rate}        # ë¶„ë¥˜ í—¤ë“œëŠ” ë†’ì€ í•™ìŠµë¥ 
            ], weight_decay=weight_decay)
        
        elif optimizer_type == "adam":
            return torch.optim.Adam([
                {'params': backbone_params, 'lr': learning_rate * 0.1},
                {'params': classifier_params, 'lr': learning_rate}
            ], weight_decay=weight_decay)
        
        elif optimizer_type == "sgd":
            return torch.optim.SGD([
                {'params': backbone_params, 'lr': learning_rate * 0.1},
                {'params': classifier_params, 'lr': learning_rate}
            ], weight_decay=weight_decay, momentum=0.9, nesterov=True)
        
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì˜µí‹°ë§ˆì´ì €: {optimizer_type}")
    
    @staticmethod
    def create_scheduler(
        optimizer: torch.optim.Optimizer,
        scheduler_type: str = "cosine_warm_restarts",
        epochs: int = 30,
        **kwargs
    ):
        """
        ê°œì„ ëœ ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±
        ë” íš¨ê³¼ì ì¸ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
        """
        if scheduler_type == "cosine_warm_restarts":
            return torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
                optimizer,
                T_0=kwargs.get('T_0', 10),
                T_mult=kwargs.get('T_mult', 2),
                eta_min=kwargs.get('eta_min', 1e-6)
            )
        
        elif scheduler_type == "cosine":
            return torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer,
                T_max=epochs,
                eta_min=kwargs.get('eta_min', 1e-6)
            )
        
        elif scheduler_type == "reduce_on_plateau":
            return torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer,
                mode='max',
                factor=kwargs.get('factor', 0.5),
                patience=kwargs.get('patience', 5),
                verbose=True
            )
        
        else:
            return None


# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    print("ğŸ§  ê°œì„ ëœ ëª¨ë¸ íŒ©í† ë¦¬ í…ŒìŠ¤íŠ¸")
    
    # ë‹¨ì¼ ê°•ë ¥í•œ ëª¨ë¸ ìƒì„±
    model = ImprovedModelFactory.create_model(
        architecture="efficientnet_b3",
        num_classes=17,
        pretrained=True,
        dropout_rate=0.4,
        use_attention=True
    )
    
    # ì•™ìƒë¸” ëª¨ë¸ë“¤ ìƒì„±
    ensemble_architectures = [
        "efficientnet_b3",
        "efficientnet_b4", 
        "convnext_base"
    ]
    
    ensemble_models = ImprovedModelFactory.create_ensemble_models(
        ensemble_architectures,
        num_classes=17
    )
    
    # ê°œì„ ëœ ì†ì‹¤ í•¨ìˆ˜ ìƒì„±
    criterion = ImprovedModelFactory.create_loss_function(
        loss_type="advanced_focal",
        focal_gamma=3.0,
        label_smoothing=0.1
    )
    
    # ê°œì„ ëœ ì˜µí‹°ë§ˆì´ì € ìƒì„±
    optimizer = ImprovedModelFactory.create_optimizer(
        model,
        optimizer_type="adamw",
        learning_rate=1e-4
    )
    
    print(f"âœ… ê°œì„ ëœ ëª¨ë¸ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"   ë‹¨ì¼ ëª¨ë¸: {model.architecture}")
    print(f"   ì•™ìƒë¸” ëª¨ë¸ ìˆ˜: {len(ensemble_models)}ê°œ")
    print(f"   ì†ì‹¤ í•¨ìˆ˜: {type(criterion).__name__}")
    print(f"   ì˜µí‹°ë§ˆì´ì €: {type(optimizer).__name__}")
    
    # í…ŒìŠ¤íŠ¸ ì…ë ¥
    test_input = torch.randn(2, 3, 512, 512)
    with torch.no_grad():
        output = model(test_input)
        features = model.get_features(test_input)
        print(f"   ì¶œë ¥ í˜•íƒœ: {output.shape}")
        print(f"   íŠ¹ì§• í˜•íƒœ: {features.shape}")
