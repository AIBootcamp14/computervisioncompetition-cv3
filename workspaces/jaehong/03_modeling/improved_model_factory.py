"""
üß† Improved Model Factory
ÌÅ¥Î¶∞ ÏΩîÎìúÏôÄ ÌÅ¥Î¶∞ ÏïÑÌÇ§ÌÖçÏ≤òÎ•º Ï†ÅÏö©Ìïú Í∞úÏÑ†Îêú Î™®Îç∏ Ìå©ÌÜ†Î¶¨

Features:
- Îçî Í∞ïÎ†•Ìïú Î™®Îç∏ ÏïÑÌÇ§ÌÖçÏ≤ò ÏßÄÏõê
- Í≥†Í∏â ÏÜêÏã§ Ìï®Ïàò (Focal Loss, Label Smoothing Îì±)
- Ïñ¥ÌÖêÏÖò Î©îÏª§ÎãàÏ¶ò ÌÜµÌï©
- ÏïôÏÉÅÎ∏îÏùÑ ÏúÑÌïú Îã§Ï§ë Î™®Îç∏ ÏßÄÏõê
- Î©îÎ™®Î¶¨ Ìö®Ïú®Ï†ÅÏù∏ Íµ¨ÌòÑ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import timm
from typing import Optional, Dict, Any, Union, List
import math
import numpy as np
from abc import ABC, abstractmethod


class BaseDocumentClassifier(nn.Module, ABC):
    """
    Î¨∏ÏÑú Î∂ÑÎ•ò Î™®Îç∏Ïùò Ï∂îÏÉÅ Í∏∞Î∞ò ÌÅ¥ÎûòÏä§
    ÌÅ¥Î¶∞ ÏïÑÌÇ§ÌÖçÏ≤ò ÏõêÏπô: ÏùòÏ°¥ÏÑ± Ïó≠Ï†Ñ
    """
    
    def __init__(self, num_classes: int = 17):
        super().__init__()
        self.num_classes = num_classes
    
    @abstractmethod
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """ÏàúÏ†ÑÌåå Ï∂îÏÉÅ Î©îÏÑúÎìú"""
        pass
    
    @abstractmethod
    def get_features(self, x: torch.Tensor) -> torch.Tensor:
        """ÌäπÏßï Î≤°ÌÑ∞ Ï∂îÏ∂ú Ï∂îÏÉÅ Î©îÏÑúÎìú"""
        pass


class AttentionModule(nn.Module):
    """
    Ï±ÑÎÑê Ïñ¥ÌÖêÏÖòÍ≥º Í≥µÍ∞Ñ Ïñ¥ÌÖêÏÖòÏùÑ Í≤∞Ìï©Ìïú Î™®Îìà
    ÏÑ±Îä• Ìñ•ÏÉÅÏùÑ ÏúÑÌïú ÌïµÏã¨ Ïª¥Ìè¨ÎÑåÌä∏
    """
    
    def __init__(self, in_channels: int, reduction: int = 16):
        super().__init__()
        
        # Ï±ÑÎÑê Ïñ¥ÌÖêÏÖò (Squeeze-and-Excitation)
        self.channel_attention = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False),
            nn.Sigmoid()
        )
        
        # Í≥µÍ∞Ñ Ïñ¥ÌÖêÏÖò (Spatial Attention)
        self.spatial_attention = nn.Sequential(
            nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False),
            nn.Sigmoid()
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Ï±ÑÎÑê Ïñ¥ÌÖêÏÖò Ï†ÅÏö©
        ca_weight = self.channel_attention(x)
        x = x * ca_weight
        
        # Í≥µÍ∞Ñ Ïñ¥ÌÖêÏÖòÏùÑ ÏúÑÌïú ÌÜµÍ≥Ñ Í≥ÑÏÇ∞
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        spatial_input = torch.cat([avg_out, max_out], dim=1)
        
        # Í≥µÍ∞Ñ Ïñ¥ÌÖêÏÖò Ï†ÅÏö©
        sa_weight = self.spatial_attention(spatial_input)
        x = x * sa_weight
        
        return x


class GeMPooling(nn.Module):
    """
    Generalized Mean Pooling
    Global Average PoolingÎ≥¥Îã§ Ïö∞ÏàòÌïú ÏÑ±Îä•
    """
    
    def __init__(self, p: float = 3.0, eps: float = 1e-6):
        super().__init__()
        self.p = nn.Parameter(torch.ones(1) * p)
        self.eps = eps
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return F.avg_pool2d(
            x.clamp(min=self.eps).pow(self.p), 
            (x.size(-2), x.size(-1))
        ).pow(1. / self.p)


class ImprovedDocumentClassifier(BaseDocumentClassifier):
    """
    Í∞úÏÑ†Îêú Î¨∏ÏÑú Î∂ÑÎ•ò Î™®Îç∏
    Îçî Í∞ïÎ†•Ìïú ÏïÑÌÇ§ÌÖçÏ≤òÏôÄ Ïñ¥ÌÖêÏÖò Î©îÏª§ÎãàÏ¶ò ÌÜµÌï©
    """
    
    def __init__(
        self,
        architecture: str = "efficientnet_b3",
        num_classes: int = 17,
        pretrained: bool = True,
        dropout_rate: float = 0.4,
        use_gem_pooling: bool = True,
        use_attention: bool = True,
        use_mixup: bool = True,
        image_size: int = 512
    ):
        """
        Args:
            architecture: Î∞±Î≥∏ ÏïÑÌÇ§ÌÖçÏ≤ò (Îçî Í∞ïÎ†•Ìïú Î™®Îç∏Îì§)
            num_classes: ÌÅ¥ÎûòÏä§ Ïàò
            pretrained: ÏÇ¨Ï†Ñ ÌõàÎ†®Îêú Í∞ÄÏ§ëÏπò ÏÇ¨Ïö©
            dropout_rate: ÎìúÎ°≠ÏïÑÏõÉ ÎπÑÏú®
            use_gem_pooling: GeM ÌíÄÎßÅ ÏÇ¨Ïö© Ïó¨Î∂Ä
            use_attention: Ïñ¥ÌÖêÏÖò Î©îÏª§ÎãàÏ¶ò ÏÇ¨Ïö© Ïó¨Î∂Ä
            use_mixup: MixUp ÏßÄÏõê Ïó¨Î∂Ä
            image_size: ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞
        """
        super().__init__(num_classes)
        
        self.architecture = architecture
        self.use_attention = use_attention
        self.use_mixup = use_mixup
        self.image_size = image_size
        
        # Î∞±Î≥∏ Î™®Îç∏ ÏÉùÏÑ±
        self.backbone = self._create_backbone(architecture, pretrained)
        
        # ÌäπÏßï Ï∞®Ïõê ÎèôÏ†Å Í≥ÑÏÇ∞
        self.feature_dim = self._get_feature_dim()
        
        # Ïñ¥ÌÖêÏÖò Î™®Îìà (ÏÑ†ÌÉùÏ†Å)
        if use_attention:
            self.attention = AttentionModule(self.feature_dim)
        
        # ÌíÄÎßÅ Î†àÏù¥Ïñ¥
        if use_gem_pooling:
            self.global_pool = GeMPooling()
        else:
            self.global_pool = nn.AdaptiveAvgPool2d(1)
        
        # Ìñ•ÏÉÅÎêú Î∂ÑÎ•ò Ìó§Îìú
        self.classifier = self._create_classifier_head(dropout_rate)
        
        # Í∞ÄÏ§ëÏπò Ï¥àÍ∏∞Ìôî
        self._init_weights()
    
    def _create_backbone(self, architecture: str, pretrained: bool) -> nn.Module:
        """
        Î∞±Î≥∏ Î™®Îç∏ ÏÉùÏÑ±
        Îçî Í∞ïÎ†•Ìïú ÏïÑÌÇ§ÌÖçÏ≤òÎì§ ÏßÄÏõê
        """
        try:
            backbone = timm.create_model(
                architecture,
                pretrained=pretrained,
                num_classes=0,  # Î∂ÑÎ•ò Ìó§Îìú Ï†úÍ±∞
                global_pool='',  # ÌíÄÎßÅ Ï†úÍ±∞
                drop_rate=0.0   # Î∞±Î≥∏Ïùò ÎìúÎ°≠ÏïÑÏõÉ Ï†úÍ±∞ (Î∂ÑÎ•ò Ìó§ÎìúÏóêÏÑú Í¥ÄÎ¶¨)
            )
            
            print(f"‚úÖ Î∞±Î≥∏ Î™®Îç∏ ÏÉùÏÑ± ÏÑ±Í≥µ: {architecture}")
            return backbone
            
        except Exception as e:
            print(f"‚ö†Ô∏è {architecture} ÏÉùÏÑ± Ïã§Ìå®: {e}")
            print("   Í∏∞Î≥∏ Î™®Îç∏(efficientnet_b0)Î°ú ÎåÄÏ≤¥")
            
            return timm.create_model(
                "efficientnet_b0",
                pretrained=pretrained,
                num_classes=0,
                global_pool='',
                drop_rate=0.0
            )
    
    def _get_feature_dim(self) -> int:
        """ÌäπÏßï Ï∞®Ïõê ÎèôÏ†Å Í≥ÑÏÇ∞"""
        with torch.no_grad():
            dummy_input = torch.randn(1, 3, self.image_size, self.image_size)
            features = self.backbone(dummy_input)
            return features.shape[1]
    
    def _create_classifier_head(self, dropout_rate: float) -> nn.Module:
        """
        Ìñ•ÏÉÅÎêú Î∂ÑÎ•ò Ìó§Îìú ÏÉùÏÑ±
        Îçî ÍπäÏùÄ Íµ¨Ï°∞Î°ú ÏÑ±Îä• Ìñ•ÏÉÅ
        """
        # Ï§ëÍ∞Ñ Ï∞®Ïõê Í≥ÑÏÇ∞ (ÌäπÏßï Ï∞®ÏõêÏóê Îî∞Îùº Ï†ÅÏùëÏ†Å Ï°∞Ï†ï)
        if self.feature_dim >= 2048:
            hidden_dim = 1024
        elif self.feature_dim >= 1280:
            hidden_dim = 640
        else:
            hidden_dim = 512
        
        return nn.Sequential(
            # Ï≤´ Î≤àÏß∏ Î†àÏù¥Ïñ¥
            nn.Dropout(dropout_rate),
            nn.Linear(self.feature_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(inplace=True),
            
            # Îëê Î≤àÏß∏ Î†àÏù¥Ïñ¥
            nn.Dropout(dropout_rate * 0.7),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(inplace=True),
            
            # Ï∂úÎ†• Î†àÏù¥Ïñ¥
            nn.Dropout(dropout_rate * 0.5),
            nn.Linear(hidden_dim // 2, self.num_classes)
        )
    
    def _init_weights(self):
        """Í∞ÄÏ§ëÏπò Ï¥àÍ∏∞Ìôî (Xavier/He Ï¥àÍ∏∞Ìôî)"""
        for module in self.classifier.modules():
            if isinstance(module, nn.Linear):
                # He Ï¥àÍ∏∞Ìôî (ReLUÏôÄ Ìï®Íªò ÏÇ¨Ïö©)
                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.BatchNorm1d):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """ÏàúÏ†ÑÌåå"""
        # Î∞±Î≥∏ÏùÑ ÌÜµÌïú ÌäπÏßï Ï∂îÏ∂ú
        features = self.backbone(x)
        
        # Ïñ¥ÌÖêÏÖò Ï†ÅÏö© (ÏÑ†ÌÉùÏ†Å)
        if self.use_attention:
            features = self.attention(features)
        
        # Í∏ÄÎ°úÎ≤å ÌíÄÎßÅ
        features = self.global_pool(features)
        features = features.flatten(1)
        
        # Î∂ÑÎ•ò
        output = self.classifier(features)
        
        return output
    
    def get_features(self, x: torch.Tensor) -> torch.Tensor:
        """ÌäπÏßï Î≤°ÌÑ∞ Ï∂îÏ∂ú (ÏïôÏÉÅÎ∏îÏö©)"""
        with torch.no_grad():
            features = self.backbone(x)
            
            if self.use_attention:
                features = self.attention(features)
            
            features = self.global_pool(features)
            features = features.flatten(1)
            
        return features


class AdvancedFocalLoss(nn.Module):
    """
    Í∞úÏÑ†Îêú Focal Loss
    ÌÅ¥ÎûòÏä§ Î∂àÍ∑†ÌòïÍ≥º Ïñ¥Î†§Ïö¥ ÏÉòÌîåÏóê Îçî Ìö®Í≥ºÏ†Å
    """
    
    def __init__(
        self,
        alpha: Optional[torch.Tensor] = None,
        gamma: float = 3.0,  # Îçî Í∞ïÌïú focusing
        reduction: str = 'mean',
        label_smoothing: float = 0.1
    ):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        self.label_smoothing = label_smoothing
    
    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        # Label smoothing Ï†ÅÏö©
        if self.label_smoothing > 0:
            num_classes = inputs.size(-1)
            smoothed_targets = targets * (1 - self.label_smoothing) + \
                             self.label_smoothing / num_classes
            ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        else:
            ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        
        # Focal weight Í≥ÑÏÇ∞
        pt = torch.exp(-ce_loss)
        focal_weight = (1 - pt) ** self.gamma
        
        # Alpha weight Ï†ÅÏö©
        if self.alpha is not None:
            if self.alpha.device != inputs.device:
                self.alpha = self.alpha.to(inputs.device)
            alpha_weight = self.alpha.gather(0, targets)
            focal_loss = alpha_weight * focal_weight * ce_loss
        else:
            focal_loss = focal_weight * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


class MixUpCutMixLoss(nn.Module):
    """
    MixUpÍ≥º CutMixÎ•º ÏúÑÌïú ÏÜêÏã§ Ìï®Ïàò
    Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ïÍ≥º Ìï®Íªò ÏÇ¨Ïö©
    """
    
    def __init__(self, criterion: nn.Module):
        super().__init__()
        self.criterion = criterion
    
    def forward(
        self,
        outputs: torch.Tensor,
        targets_a: torch.Tensor,
        targets_b: torch.Tensor,
        lam: float
    ) -> torch.Tensor:
        """
        MixUp/CutMix ÏÜêÏã§ Í≥ÑÏÇ∞
        Îëê ÌÉÄÍ≤üÏóê ÎåÄÌïú Í∞ÄÏ§ë ÌèâÍ∑†
        """
        return lam * self.criterion(outputs, targets_a) + \
               (1 - lam) * self.criterion(outputs, targets_b)


class ImprovedModelFactory:
    """
    Í∞úÏÑ†Îêú Î™®Îç∏ Ìå©ÌÜ†Î¶¨
    Îçî Í∞ïÎ†•Ìïú Î™®Îç∏Îì§Í≥º Í≥†Í∏â Í∏∞Î≤ï ÏßÄÏõê
    """
    
    # ÏßÄÏõêÌïòÎäî Í∞ïÎ†•Ìïú ÏïÑÌÇ§ÌÖçÏ≤òÎì§
    POWERFUL_ARCHITECTURES = [
        # EfficientNet Í≥ÑÏó¥ (Í∑†ÌòïÏû°Ìûå ÏÑ±Îä•)
        'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5',
        'efficientnetv2_m', 'efficientnetv2_l',
        
        # ConvNeXt Í≥ÑÏó¥ (ÏµúÏã† CNN)
        'convnext_tiny', 'convnext_small', 'convnext_base',
        
        # Swin Transformer Í≥ÑÏó¥ (Vision Transformer)
        'swin_tiny_patch4_window7_224', 
        'swin_small_patch4_window7_224',
        'swin_base_patch4_window7_224',
        
        # RegNet Í≥ÑÏó¥ (Ìö®Ïú®Ï†ÅÏù∏ CNN)
        'regnetx_032', 'regnetx_040', 'regnetx_064',
        'regnety_032', 'regnety_040', 'regnety_064',
        
        # ResNet Í≥ÑÏó¥ (ÏïàÏ†ïÏ†ÅÏù∏ Í∏∞Î≥∏)
        'resnet50', 'resnet101', 'resnet152',
        'resnext50_32x4d', 'resnext101_32x8d',
    ]
    
    @staticmethod
    def create_model(
        architecture: str,
        num_classes: int = 17,
        pretrained: bool = True,
        **kwargs
    ) -> ImprovedDocumentClassifier:
        """
        Í∞úÏÑ†Îêú Î™®Îç∏ ÏÉùÏÑ±
        Îçî Í∞ïÎ†•Ìïú ÏïÑÌÇ§ÌÖçÏ≤ò Ïö∞ÏÑ† ÏÇ¨Ïö©
        """
        # ÏïÑÌÇ§ÌÖçÏ≤ò Í≤ÄÏ¶ù Î∞è Ï∂îÏ≤ú
        if architecture not in ImprovedModelFactory.POWERFUL_ARCHITECTURES:
            print(f"‚ö†Ô∏è {architecture}Îäî ÏµúÏ†ÅÌôîÎêòÏßÄ ÏïäÏùÄ ÏïÑÌÇ§ÌÖçÏ≤òÏûÖÎãàÎã§.")
            
            # ÏÑ±Îä• Í∏∞Î∞ò Ï∂îÏ≤ú
            if 'efficientnet' in architecture.lower():
                recommended = 'efficientnet_b3'
            elif 'convnext' in architecture.lower():
                recommended = 'convnext_base'
            elif 'swin' in architecture.lower():
                recommended = 'swin_base_patch4_window7_224'
            else:
                recommended = 'efficientnet_b3'  # Í∏∞Î≥∏ Ï∂îÏ≤ú
            
            print(f"   Ï∂îÏ≤ú ÏïÑÌÇ§ÌÖçÏ≤ò: {recommended}")
            architecture = recommended
        
        # Î™®Îç∏ ÏÉùÏÑ±
        model = ImprovedDocumentClassifier(
            architecture=architecture,
            num_classes=num_classes,
            pretrained=pretrained,
            **kwargs
        )
        
        print(f"üß† Í∞úÏÑ†Îêú Î™®Îç∏ ÏÉùÏÑ± ÏôÑÎ£å:")
        print(f"   ÏïÑÌÇ§ÌÖçÏ≤ò: {architecture}")
        print(f"   ÌååÎùºÎØ∏ÌÑ∞ Ïàò: {sum(p.numel() for p in model.parameters()):,}")
        print(f"   ÌäπÏßï Ï∞®Ïõê: {model.feature_dim}")
        
        return model
    
    @staticmethod
    def create_ensemble_models(
        architectures: List[str],
        num_classes: int = 17,
        pretrained: bool = True,
        **kwargs
    ) -> List[ImprovedDocumentClassifier]:
        """
        ÏïôÏÉÅÎ∏îÏö© Îã§Ï§ë Î™®Îç∏ ÏÉùÏÑ±
        Îã§ÏñëÌïú ÏïÑÌÇ§ÌÖçÏ≤ò Ï°∞Ìï©
        """
        models = []
        
        print(f"üé≠ ÏïôÏÉÅÎ∏î Î™®Îç∏ ÏÉùÏÑ± Ï§ë... ({len(architectures)}Í∞ú)")
        
        for i, arch in enumerate(architectures):
            print(f"   Î™®Îç∏ {i+1}/{len(architectures)}: {arch}")
            
            model = ImprovedModelFactory.create_model(
                architecture=arch,
                num_classes=num_classes,
                pretrained=pretrained,
                **kwargs
            )
            models.append(model)
        
        print(f"‚úÖ ÏïôÏÉÅÎ∏î Î™®Îç∏ ÏÉùÏÑ± ÏôÑÎ£å!")
        return models
    
    @staticmethod
    def create_loss_function(
        loss_type: str = "advanced_focal",
        class_weights: Optional[torch.Tensor] = None,
        **kwargs
    ) -> nn.Module:
        """
        Í∞úÏÑ†Îêú ÏÜêÏã§ Ìï®Ïàò ÏÉùÏÑ±
        ÌÅ¥ÎûòÏä§ Î∂àÍ∑†ÌòïÏóê Îçî Ìö®Í≥ºÏ†Å
        """
        if loss_type == "advanced_focal":
            return AdvancedFocalLoss(
                alpha=class_weights,
                gamma=kwargs.get('focal_gamma', 3.0),  # Îçî Í∞ïÌïú focusing
                label_smoothing=kwargs.get('label_smoothing', 0.1)
            )
        
        elif loss_type == "focal":
            # Í∏∞Ï°¥ Focal LossÏôÄ Ìò∏ÌôòÏÑ± - Í∞ÑÎã®Ìïú Focal Loss Íµ¨ÌòÑ
            class FocalLoss(nn.Module):
                def __init__(self, alpha=None, gamma=2.0):
                    super().__init__()
                    self.alpha = alpha
                    self.gamma = gamma
                
                def forward(self, inputs, targets):
                    ce_loss = F.cross_entropy(inputs, targets, reduction='none')
                    pt = torch.exp(-ce_loss)
                    focal_loss = (1 - pt) ** self.gamma * ce_loss
                    
                    if self.alpha is not None:
                        if self.alpha.device != inputs.device:
                            self.alpha = self.alpha.to(inputs.device)
                        at = self.alpha.gather(0, targets)
                        focal_loss = at * focal_loss
                    
                    return focal_loss.mean()
            
            return FocalLoss(
                alpha=class_weights,
                gamma=kwargs.get('focal_gamma', 2.0)
            )
        
        elif loss_type == "label_smoothing":
            return nn.CrossEntropyLoss(
                weight=class_weights,
                label_smoothing=kwargs.get('label_smoothing', 0.1)
            )
        
        elif loss_type == "cross_entropy":
            return nn.CrossEntropyLoss(weight=class_weights)
        
        else:
            raise ValueError(f"ÏßÄÏõêÌïòÏßÄ ÏïäÎäî ÏÜêÏã§ Ìï®Ïàò: {loss_type}")
    
    @staticmethod
    def create_optimizer(
        model: nn.Module,
        optimizer_type: str = "adamw",
        learning_rate: float = 1e-4,
        weight_decay: float = 1e-2,
        **kwargs
    ) -> torch.optim.Optimizer:
        """
        Í∞úÏÑ†Îêú ÏòµÌã∞ÎßàÏù¥Ï†Ä ÏÉùÏÑ±
        Îçî ÎÇòÏùÄ Í∏∞Î≥∏Í∞í ÏÇ¨Ïö©
        """
        # ÌååÎùºÎØ∏ÌÑ∞ Í∑∏Î£π Î∂ÑÎ¶¨ (Î∞±Î≥∏ vs Î∂ÑÎ•ò Ìó§Îìú)
        backbone_params = []
        classifier_params = []
        
        for name, param in model.named_parameters():
            if 'classifier' in name:
                classifier_params.append(param)
            else:
                backbone_params.append(param)
        
        if optimizer_type == "adamw":
            return torch.optim.AdamW([
                {'params': backbone_params, 'lr': learning_rate * 0.1},  # Î∞±Î≥∏ÏùÄ ÎÇÆÏùÄ ÌïôÏäµÎ•†
                {'params': classifier_params, 'lr': learning_rate}        # Î∂ÑÎ•ò Ìó§ÎìúÎäî ÎÜíÏùÄ ÌïôÏäµÎ•†
            ], weight_decay=weight_decay)
        
        elif optimizer_type == "adam":
            return torch.optim.Adam([
                {'params': backbone_params, 'lr': learning_rate * 0.1},
                {'params': classifier_params, 'lr': learning_rate}
            ], weight_decay=weight_decay)
        
        elif optimizer_type == "sgd":
            return torch.optim.SGD([
                {'params': backbone_params, 'lr': learning_rate * 0.1},
                {'params': classifier_params, 'lr': learning_rate}
            ], weight_decay=weight_decay, momentum=0.9, nesterov=True)
        
        else:
            raise ValueError(f"ÏßÄÏõêÌïòÏßÄ ÏïäÎäî ÏòµÌã∞ÎßàÏù¥Ï†Ä: {optimizer_type}")
    
    @staticmethod
    def create_scheduler(
        optimizer: torch.optim.Optimizer,
        scheduler_type: str = "cosine_warm_restarts",
        epochs: int = 30,
        **kwargs
    ):
        """
        Í∞úÏÑ†Îêú Ïä§ÏºÄÏ§ÑÎü¨ ÏÉùÏÑ±
        Îçî Ìö®Í≥ºÏ†ÅÏù∏ ÌïôÏäµÎ•† Ïä§ÏºÄÏ§ÑÎßÅ
        """
        if scheduler_type == "cosine_warm_restarts":
            return torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
                optimizer,
                T_0=kwargs.get('T_0', 10),
                T_mult=kwargs.get('T_mult', 2),
                eta_min=kwargs.get('eta_min', 1e-6)
            )
        
        elif scheduler_type == "cosine":
            return torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer,
                T_max=epochs,
                eta_min=kwargs.get('eta_min', 1e-6)
            )
        
        elif scheduler_type == "reduce_on_plateau":
            return torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer,
                mode='max',
                factor=kwargs.get('factor', 0.5),
                patience=kwargs.get('patience', 5),
                verbose=True
            )
        
        else:
            return None


# ÏÇ¨Ïö© ÏòàÏãú Î∞è ÌÖåÏä§Ìä∏
if __name__ == "__main__":
    print("üß† Í∞úÏÑ†Îêú Î™®Îç∏ Ìå©ÌÜ†Î¶¨ ÌÖåÏä§Ìä∏")
    
    # Îã®Ïùº Í∞ïÎ†•Ìïú Î™®Îç∏ ÏÉùÏÑ±
    model = ImprovedModelFactory.create_model(
        architecture="efficientnet_b3",
        num_classes=17,
        pretrained=True,
        dropout_rate=0.4,
        use_attention=True
    )
    
    # ÏïôÏÉÅÎ∏î Î™®Îç∏Îì§ ÏÉùÏÑ±
    ensemble_architectures = [
        "efficientnet_b3",
        "efficientnet_b4", 
        "convnext_base"
    ]
    
    ensemble_models = ImprovedModelFactory.create_ensemble_models(
        ensemble_architectures,
        num_classes=17
    )
    
    # Í∞úÏÑ†Îêú ÏÜêÏã§ Ìï®Ïàò ÏÉùÏÑ±
    criterion = ImprovedModelFactory.create_loss_function(
        loss_type="advanced_focal",
        focal_gamma=3.0,
        label_smoothing=0.1
    )
    
    # Í∞úÏÑ†Îêú ÏòµÌã∞ÎßàÏù¥Ï†Ä ÏÉùÏÑ±
    optimizer = ImprovedModelFactory.create_optimizer(
        model,
        optimizer_type="adamw",
        learning_rate=1e-4
    )
    
    print(f"‚úÖ Í∞úÏÑ†Îêú Î™®Îç∏ ÏãúÏä§ÌÖú ÌÖåÏä§Ìä∏ ÏôÑÎ£å!")
    print(f"   Îã®Ïùº Î™®Îç∏: {model.architecture}")
    print(f"   ÏïôÏÉÅÎ∏î Î™®Îç∏ Ïàò: {len(ensemble_models)}Í∞ú")
    print(f"   ÏÜêÏã§ Ìï®Ïàò: {type(criterion).__name__}")
    print(f"   ÏòµÌã∞ÎßàÏù¥Ï†Ä: {type(optimizer).__name__}")
    
    # ÌÖåÏä§Ìä∏ ÏûÖÎ†•
    test_input = torch.randn(2, 3, 512, 512)
    with torch.no_grad():
        output = model(test_input)
        features = model.get_features(test_input)
        print(f"   Ï∂úÎ†• ÌòïÌÉú: {output.shape}")
        print(f"   ÌäπÏßï ÌòïÌÉú: {features.shape}")
