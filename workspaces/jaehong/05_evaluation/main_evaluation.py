"""
ğŸ¯ Main Evaluation System
ì‹œë‹ˆì–´ ê·¸ëœë“œë§ˆìŠ¤í„° ìˆ˜ì¤€ì˜ í†µí•© í‰ê°€ ì‹œìŠ¤í…œ

Features:
- 01~04 ë‹¨ê³„ ì™„ì „ ì—°ê³„
- TTA ê¸°ë°˜ ê³ ê¸‰ ì˜ˆì¸¡
- ì¢…í•©ì ì¸ ëª¨ë¸ ë¶„ì„
- ìë™ ì œì¶œ íŒŒì¼ ìƒì„±
"""

import argparse
import warnings
from pathlib import Path
import torch
import pandas as pd
import numpy as np
from torch.utils.data import DataLoader

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore')

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from evaluation_config import EvaluationConfigManager
from tta_predictor import TTAPredictor, TTATransformFactory
from model_evaluator import ModelEvaluator

# 06_submission ëª¨ë“ˆ ì„í¬íŠ¸
import sys
sys.path.insert(0, str(Path(__file__).parent.parent / "06_submission"))
from submission_generator import SubmissionGenerator

# 04_training ëª¨ë“ˆ ì„í¬íŠ¸
sys.path.insert(0, str(Path(__file__).parent.parent / "04_training"))
from config import ConfigManager
from model import ModelFactory, DocumentClassifier
from data_loader import DataLoaderFactory
from utils import set_seed, get_gpu_info, optimize_gpu_settings, log_system_info


def parse_args():
    """ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±"""
    parser = argparse.ArgumentParser(description="ğŸ¯ Document Classification Evaluation")
    
    # ê¸°ë³¸ ì„¤ì •
    parser.add_argument("--training_experiment", type=str, required=True,
                       help="04_training ì‹¤í—˜ ì´ë¦„")
    parser.add_argument("--experiment_name", type=str, default="grandmaster_eval",
                       help="í‰ê°€ ì‹¤í—˜ ì´ë¦„")
    
    # TTA ì„¤ì •
    parser.add_argument("--enable_tta", action="store_true", default=True,
                       help="TTA í™œì„±í™”")
    parser.add_argument("--n_tta", type=int, default=8,
                       help="TTA ìˆ˜")
    parser.add_argument("--tta_weights", type=str, default=None,
                       help="TTA ê°€ì¤‘ì¹˜ (ì½¤ë§ˆë¡œ êµ¬ë¶„)")
    
    # í‰ê°€ ì„¤ì •
    parser.add_argument("--batch_size", type=int, default=None,
                       help="ë°°ì¹˜ í¬ê¸° (Noneì´ë©´ ìë™ ì„¤ì •)")
    parser.add_argument("--confidence_threshold", type=float, default=0.0,
                       help="ì‹ ë¢°ë„ ì„ê³„ê°’")
    
    # ì¶œë ¥ ì„¤ì •
    parser.add_argument("--create_submission", action="store_true", default=True,
                       help="ì œì¶œ íŒŒì¼ ìƒì„±")
    parser.add_argument("--save_probabilities", action="store_true", default=True,
                       help="ì˜ˆì¸¡ í™•ë¥  ì €ì¥")
    parser.add_argument("--detailed_analysis", action="store_true", default=True,
                       help="ìƒì„¸ ë¶„ì„ ìˆ˜í–‰")
    
    # ê²€ì¦ ì„¤ì •
    parser.add_argument("--validate_on_train", action="store_true",
                       help="í›ˆë ¨ ë°ì´í„°ë¡œ ê²€ì¦ ìˆ˜í–‰")
    parser.add_argument("--val_ratio", type=float, default=0.2,
                       help="ê²€ì¦ ë°ì´í„° ë¹„ìœ¨")
    
    # ê¸°íƒ€
    parser.add_argument("--seed", type=int, default=42,
                       help="ëœë¤ ì‹œë“œ")
    parser.add_argument("--output_dir", type=str, default="evaluation_results",
                       help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    
    return parser.parse_args()


def load_trained_model(model_path: str, model_config: dict, device: str) -> torch.nn.Module:
    """í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ"""
    
    print(f"ğŸ§  í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    
    # ëª¨ë¸ ìƒì„±
    model = DocumentClassifier(
        architecture=model_config.get('architecture', 'efficientnetv2_s'),
        num_classes=model_config.get('num_classes', 17),
        dropout_rate=model_config.get('dropout_rate', 0.3)
    )
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint = torch.load(model_path, map_location=device)
    
    # ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)
    
    model = model.to(device)
    model.eval()
    
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ:")
    print(f"   ì•„í‚¤í…ì²˜: {model_config.get('architecture', 'efficientnetv2_s')}")
    print(f"   íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()):,}")
    
    return model


def create_test_data_loader(data_factory: DataLoaderFactory, batch_size: int, image_size: int) -> DataLoader:
    """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë” ìƒì„±"""
    
    print(f"ğŸ—ƒï¸ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë” ìƒì„± ì¤‘...")
    
    test_loader = data_factory.create_test_loader(
        batch_size=batch_size,
        image_size=image_size,
        num_workers=4,
        tta=False
    )
    
    print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ:")
    print(f"   ë°°ì¹˜ ìˆ˜: {len(test_loader)}")
    print(f"   ì´ ìƒ˜í”Œ ìˆ˜: {len(test_loader.dataset)}")
    
    return test_loader


def create_validation_data_loader(
    data_factory: DataLoaderFactory, 
    batch_size: int, 
    image_size: int, 
    val_ratio: float
) -> DataLoader:
    """ê²€ì¦ ë°ì´í„° ë¡œë” ìƒì„±"""
    
    print(f"ğŸ” ê²€ì¦ ë°ì´í„° ë¡œë” ìƒì„± ì¤‘...")
    
    _, val_loader = data_factory.create_train_val_loaders(
        batch_size=batch_size,
        val_ratio=val_ratio,
        image_size=image_size,
        augmentation_level="none",  # ê²€ì¦ ì‹œì—ëŠ” ì¦ê°• ì—†ìŒ
        use_weighted_sampler=False
    )
    
    print(f"âœ… ê²€ì¦ ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ:")
    print(f"   ë°°ì¹˜ ìˆ˜: {len(val_loader)}")
    print(f"   ê²€ì¦ ìƒ˜í”Œ ìˆ˜: {len(val_loader.dataset)}")
    
    return val_loader


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ¯ Document Classification Evaluation")
    print("=" * 60)
    
    # ì¸ìˆ˜ íŒŒì‹±
    args = parse_args()
    
    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    system_info = log_system_info()
    optimize_gpu_settings()
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"\nğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    # ì¬í˜„ì„± ë³´ì¥
    set_seed(args.seed)
    
    # ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ê²½ë¡œ
    workspace_root = Path(__file__).parent.parent
    
    # í‰ê°€ ì„¤ì • ê´€ë¦¬ì ìƒì„±
    print(f"\nğŸ“Š í‰ê°€ ì„¤ì • ê´€ë¦¬ì ì´ˆê¸°í™”...")
    eval_config_manager = EvaluationConfigManager(
        str(workspace_root), 
        args.training_experiment
    )
    
    # TTA ê°€ì¤‘ì¹˜ íŒŒì‹±
    tta_weights = None
    if args.tta_weights:
        tta_weights = [float(w.strip()) for w in args.tta_weights.split(',')]
        if len(tta_weights) != args.n_tta:
            raise ValueError(f"TTA ê°€ì¤‘ì¹˜ ìˆ˜({len(tta_weights)})ê°€ TTA ìˆ˜({args.n_tta})ì™€ ë‹¤ë¦…ë‹ˆë‹¤.")
    
    # ë°°ì¹˜ í¬ê¸° ì„¤ì • (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
    batch_size = args.batch_size if args.batch_size is not None else 32
    
    # í‰ê°€ ì„¤ì • ìƒì„±
    eval_config = eval_config_manager.create_evaluation_config(
        experiment_name=args.experiment_name,
        enable_tta=args.enable_tta,
        n_tta=args.n_tta,
        batch_size=batch_size
    )
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(args.output_dir) / args.experiment_name
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ì„¤ì • ì €ì¥
    config_file = eval_config_manager.save_evaluation_config(eval_config, output_dir)
    
    print(f"\nğŸ¯ í‰ê°€ ì„¤ì •:")
    print(f"   ì‹¤í—˜ëª…: {eval_config.experiment_name}")
    print(f"   í›ˆë ¨ ì‹¤í—˜: {args.training_experiment}")
    print(f"   TTA: {'âœ…' if eval_config.tta.enabled else 'âŒ'} ({eval_config.tta.n_tta}ê°œ)")
    print(f"   ë°°ì¹˜ í¬ê¸°: {eval_config.batch_size}")
    print(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    
    # ëª¨ë¸ ì •ë³´ ë° ë°ì´í„° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    model_info = eval_config_manager.get_model_info()
    data_info = eval_config_manager.get_data_info()
    
    print(f"   ëª¨ë¸ F1: {model_info['best_f1']:.4f}")
    print(f"   ì†Œìˆ˜ í´ë˜ìŠ¤: {model_info['minority_classes']}")
    
    # í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ
    model = load_trained_model(
        model_info['model_path'],
        {
            'architecture': model_info['architecture'],
            'num_classes': model_info['num_classes'],
            'dropout_rate': 0.3
        },
        device
    )
    
    # 04_training ì„¤ì • ê´€ë¦¬ì ìƒì„± (ë°ì´í„° ë¡œë”ìš©)
    training_config_manager = ConfigManager(str(workspace_root))
    data_factory = DataLoaderFactory(training_config_manager)
    
    # ê²€ì¦ ìˆ˜í–‰ (ì„ íƒì )
    if args.validate_on_train:
        print(f"\nğŸ” í›ˆë ¨ ë°ì´í„° ê²€ì¦ ìˆ˜í–‰...")
        
        val_loader = create_validation_data_loader(
            data_factory, eval_config.batch_size, model_info['image_size'], args.val_ratio
        )
        
        # ëª¨ë¸ í‰ê°€ê¸° ìƒì„±
        evaluator = ModelEvaluator(
            model=model,
            class_names=data_info['class_names'],
            class_weights=data_info['class_weights'],
            device=device
        )
        
        # ì¢…í•© í‰ê°€ ìˆ˜í–‰
        validation_results = evaluator.evaluate_comprehensive(
            data_loader=val_loader,
            save_dir=output_dir / "validation_analysis",
            experiment_name=f"{args.experiment_name}_validation"
        )
        
        print(f"âœ… ê²€ì¦ ì™„ë£Œ:")
        print(f"   ì •í™•ë„: {validation_results['basic_metrics']['accuracy']:.4f}")
        print(f"   Macro F1: {validation_results['basic_metrics']['macro_f1']:.4f}")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
    print(f"\nğŸ”® í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì‹œì‘...")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë” ìƒì„±
    test_loader = create_test_data_loader(
        data_factory, eval_config.batch_size, model_info['image_size']
    )
    
    # ì´ë¯¸ì§€ ê²½ë¡œ ìˆ˜ì§‘
    sample_df = pd.read_csv(data_info['data_root'] + "/sample_submission.csv")
    test_image_paths = [
        str(Path(data_info['data_root']) / "test" / img_id) 
        for img_id in sample_df['ID']
    ]
    
    if eval_config.tta.enabled:
        # TTA ì˜ˆì¸¡
        print(f"ğŸ”® TTA ì˜ˆì¸¡ ìˆ˜í–‰...")
        
        tta_predictor = TTAPredictor(
            model=model,
            device=device,
            tta_weights=tta_weights
        )
        
        predictions, probabilities, tta_stats = tta_predictor.predict_with_tta(
            image_paths=test_image_paths,
            image_size=model_info['image_size'],
            batch_size=eval_config.batch_size,
            confidence_threshold=args.confidence_threshold
        )
        
        confidence_scores = np.max(probabilities, axis=1)
        
        print(f"âœ… TTA ì˜ˆì¸¡ ì™„ë£Œ:")
        print(f"   í‰ê·  ì‹ ë¢°ë„: {tta_stats['mean_confidence']:.4f}")
        print(f"   ì˜ˆì¸¡ ë¶„í¬: {len(tta_stats['class_distribution'])}ê°œ í´ë˜ìŠ¤")
        
        # TTA ê¸°ì—¬ë„ ë¶„ì„ (ì„ íƒì )
        if args.detailed_analysis:
            print(f"ğŸ” TTA ê¸°ì—¬ë„ ë¶„ì„...")
            tta_analysis = tta_predictor.analyze_tta_contribution(
                test_image_paths[:500],  # ìƒ˜í”Œë§Œ ë¶„ì„
                sample_size=100,
                image_size=model_info['image_size']
            )
            
            # TTA ë¶„ì„ ê²°ê³¼ ì €ì¥
            import json
            with open(output_dir / "tta_analysis.json", 'w') as f:
                json.dump(tta_analysis, f, indent=2, default=str)
    
    else:
        # ë‹¨ì¼ ì˜ˆì¸¡
        print(f"ğŸ¯ ë‹¨ì¼ ì˜ˆì¸¡ ìˆ˜í–‰...")
        
        all_predictions = []
        all_probabilities = []
        
        with torch.no_grad():
            for batch in test_loader:
                images = batch['image'].to(device, non_blocking=True)
                
                logits = model(images)
                probs = torch.nn.functional.softmax(logits, dim=1)
                preds = torch.argmax(logits, dim=1)
                
                all_predictions.extend(preds.cpu().numpy())
                all_probabilities.append(probs.cpu().numpy())
        
        predictions = np.array(all_predictions)
        probabilities = np.vstack(all_probabilities)
        confidence_scores = np.max(probabilities, axis=1)
        
        print(f"âœ… ë‹¨ì¼ ì˜ˆì¸¡ ì™„ë£Œ:")
        print(f"   í‰ê·  ì‹ ë¢°ë„: {np.mean(confidence_scores):.4f}")
    
    # ì œì¶œ íŒŒì¼ ìƒì„±
    if args.create_submission:
        print(f"\nğŸ“¤ ì œì¶œ íŒŒì¼ ìƒì„±...")
        
        submission_generator = SubmissionGenerator(
            sample_submission_path=str(Path(data_info['data_root']) / "sample_submission.csv"),
            class_names=data_info['class_names'],
            class_weights=data_info['class_weights']
        )
        
        submission_info = submission_generator.create_submission(
            predictions=predictions,
            probabilities=probabilities if args.save_probabilities else None,
            confidence_scores=confidence_scores,
            experiment_name=args.experiment_name,
            output_dir=output_dir / "submissions",
            save_probabilities=args.save_probabilities,
            create_analysis=args.detailed_analysis
        )
        
        print(f"âœ… ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ:")
        print(f"   íŒŒì¼: {submission_info['submission_file']}")
        print(f"   í´ë˜ìŠ¤ ë¶„í¬: {len(submission_info['class_distribution'])}ê°œ í´ë˜ìŠ¤")
        
        if 'confidence_stats' in submission_info:
            conf_stats = submission_info['confidence_stats']
            print(f"   ì‹ ë¢°ë„: {conf_stats['mean_confidence']:.4f} Â± {conf_stats['std_confidence']:.4f}")
    
    # ìµœì¢… ìš”ì•½
    print(f"\nğŸ í‰ê°€ ì™„ë£Œ!")
    print(f"   ì‹¤í—˜ëª…: {args.experiment_name}")
    print(f"   ì˜ˆì¸¡ ìˆ˜: {len(predictions)}")
    print(f"   í‰ê·  ì‹ ë¢°ë„: {np.mean(confidence_scores):.4f}")
    print(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    
    # ì„±ëŠ¥ ì˜ˆìƒ ì¶œë ¥
    if args.validate_on_train and 'validation_results' in locals():
        val_f1 = validation_results['basic_metrics']['macro_f1']
        print(f"   ê²€ì¦ F1: {val_f1:.4f}")
        
        # ê°„ë‹¨í•œ ì„±ëŠ¥ ì˜ˆì¸¡
        if val_f1 > 0.85:
            print(f"   ğŸ¯ ì˜ˆìƒ LB ì ìˆ˜: ë§¤ìš° ë†’ìŒ (0.80+)")
        elif val_f1 > 0.75:
            print(f"   ğŸ¯ ì˜ˆìƒ LB ì ìˆ˜: ë†’ìŒ (0.70+)")
        elif val_f1 > 0.65:
            print(f"   ğŸ¯ ì˜ˆìƒ LB ì ìˆ˜: ë³´í†µ (0.60+)")
        else:
            print(f"   ğŸ¯ ì˜ˆìƒ LB ì ìˆ˜: ê°œì„  í•„ìš”")
    
    return {
        'predictions': predictions,
        'probabilities': probabilities,
        'confidence_scores': confidence_scores,
        'submission_info': submission_info if args.create_submission else None,
        'validation_results': validation_results if args.validate_on_train else None
    }


if __name__ == "__main__":
    try:
        results = main()
        print("\nâœ… í‰ê°€ ì‹œìŠ¤í…œ ì‹¤í–‰ ì„±ê³µ!")
        
    except Exception as e:
        print(f"\nâŒ í‰ê°€ ì‹œìŠ¤í…œ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
