"""
ğŸ“Š Advanced Model Evaluation System
ì‹œë‹ˆì–´ ê·¸ëœë“œë§ˆìŠ¤í„° ìˆ˜ì¤€ì˜ ëª¨ë¸ í‰ê°€ ì‹œìŠ¤í…œ

Features:
- ì¢…í•©ì ì¸ ì„±ëŠ¥ ë¶„ì„
- í´ë˜ìŠ¤ë³„ ìƒì„¸ ë¶„ì„
- í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
- ì˜¤ë¶„ë¥˜ ì‚¬ë¡€ ë¶„ì„
- ì‹ ë¢°ë„ ë¶„ì„
"""

import torch
import torch.nn.functional as F
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    classification_report, confusion_matrix, f1_score, 
    accuracy_score, precision_score, recall_score,
    roc_curve, auc, precision_recall_curve
)
from sklearn.preprocessing import label_binarize
import json
from PIL import Image
import cv2
from tqdm import tqdm

# 04_training ëª¨ë“ˆ ì„í¬íŠ¸
import sys
sys.path.insert(0, str(Path(__file__).parent.parent / "04_training"))
from model import DocumentClassifier, ModelFactory


class ModelEvaluator:
    """ê³ ê¸‰ ëª¨ë¸ í‰ê°€ê¸°"""
    
    def __init__(
        self,
        model: torch.nn.Module,
        class_names: List[str],
        class_weights: Optional[Dict[int, float]] = None,
        device: str = "cuda"
    ):
        """
        Args:
            model: í‰ê°€í•  ëª¨ë¸
            class_names: í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
            class_weights: í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜
            device: ë””ë°”ì´ìŠ¤
        """
        self.model = model.to(device)
        self.device = device
        self.class_names = class_names
        self.num_classes = len(class_names)
        self.class_weights = class_weights or {}
        self.model.eval()
        
        print(f"ğŸ“Š ëª¨ë¸ í‰ê°€ê¸° ì´ˆê¸°í™”:")
        print(f"   í´ë˜ìŠ¤ ìˆ˜: {self.num_classes}")
        print(f"   ë””ë°”ì´ìŠ¤: {device}")
        print(f"   í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {'ì ìš©' if class_weights else 'ì—†ìŒ'}")
    
    def evaluate_comprehensive(
        self,
        data_loader: torch.utils.data.DataLoader,
        save_dir: Path,
        experiment_name: str = "evaluation"
    ) -> Dict[str, Any]:
        """ì¢…í•©ì ì¸ ëª¨ë¸ í‰ê°€"""
        
        print(f"ğŸ” ì¢…í•© í‰ê°€ ì‹œì‘...")
        save_dir = Path(save_dir)
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        predictions, targets, probabilities, image_paths = self._predict_all(data_loader)
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­ ê³„ì‚°
        basic_metrics = self._calculate_basic_metrics(targets, predictions, probabilities)
        
        # í´ë˜ìŠ¤ë³„ ë¶„ì„
        class_analysis = self._analyze_by_class(targets, predictions, probabilities)
        
        # í˜¼ë™ í–‰ë ¬ ìƒì„±
        confusion_results = self._create_confusion_matrix(targets, predictions, save_dir)
        
        # ì‹ ë¢°ë„ ë¶„ì„
        confidence_analysis = self._analyze_confidence(probabilities, targets, predictions, save_dir)
        
        # ì˜¤ë¶„ë¥˜ ë¶„ì„
        misclassification_analysis = self._analyze_misclassifications(
            targets, predictions, probabilities, image_paths, save_dir
        )
        
        # ROC/PR ê³¡ì„  (ë‹¤ì¤‘ í´ë˜ìŠ¤)
        roc_pr_analysis = self._create_roc_pr_curves(targets, probabilities, save_dir)
        
        # í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¶„ì„
        imbalance_analysis = self._analyze_class_imbalance(targets, predictions)
        
        # ì¢…í•© ê²°ê³¼
        comprehensive_results = {
            'experiment_name': experiment_name,
            'basic_metrics': basic_metrics,
            'class_analysis': class_analysis,
            'confusion_matrix': confusion_results,
            'confidence_analysis': confidence_analysis,
            'misclassification_analysis': misclassification_analysis,
            'roc_pr_analysis': roc_pr_analysis,
            'imbalance_analysis': imbalance_analysis,
            'sample_count': len(targets)
        }
        
        # ê²°ê³¼ ì €ì¥
        self._save_evaluation_results(comprehensive_results, save_dir)
        
        # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        self._generate_summary_report(comprehensive_results, save_dir)
        
        print(f"âœ… ì¢…í•© í‰ê°€ ì™„ë£Œ:")
        print(f"   ì •í™•ë„: {basic_metrics['accuracy']:.4f}")
        print(f"   Macro F1: {basic_metrics['macro_f1']:.4f}")
        print(f"   Weighted F1: {basic_metrics['weighted_f1']:.4f}")
        
        return comprehensive_results
    
    def _predict_all(self, data_loader) -> Tuple[np.ndarray, np.ndarray, np.ndarray, List[str]]:
        """ëª¨ë“  ë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰"""
        
        all_predictions = []
        all_targets = []
        all_probabilities = []
        all_image_paths = []
        
        with torch.no_grad():
            for batch in tqdm(data_loader, desc="ì˜ˆì¸¡ ì¤‘"):
                images = batch['image'].to(self.device, non_blocking=True)
                targets = batch.get('target', torch.zeros(images.size(0), dtype=torch.long))
                image_paths = batch.get('image_id', [''] * images.size(0))
                
                # ëª¨ë¸ ì˜ˆì¸¡
                logits = self.model(images)
                probabilities = F.softmax(logits, dim=1)
                predictions = torch.argmax(logits, dim=1)
                
                # ê²°ê³¼ ìˆ˜ì§‘
                all_predictions.extend(predictions.cpu().numpy())
                all_targets.extend(targets.cpu().numpy())
                all_probabilities.append(probabilities.cpu().numpy())
                all_image_paths.extend(image_paths)
        
        return (
            np.array(all_predictions),
            np.array(all_targets),
            np.vstack(all_probabilities),
            all_image_paths
        )
    
    def _calculate_basic_metrics(
        self, 
        targets: np.ndarray, 
        predictions: np.ndarray, 
        probabilities: np.ndarray
    ) -> Dict[str, float]:
        """ê¸°ë³¸ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        
        return {
            'accuracy': float(accuracy_score(targets, predictions)),
            'macro_f1': float(f1_score(targets, predictions, average='macro', zero_division=0)),
            'weighted_f1': float(f1_score(targets, predictions, average='weighted', zero_division=0)),
            'macro_precision': float(precision_score(targets, predictions, average='macro', zero_division=0)),
            'macro_recall': float(recall_score(targets, predictions, average='macro', zero_division=0)),
            'weighted_precision': float(precision_score(targets, predictions, average='weighted', zero_division=0)),
            'weighted_recall': float(recall_score(targets, predictions, average='weighted', zero_division=0))
        }
    
    def _analyze_by_class(
        self,
        targets: np.ndarray,
        predictions: np.ndarray, 
        probabilities: np.ndarray
    ) -> Dict[str, Any]:
        """í´ë˜ìŠ¤ë³„ ìƒì„¸ ë¶„ì„"""
        
        # í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­
        per_class_f1 = f1_score(targets, predictions, average=None, zero_division=0)
        per_class_precision = precision_score(targets, predictions, average=None, zero_division=0)
        per_class_recall = recall_score(targets, predictions, average=None, zero_division=0)
        
        # í´ë˜ìŠ¤ë³„ ì§€ì› ìƒ˜í”Œ ìˆ˜
        unique_targets, target_counts = np.unique(targets, return_counts=True)
        support_dict = dict(zip(unique_targets, target_counts))
        
        # í´ë˜ìŠ¤ë³„ í‰ê·  ì‹ ë¢°ë„
        class_confidences = {}
        for class_idx in range(self.num_classes):
            class_mask = targets == class_idx
            if np.sum(class_mask) > 0:
                class_probs = probabilities[class_mask, class_idx]
                class_confidences[class_idx] = float(np.mean(class_probs))
            else:
                class_confidences[class_idx] = 0.0
        
        # ê²°ê³¼ êµ¬ì„±
        class_analysis = {}
        for i in range(self.num_classes):
            class_analysis[f'class_{i}'] = {
                'name': self.class_names[i] if i < len(self.class_names) else f'class_{i}',
                'f1_score': float(per_class_f1[i]) if i < len(per_class_f1) else 0.0,
                'precision': float(per_class_precision[i]) if i < len(per_class_precision) else 0.0,
                'recall': float(per_class_recall[i]) if i < len(per_class_recall) else 0.0,
                'support': int(support_dict.get(i, 0)),
                'avg_confidence': class_confidences.get(i, 0.0),
                'class_weight': self.class_weights.get(i, 1.0),
                'is_minority': self.class_weights.get(i, 1.0) > 1.5
            }
        
        return class_analysis
    
    def _create_confusion_matrix(
        self,
        targets: np.ndarray,
        predictions: np.ndarray,
        save_dir: Path
    ) -> Dict[str, Any]:
        """í˜¼ë™ í–‰ë ¬ ìƒì„± ë° ì‹œê°í™”"""
        
        # í˜¼ë™ í–‰ë ¬ ê³„ì‚°
        cm = confusion_matrix(targets, predictions)
        
        # ì •ê·œí™”ëœ í˜¼ë™ í–‰ë ¬
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        # ì‹œê°í™”
        plt.figure(figsize=(15, 12))
        
        # ì ˆëŒ€ê°’ í˜¼ë™ í–‰ë ¬
        plt.subplot(2, 2, 1)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=self.class_names[:cm.shape[1]],
                   yticklabels=self.class_names[:cm.shape[0]])
        plt.title('Confusion Matrix (Absolute)')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        
        # ì •ê·œí™”ëœ í˜¼ë™ í–‰ë ¬
        plt.subplot(2, 2, 2)
        sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues',
                   xticklabels=self.class_names[:cm.shape[1]],
                   yticklabels=self.class_names[:cm.shape[0]])
        plt.title('Confusion Matrix (Normalized)')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        
        # í´ë˜ìŠ¤ë³„ ì •í™•ë„
        plt.subplot(2, 2, 3)
        class_accuracy = np.diag(cm) / np.sum(cm, axis=1)
        plt.bar(range(len(class_accuracy)), class_accuracy)
        plt.title('Per-Class Accuracy')
        plt.xlabel('Class')
        plt.ylabel('Accuracy')
        plt.xticks(range(len(class_accuracy)), 
                  [f'C{i}' for i in range(len(class_accuracy))], rotation=45)
        
        # í´ë˜ìŠ¤ ë¶„í¬
        plt.subplot(2, 2, 4)
        class_distribution = np.sum(cm, axis=1)
        plt.bar(range(len(class_distribution)), class_distribution)
        plt.title('Class Distribution')
        plt.xlabel('Class')
        plt.ylabel('Count')
        plt.xticks(range(len(class_distribution)), 
                  [f'C{i}' for i in range(len(class_distribution))], rotation=45)
        
        plt.tight_layout()
        plt.savefig(save_dir / 'confusion_matrix.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        return {
            'confusion_matrix': cm.tolist(),
            'normalized_confusion_matrix': cm_normalized.tolist(),
            'per_class_accuracy': class_accuracy.tolist(),
            'class_distribution': class_distribution.tolist()
        }
    
    def _analyze_confidence(
        self,
        probabilities: np.ndarray,
        targets: np.ndarray,
        predictions: np.ndarray,
        save_dir: Path
    ) -> Dict[str, Any]:
        """ì‹ ë¢°ë„ ë¶„ì„"""
        
        # ìµœëŒ€ í™•ë¥  (ì‹ ë¢°ë„)
        max_probs = np.max(probabilities, axis=1)
        
        # ì •ë‹µ/ì˜¤ë‹µë³„ ì‹ ë¢°ë„
        correct_mask = predictions == targets
        correct_confidences = max_probs[correct_mask]
        incorrect_confidences = max_probs[~correct_mask]
        
        # ì‹ ë¢°ë„ êµ¬ê°„ë³„ ì •í™•ë„
        confidence_bins = np.arange(0, 1.1, 0.1)
        bin_accuracies = []
        bin_counts = []
        
        for i in range(len(confidence_bins) - 1):
            bin_mask = (max_probs >= confidence_bins[i]) & (max_probs < confidence_bins[i + 1])
            if np.sum(bin_mask) > 0:
                bin_accuracy = np.mean(correct_mask[bin_mask])
                bin_accuracies.append(bin_accuracy)
                bin_counts.append(np.sum(bin_mask))
            else:
                bin_accuracies.append(0.0)
                bin_counts.append(0)
        
        # ì‹œê°í™”
        plt.figure(figsize=(12, 8))
        
        # ì‹ ë¢°ë„ ë¶„í¬
        plt.subplot(2, 2, 1)
        plt.hist([correct_confidences, incorrect_confidences], 
                bins=20, alpha=0.7, label=['Correct', 'Incorrect'])
        plt.xlabel('Confidence')
        plt.ylabel('Count')
        plt.title('Confidence Distribution')
        plt.legend()
        
        # ì‹ ë¢°ë„ vs ì •í™•ë„
        plt.subplot(2, 2, 2)
        bin_centers = (confidence_bins[:-1] + confidence_bins[1:]) / 2
        plt.plot(bin_centers, bin_accuracies, 'o-')
        plt.plot([0, 1], [0, 1], 'r--', alpha=0.5, label='Perfect Calibration')
        plt.xlabel('Confidence')
        plt.ylabel('Accuracy')
        plt.title('Reliability Diagram')
        plt.legend()
        
        # ì‹ ë¢°ë„ êµ¬ê°„ë³„ ìƒ˜í”Œ ìˆ˜
        plt.subplot(2, 2, 3)
        plt.bar(bin_centers, bin_counts)
        plt.xlabel('Confidence')
        plt.ylabel('Count')
        plt.title('Samples per Confidence Bin')
        
        # í´ë˜ìŠ¤ë³„ í‰ê·  ì‹ ë¢°ë„
        plt.subplot(2, 2, 4)
        class_confidences = []
        for class_idx in range(self.num_classes):
            class_mask = targets == class_idx
            if np.sum(class_mask) > 0:
                class_conf = np.mean(max_probs[class_mask])
                class_confidences.append(class_conf)
            else:
                class_confidences.append(0.0)
        
        plt.bar(range(self.num_classes), class_confidences)
        plt.xlabel('Class')
        plt.ylabel('Average Confidence')
        plt.title('Per-Class Average Confidence')
        plt.xticks(range(self.num_classes), [f'C{i}' for i in range(self.num_classes)], rotation=45)
        
        plt.tight_layout()
        plt.savefig(save_dir / 'confidence_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        return {
            'overall_confidence': {
                'mean': float(np.mean(max_probs)),
                'std': float(np.std(max_probs)),
                'min': float(np.min(max_probs)),
                'max': float(np.max(max_probs))
            },
            'correct_predictions': {
                'mean_confidence': float(np.mean(correct_confidences)) if len(correct_confidences) > 0 else 0.0,
                'count': int(len(correct_confidences))
            },
            'incorrect_predictions': {
                'mean_confidence': float(np.mean(incorrect_confidences)) if len(incorrect_confidences) > 0 else 0.0,
                'count': int(len(incorrect_confidences))
            },
            'calibration': {
                'bin_accuracies': bin_accuracies,
                'bin_counts': bin_counts,
                'bin_centers': bin_centers.tolist()
            },
            'per_class_confidence': class_confidences
        }
    
    def _analyze_misclassifications(
        self,
        targets: np.ndarray,
        predictions: np.ndarray,
        probabilities: np.ndarray,
        image_paths: List[str],
        save_dir: Path,
        top_k: int = 20
    ) -> Dict[str, Any]:
        """ì˜¤ë¶„ë¥˜ ë¶„ì„"""
        
        # ì˜¤ë¶„ë¥˜ ì°¾ê¸°
        misclassified_mask = predictions != targets
        misclassified_indices = np.where(misclassified_mask)[0]
        
        if len(misclassified_indices) == 0:
            return {'misclassification_count': 0}
        
        # ì‹ ë¢°ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ì‹ ë¢°ë„ë¡œ í‹€ë¦° ê²ƒë“¤)
        misclassified_confidences = np.max(probabilities[misclassified_indices], axis=1)
        sorted_indices = np.argsort(misclassified_confidences)[::-1]
        
        # ìƒìœ„ Kê°œ ì˜¤ë¶„ë¥˜ ì‚¬ë¡€
        top_misclassified = []
        for i in range(min(top_k, len(sorted_indices))):
            idx = misclassified_indices[sorted_indices[i]]
            top_misclassified.append({
                'index': int(idx),
                'image_path': image_paths[idx] if idx < len(image_paths) else '',
                'true_class': int(targets[idx]),
                'predicted_class': int(predictions[idx]),
                'confidence': float(misclassified_confidences[sorted_indices[i]]),
                'true_class_prob': float(probabilities[idx, targets[idx]]),
                'predicted_class_prob': float(probabilities[idx, predictions[idx]])
            })
        
        # ì˜¤ë¶„ë¥˜ íŒ¨í„´ ë¶„ì„
        misclass_patterns = {}
        for true_class in range(self.num_classes):
            true_mask = targets == true_class
            misclass_in_true = misclassified_mask & true_mask
            
            if np.sum(misclass_in_true) > 0:
                pred_classes = predictions[misclass_in_true]
                unique_preds, counts = np.unique(pred_classes, return_counts=True)
                
                patterns = []
                for pred_class, count in zip(unique_preds, counts):
                    patterns.append({
                        'predicted_as': int(pred_class),
                        'count': int(count),
                        'percentage': float(count / np.sum(true_mask) * 100)
                    })
                
                misclass_patterns[f'true_class_{true_class}'] = sorted(
                    patterns, key=lambda x: x['count'], reverse=True
                )
        
        return {
            'misclassification_count': int(np.sum(misclassified_mask)),
            'misclassification_rate': float(np.sum(misclassified_mask) / len(targets)),
            'top_misclassified': top_misclassified,
            'misclassification_patterns': misclass_patterns
        }
    
    def _create_roc_pr_curves(
        self,
        targets: np.ndarray,
        probabilities: np.ndarray,
        save_dir: Path
    ) -> Dict[str, Any]:
        """ROC ë° PR ê³¡ì„  ìƒì„± (ë‹¤ì¤‘ í´ë˜ìŠ¤)"""
        
        # ì´ì§„í™”
        targets_binarized = label_binarize(targets, classes=range(self.num_classes))
        
        # í´ë˜ìŠ¤ë³„ ROC/PR ê³„ì‚°
        roc_auc_scores = {}
        pr_auc_scores = {}
        
        plt.figure(figsize=(15, 10))
        
        # ROC ê³¡ì„ 
        plt.subplot(2, 3, 1)
        for i in range(min(self.num_classes, 10)):  # ìµœëŒ€ 10ê°œ í´ë˜ìŠ¤ë§Œ í‘œì‹œ
            if np.sum(targets == i) > 0:  # í•´ë‹¹ í´ë˜ìŠ¤ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°ë§Œ
                fpr, tpr, _ = roc_curve(targets_binarized[:, i], probabilities[:, i])
                roc_auc = auc(fpr, tpr)
                roc_auc_scores[f'class_{i}'] = float(roc_auc)
                
                plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.3f})')
        
        plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curves')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        
        # PR ê³¡ì„ 
        plt.subplot(2, 3, 2)
        for i in range(min(self.num_classes, 10)):
            if np.sum(targets == i) > 0:
                precision, recall, _ = precision_recall_curve(targets_binarized[:, i], probabilities[:, i])
                pr_auc = auc(recall, precision)
                pr_auc_scores[f'class_{i}'] = float(pr_auc)
                
                plt.plot(recall, precision, label=f'Class {i} (AUC = {pr_auc:.3f})')
        
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title('Precision-Recall Curves')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        
        # í´ë˜ìŠ¤ë³„ AUC ì ìˆ˜ ë°” ì°¨íŠ¸
        plt.subplot(2, 3, 3)
        if roc_auc_scores:
            classes = list(roc_auc_scores.keys())
            scores = list(roc_auc_scores.values())
            plt.bar(range(len(classes)), scores)
            plt.xlabel('Class')
            plt.ylabel('ROC AUC')
            plt.title('ROC AUC by Class')
            plt.xticks(range(len(classes)), [c.replace('class_', 'C') for c in classes], rotation=45)
        
        plt.tight_layout()
        plt.savefig(save_dir / 'roc_pr_curves.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        return {
            'roc_auc_scores': roc_auc_scores,
            'pr_auc_scores': pr_auc_scores,
            'macro_roc_auc': float(np.mean(list(roc_auc_scores.values()))) if roc_auc_scores else 0.0,
            'macro_pr_auc': float(np.mean(list(pr_auc_scores.values()))) if pr_auc_scores else 0.0
        }
    
    def _analyze_class_imbalance(
        self,
        targets: np.ndarray,
        predictions: np.ndarray
    ) -> Dict[str, Any]:
        """í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¶„ì„"""
        
        # ì‹¤ì œ í´ë˜ìŠ¤ ë¶„í¬
        unique_targets, target_counts = np.unique(targets, return_counts=True)
        true_distribution = dict(zip(unique_targets.astype(int), target_counts.astype(int)))
        
        # ì˜ˆì¸¡ í´ë˜ìŠ¤ ë¶„í¬
        unique_preds, pred_counts = np.unique(predictions, return_counts=True)
        pred_distribution = dict(zip(unique_preds.astype(int), pred_counts.astype(int)))
        
        # ë¶ˆê· í˜• ë¹„ìœ¨ ê³„ì‚°
        max_count = max(target_counts)
        imbalance_ratios = {}
        for class_idx, count in true_distribution.items():
            imbalance_ratios[class_idx] = float(max_count / count)
        
        # ì†Œìˆ˜ í´ë˜ìŠ¤ ì„±ëŠ¥ ë¶„ì„
        minority_threshold = 1.5
        minority_classes = [cls for cls, ratio in imbalance_ratios.items() if ratio > minority_threshold]
        
        minority_performance = {}
        for cls in minority_classes:
            class_mask = targets == cls
            if np.sum(class_mask) > 0:
                class_predictions = predictions[class_mask]
                class_accuracy = np.mean(class_predictions == cls)
                minority_performance[cls] = {
                    'accuracy': float(class_accuracy),
                    'support': int(np.sum(class_mask)),
                    'imbalance_ratio': imbalance_ratios[cls],
                    'class_weight': self.class_weights.get(cls, 1.0)
                }
        
        return {
            'true_distribution': true_distribution,
            'predicted_distribution': pred_distribution,
            'imbalance_ratios': imbalance_ratios,
            'minority_classes': minority_classes,
            'minority_performance': minority_performance,
            'most_imbalanced_class': max(imbalance_ratios.keys(), key=lambda x: imbalance_ratios[x]),
            'least_imbalanced_class': min(imbalance_ratios.keys(), key=lambda x: imbalance_ratios[x])
        }
    
    def _save_evaluation_results(self, results: Dict[str, Any], save_dir: Path):
        """í‰ê°€ ê²°ê³¼ ì €ì¥"""
        
        # JSON ì €ì¥ (numpy íƒ€ì… ë³€í™˜)
        def convert_numpy_types(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {str(k): convert_numpy_types(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            else:
                return obj
        
        results_converted = convert_numpy_types(results)
        results_file = save_dir / 'evaluation_results.json'
        with open(results_file, 'w') as f:
            json.dump(results_converted, f, indent=2, default=str)
        
        print(f"ğŸ’¾ í‰ê°€ ê²°ê³¼ ì €ì¥: {results_file}")
    
    def _generate_summary_report(self, results: Dict[str, Any], save_dir: Path):
        """ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        report_lines = [
            "# ğŸ“Š ëª¨ë¸ í‰ê°€ ìš”ì•½ ë¦¬í¬íŠ¸",
            f"**ì‹¤í—˜ëª…**: {results['experiment_name']}",
            f"**í‰ê°€ ìƒ˜í”Œ ìˆ˜**: {results['sample_count']}",
            "",
            "## ğŸ¯ ì „ì²´ ì„±ëŠ¥",
            f"- **ì •í™•ë„**: {results['basic_metrics']['accuracy']:.4f}",
            f"- **Macro F1**: {results['basic_metrics']['macro_f1']:.4f}",
            f"- **Weighted F1**: {results['basic_metrics']['weighted_f1']:.4f}",
            f"- **Macro Precision**: {results['basic_metrics']['macro_precision']:.4f}",
            f"- **Macro Recall**: {results['basic_metrics']['macro_recall']:.4f}",
            "",
            "## ğŸ” ì‹ ë¢°ë„ ë¶„ì„",
            f"- **í‰ê·  ì‹ ë¢°ë„**: {results['confidence_analysis']['overall_confidence']['mean']:.4f}",
            f"- **ì •ë‹µ ì˜ˆì¸¡ ì‹ ë¢°ë„**: {results['confidence_analysis']['correct_predictions']['mean_confidence']:.4f}",
            f"- **ì˜¤ë‹µ ì˜ˆì¸¡ ì‹ ë¢°ë„**: {results['confidence_analysis']['incorrect_predictions']['mean_confidence']:.4f}",
            "",
            "## âŒ ì˜¤ë¶„ë¥˜ ë¶„ì„",
            f"- **ì˜¤ë¶„ë¥˜ ìˆ˜**: {results['misclassification_analysis']['misclassification_count']}",
            f"- **ì˜¤ë¶„ë¥˜ ë¹„ìœ¨**: {results['misclassification_analysis']['misclassification_rate']:.4f}",
            "",
            "## âš–ï¸ í´ë˜ìŠ¤ ë¶ˆê· í˜• ì˜í–¥",
            f"- **ì†Œìˆ˜ í´ë˜ìŠ¤ ìˆ˜**: {len(results['imbalance_analysis']['minority_classes'])}",
            f"- **ê°€ì¥ ë¶ˆê· í˜•í•œ í´ë˜ìŠ¤**: {results['imbalance_analysis']['most_imbalanced_class']}",
            "",
            "## ğŸ† í´ë˜ìŠ¤ë³„ ìµœê³ /ìµœì € ì„±ëŠ¥",
        ]
        
        # í´ë˜ìŠ¤ë³„ F1 ì ìˆ˜ ì •ë ¬
        class_f1_scores = [(cls, info['f1_score']) for cls, info in results['class_analysis'].items()]
        class_f1_scores.sort(key=lambda x: x[1], reverse=True)
        
        report_lines.extend([
            f"- **ìµœê³  ì„±ëŠ¥**: {class_f1_scores[0][0]} (F1: {class_f1_scores[0][1]:.4f})",
            f"- **ìµœì € ì„±ëŠ¥**: {class_f1_scores[-1][0]} (F1: {class_f1_scores[-1][1]:.4f})",
        ])
        
        # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥
        report_file = save_dir / 'evaluation_summary.md'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        print(f"ğŸ“‹ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±: {report_file}")


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    print("ğŸ“Š ëª¨ë¸ í‰ê°€ê¸° í…ŒìŠ¤íŠ¸ëŠ” main_evaluation.pyì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
