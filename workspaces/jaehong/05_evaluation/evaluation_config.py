"""
ğŸ¯ Evaluation Configuration Manager
ì‹œë‹ˆì–´ ê·¸ëœë“œë§ˆìŠ¤í„° ìˆ˜ì¤€ì˜ í‰ê°€ ì„¤ì • ê´€ë¦¬

Features:
- 01~04 ë‹¨ê³„ ì™„ì „ ì—°ê³„
- ë‹¤ì–‘í•œ í‰ê°€ ì „ëµ
- TTA ìµœì í™” ì„¤ì •
- ì•™ìƒë¸” êµ¬ì„± ìë™í™”
"""

import os
import json
from pathlib import Path
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, asdict
import torch

# 04_training ëª¨ë“ˆ ì„í¬íŠ¸
import sys
sys.path.insert(0, str(Path(__file__).parent.parent / "04_training"))
from config import ConfigManager


@dataclass
class TTAConfig:
    """TTA ì„¤ì •"""
    enabled: bool = True
    n_tta: int = 8
    strategies: List[str] = None
    weights: List[float] = None
    
    def __post_init__(self):
        if self.strategies is None:
            self.strategies = [
                "original",
                "horizontal_flip", 
                "rotate_5",
                "rotate_minus_5",
                "brightness_up",
                "brightness_down",
                "contrast_up",
                "contrast_down"
            ]
        
        if self.weights is None:
            # ì›ë³¸ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
            self.weights = [0.3, 0.15, 0.1, 0.1, 0.1, 0.1, 0.075, 0.075]


@dataclass  
class EnsembleConfig:
    """ì•™ìƒë¸” ì„¤ì •"""
    enabled: bool = True
    strategy: str = "weighted_soft_voting"
    models: List[str] = None
    weights: List[float] = None
    confidence_threshold: float = 0.9
    
    def __post_init__(self):
        if self.models is None:
            self.models = ["best_model"]
        if self.weights is None:
            self.weights = [1.0]


@dataclass
class EvaluationConfig:
    """í‰ê°€ ì„¤ì •"""
    # ê¸°ë³¸ ì„¤ì •
    experiment_name: str = "evaluation_v1"
    device: str = "cuda"
    batch_size: int = 32
    num_workers: int = 4
    
    # TTA ì„¤ì •
    tta: TTAConfig = None
    
    # ì•™ìƒë¸” ì„¤ì •
    ensemble: EnsembleConfig = None
    
    # ë¶„ì„ ì„¤ì •
    generate_confusion_matrix: bool = True
    generate_classification_report: bool = True
    analyze_misclassifications: bool = True
    visualize_predictions: bool = True
    save_prediction_samples: int = 50
    
    # ì¶œë ¥ ì„¤ì •
    create_submission: bool = True
    save_probabilities: bool = True
    
    def __post_init__(self):
        if self.tta is None:
            self.tta = TTAConfig()
        if self.ensemble is None:
            self.ensemble = EnsembleConfig()


class EvaluationConfigManager:
    """í‰ê°€ ì„¤ì • ê´€ë¦¬ì - 01~04 ë‹¨ê³„ ì™„ì „ ì—°ê³„"""
    
    def __init__(self, workspace_root: str, training_experiment_name: str):
        """
        Args:
            workspace_root: ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ë£¨íŠ¸ ê²½ë¡œ
            training_experiment_name: 04_training ì‹¤í—˜ ì´ë¦„
        """
        self.workspace_root = Path(workspace_root)
        self.training_experiment_name = training_experiment_name
        
        # ê²½ë¡œ ì„¤ì •
        self.training_dir = self.workspace_root / "04_training"
        self.evaluation_dir = self.workspace_root / "05_evaluation"
        self.evaluation_dir.mkdir(exist_ok=True)
        
        # 04_training ì„¤ì • ê´€ë¦¬ì ìƒì„±
        self.training_config_manager = ConfigManager(str(self.workspace_root))
        
        # 04_training ê²°ê³¼ ë¡œë“œ
        self._load_training_results()
        
        print(f"ğŸ¯ Evaluation ì„¤ì • ê´€ë¦¬ì ì´ˆê¸°í™”:")
        print(f"   Training ì‹¤í—˜: {training_experiment_name}")
        print(f"   ëª¨ë¸ ê²½ë¡œ: {self.model_path}")
        print(f"   ìµœê³  F1: {self.training_results.get('best_f1', 'N/A')}")
    
    def _load_training_results(self):
        """04_training ê²°ê³¼ ë¡œë“œ"""
        # í›ˆë ¨ ê²°ê³¼ ë””ë ‰í† ë¦¬
        training_output_dir = self.training_dir / "outputs" / self.training_experiment_name
        
        # ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
        self.model_path = training_output_dir / "best_model.pth"
        
        # í›ˆë ¨ ì„¤ì • ë¡œë“œ
        config_path = training_output_dir / "training_config.json"
        if config_path.exists():
            with open(config_path, 'r') as f:
                self.training_config = json.load(f)
        else:
            print(f"âš ï¸ í›ˆë ¨ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
            self.training_config = {}
        
        # í›ˆë ¨ ê²°ê³¼ ë¡œë“œ (ì²´í¬í¬ì¸íŠ¸ì—ì„œ)
        if self.model_path.exists():
            checkpoint = torch.load(self.model_path, map_location='cpu', weights_only=False)
            self.training_results = {
                'best_f1': checkpoint.get('best_score', 0.0),
                'epoch': checkpoint.get('epoch', 0),
                'model_config': checkpoint.get('config', {}).get('model', {}),
                'training_config': checkpoint.get('config', {}).get('training', {})
            }
        else:
            print(f"âš ï¸ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.model_path}")
            self.training_results = {'best_f1': 0.0}
    
    def create_evaluation_config(
        self,
        experiment_name: str = None,
        enable_tta: bool = True,
        n_tta: int = 8,
        enable_ensemble: bool = False,
        **overrides
    ) -> EvaluationConfig:
        """í‰ê°€ ì„¤ì • ìƒì„±"""
        
        if experiment_name is None:
            experiment_name = f"eval_{self.training_experiment_name}"
        
        # GPU ë©”ëª¨ë¦¬ ê¸°ë°˜ ë°°ì¹˜ í¬ê¸° ì¡°ì •
        batch_size = 32
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory // 1024**3
            if gpu_memory < 8:
                batch_size = 16
            elif gpu_memory >= 16:
                batch_size = 64
        
        # TTA ì„¤ì •
        tta_config = TTAConfig(
            enabled=enable_tta,
            n_tta=n_tta if enable_tta else 1
        )
        
        # ì•™ìƒë¸” ì„¤ì •
        ensemble_config = EnsembleConfig(
            enabled=enable_ensemble
        )
        
        # ê¸°ë³¸ ì„¤ì • ìƒì„±
        config = EvaluationConfig(
            experiment_name=experiment_name,
            batch_size=batch_size,
            tta=tta_config,
            ensemble=ensemble_config
        )
        
        # ì˜¤ë²„ë¼ì´ë“œ ì ìš©
        for key, value in overrides.items():
            if hasattr(config, key):
                setattr(config, key, value)
        
        return config
    
    def get_model_info(self) -> Dict[str, Any]:
        """í›ˆë ¨ëœ ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            'model_path': str(self.model_path),
            'architecture': self.training_results.get('model_config', {}).get('architecture', 'efficientnetv2_s'),
            'num_classes': self.training_results.get('model_config', {}).get('num_classes', 17),
            'image_size': self.training_results.get('model_config', {}).get('image_size', 512),
            'best_f1': self.training_results.get('best_f1', 0.0),
            'training_epochs': self.training_results.get('epoch', 0),
            'class_weights': self.training_config_manager.class_weights,
            'minority_classes': self.training_config_manager.get_minority_classes()
        }
    
    def get_data_info(self) -> Dict[str, Any]:
        """ë°ì´í„° ì •ë³´ ë°˜í™˜"""
        return {
            'data_root': str(self.training_config_manager.data_root),
            'num_classes': 17,
            'class_names': [f"class_{i}" for i in range(17)],
            'class_weights': self.training_config_manager.class_weights,
            'minority_classes': self.training_config_manager.get_minority_classes()
        }
    
    def save_evaluation_config(self, config: EvaluationConfig, output_dir: Path):
        """í‰ê°€ ì„¤ì • ì €ì¥"""
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ì„¤ì •ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        config_dict = {
            "evaluation": asdict(config),
            "model_info": self.get_model_info(),
            "data_info": self.get_data_info(),
            "training_experiment": self.training_experiment_name,
            "integration": {
                "eda_results": str(self.training_config_manager.eda_path),
                "preprocessing_config": str(self.training_config_manager.preprocessing_path),
                "modeling_results": str(self.training_config_manager.modeling_path),
                "training_output": str(self.training_dir / "outputs" / self.training_experiment_name)
            }
        }
        
        # JSON ì €ì¥
        config_file = output_dir / "evaluation_config.json"
        with open(config_file, 'w') as f:
            json.dump(config_dict, f, indent=2, default=str)
        
        print(f"ğŸ’¾ í‰ê°€ ì„¤ì • ì €ì¥: {config_file}")
        return config_file
    
    def create_tta_strategies(self, n_tta: int = 8) -> List[Dict[str, Any]]:
        """TTA ì „ëµ ìƒì„±"""
        strategies = [
            {"name": "original", "transform": "none", "weight": 0.3},
            {"name": "horizontal_flip", "transform": "hflip", "weight": 0.15},
            {"name": "rotate_5", "transform": "rotate", "params": {"degrees": 5}, "weight": 0.1},
            {"name": "rotate_minus_5", "transform": "rotate", "params": {"degrees": -5}, "weight": 0.1},
            {"name": "brightness_up", "transform": "brightness", "params": {"factor": 1.1}, "weight": 0.1},
            {"name": "brightness_down", "transform": "brightness", "params": {"factor": 0.9}, "weight": 0.1},
            {"name": "contrast_up", "transform": "contrast", "params": {"factor": 1.1}, "weight": 0.075},
            {"name": "contrast_down", "transform": "contrast", "params": {"factor": 0.9}, "weight": 0.075},
        ]
        
        return strategies[:n_tta]
    
    def optimize_tta_config(self, validation_results: Dict[str, float]) -> TTAConfig:
        """ê²€ì¦ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ TTA ì„¤ì • ìµœì í™”"""
        # ì„±ëŠ¥ì´ ì¢‹ìœ¼ë©´ ë” ë§ì€ TTA, ë‚˜ì˜ë©´ ì ê²Œ
        best_f1 = validation_results.get('f1', 0.0)
        
        if best_f1 > 0.85:
            n_tta = 8
        elif best_f1 > 0.75:
            n_tta = 6
        else:
            n_tta = 4
        
        return TTAConfig(
            enabled=True,
            n_tta=n_tta,
            strategies=self.create_tta_strategies(n_tta)
        )


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì„¤ì • ê´€ë¦¬ì ìƒì„±
    workspace_root = "/home/james/doc-classification/computervisioncompetition-cv3/workspaces/jaehong"
    training_experiment = "kaggle_real_training_v1"
    
    eval_config_manager = EvaluationConfigManager(workspace_root, training_experiment)
    
    # í‰ê°€ ì„¤ì • ìƒì„±
    eval_config = eval_config_manager.create_evaluation_config(
        experiment_name="grandmaster_evaluation_v1",
        enable_tta=True,
        n_tta=8,
        enable_ensemble=False
    )
    
    print(f"\nğŸ¯ í‰ê°€ ì„¤ì • ìƒì„± ì™„ë£Œ:")
    print(f"   ì‹¤í—˜ëª…: {eval_config.experiment_name}")
    print(f"   TTA: {eval_config.tta.enabled} ({eval_config.tta.n_tta}ê°œ)")
    print(f"   ì•™ìƒë¸”: {eval_config.ensemble.enabled}")
    print(f"   ë°°ì¹˜ í¬ê¸°: {eval_config.batch_size}")
    
    # ëª¨ë¸ ì •ë³´ ì¶œë ¥
    model_info = eval_config_manager.get_model_info()
    print(f"   ëª¨ë¸ F1: {model_info['best_f1']:.4f}")
    print(f"   ì†Œìˆ˜ í´ë˜ìŠ¤: {model_info['minority_classes']}")
